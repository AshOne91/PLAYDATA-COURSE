좋음. 아래는 앞서 통합해서 보내드린 Netflix 주가 예측 RNN 모델 전체 코드의 **각 코드 스텝**과 **파라미터 의미**를 한땀한땀 **세세하게** 설명한 것임. 각 구간별로 어떤 역할을 하는지, 어떤 파라미터가 어떤 의미인지, 왜 그렇게 사용하는지를 모두 포함했음.  

---

## ✅ 1. 데이터 로딩 및 전처리
```python
import pandas as pd
df = pd.read_csv('/content/train.csv')
df['Date'] = pd.to_datetime(df['Date'])  # 날짜 문자열을 datetime으로 변환
df['year'] = df['Date'].dt.year          # 연도만 추출해 새 컬럼 생성
df.describe()
```

- **`pd.read_csv()`**: CSV 파일을 DataFrame 형태로 읽음.
- **`pd.to_datetime()`**: 문자열로 된 날짜를 datetime 자료형으로 변환해 시계열 데이터로 활용 가능.
- **`dt.year`**: datetime 컬럼에서 연도만 추출 (예: 2020-05-03 → 2020).

---

## ✅ 2. 데이터 시각화
```python
import matplotlib.pyplot as plt
sample = df.iloc[:, 1:4]
sample['Close'] = df['Close']
sample.hist()
plt.show()
```

- **`df.iloc[:, 1:4]`**: Open, High, Low 컬럼 선택
- **`sample['Close']`**: 종가도 시각화에 포함
- **`.hist()`**: 각 피처에 대한 히스토그램으로 분포 확인

---

## ✅ 3. Dataset 클래스 정의
```python
from torch.utils.data import Dataset, DataLoader
import numpy as np

class NetflixDataset(Dataset):
  def __init__(self, csv_path):
    df = pd.read_csv(csv_path)
    self.data = df.iloc[:, 1:4].to_numpy()  # Open, High, Low
    self.data = self.data / np.max(self.data)  # 정규화 (0~1)
    self.label = df.iloc[:, -1].to_numpy()  # Close 종가
    self.label = self.label / np.max(self.label)  # 정규화
  def __len__(self):
    return len(self.data) - 30  # 시계열 예측이므로 30일치 사용, 마지막 인덱스 예외처리
  def __getitem__(self, idx):
    data = self.data[idx:idx+30]  # 30일 시계열 데이터
    label = self.label[idx+30]    # 예측할 다음날 종가
    return torch.tensor(data, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)
```

- **Dataset 클래스 역할**: `DataLoader`가 배치 단위로 데이터를 자동으로 공급하게 도와줌.
- `__len__()` : 전체 샘플 개수 반환
- `__getitem__()` : 인덱스를 입력받아 (30일간의 입력 시퀀스, 다음날 종가) 쌍으로 반환
- **정규화**: RNN은 수치 범위에 민감 → 안정적 학습을 위해 0~1 범위로 스케일링

---

## ✅ 4. DataLoader 정의 및 확인
```python
netflix_dataset = NetflixDataset('/content/train.csv')
netflix_dataloader = DataLoader(netflix_dataset, batch_size=32)

data, label = next(iter(netflix_dataloader))
print(data.shape, label.shape)  # (32, 30, 3), (32,)
```

- **`batch_size=32`**: 32개의 샘플을 한 번에 모델에 공급
- **입력 shape**: `(batch_size, sequence_length, input_features)`
  - 32개의 샘플
  - 각 샘플은 30일 시계열
  - 피처 3개 (Open, High, Low)

---

## ✅ 5. RNN 모델 정의
```python
import torch.nn as nn
class NetflixRnn(nn.Module):
  def __init__(self):
    super().__init__()
    self.rnn = nn.RNN(input_size=3, hidden_size=10, num_layers=5, batch_first=True)
    self.linear1 = nn.Linear(in_features=30*10, out_features=100)
    self.linear2 = nn.Linear(in_features=100, out_features=1)
    self.relu = nn.ReLU()

  def forward(self, x, h0):
    x, hn = self.rnn(x, h0)  # x: (batch, 30, 10)
    x = x.reshape(x.size(0), -1)  # flatten: (batch, 30*10)
    x = self.relu(self.linear1(x))
    x = self.linear2(x)
    return x
```

- **`nn.RNN()` 파라미터**:
  - `input_size=3`: 시계열 하나당 특성 수 (Open, High, Low)
  - `hidden_size=10`: RNN 은닉 상태의 차원
  - `num_layers=5`: RNN 층을 5겹으로 쌓음
  - `batch_first=True`: 입력 텐서가 (batch, seq_len, input_size) 순서

- **`linear1`, `linear2`**:
  - RNN의 출력을 완전연결층에 입력해 종가 예측
  - 30일 × 10 hidden → 100 → 1 값 출력

---

## ✅ 6. 모델 학습
```python
netflix = NetflixRnn().to(device)
loss_fn = nn.MSELoss()
optim = torch.optim.Adam(netflix.parameters(), lr=1e-4)
epocs = 200
```

- **손실 함수**: 평균 제곱 오차 (MSELoss) – 회귀 문제이므로 사용
- **옵티마이저**: Adam – 가중치 갱신 자동 처리
- **러닝레이트 `lr=1e-4`**: 가중치 학습 속도 조절

---

## ✅ 7. 학습 루프
```python
for epoch in range(epocs):
  iterator = tqdm(netflix_dataloader)
  epoch_loss = 0.0
  for data, label in iterator:
    data, label = data.to(device), label.to(device)
    h0 = torch.zeros(5, data.size(0), 10).to(device)  # (num_layers, batch_size, hidden_size)
    pred = netflix(data, h0)
    loss = loss_fn(pred.squeeze(), label)
    loss.backward()
    optim.step()
    optim.zero_grad()
    epoch_loss += loss.item()
    iterator.set_description(f"loss : {loss.item():.4f}")
  print(f'epoch : {epoch+1}  loss : {epoch_loss / len(netflix_dataloader):.4f}')
```

- **`h0`**: RNN의 은닉상태 초기화 – 모든 시퀀스가 처음부터 시작하므로 매번 0으로 설정
- **`loss.backward()`**: 손실 함수의 미분 계산 (역전파)
- **`optim.step()`**: 파라미터 갱신
- **`optim.zero_grad()`**: 이전 미분값 제거
- **`pred.squeeze()`**: (32,1) → (32,) 형태로 차원 맞춤

---

## ✅ 8. 모델 저장
```python
torch.save(netflix.state_dict(), "netflix.pth")
```

- 학습된 모델의 가중치를 저장하여 재사용 가능

---

## ✅ 요약

| 단계 | 목적 | 주요 함수/파라미터 |
|------|------|--------------------|
| 데이터 로딩 | CSV 읽고 전처리 | `pd.read_csv`, `to_datetime` |
| 시각화 | 분포 확인 | `hist()`, `matplotlib` |
| Dataset 정의 | 시계열 슬라이딩 윈도우 구성 | `__getitem__`, `__len__` |
| DataLoader | 배치 단위 데이터 공급 | `batch_size=32` |
| RNN 모델 | 시계열 입력 학습 | `nn.RNN`, `Linear` |
| 학습 | 손실 계산, 가중치 업데이트 | `loss_fn`, `Adam`, `backward` |
| 저장 | 모델 재사용 | `torch.save()` |

---

필요 시 이 모델로 추론(예측)도 가능하며, `torch.no_grad()`로 감싸서 사용하면 됨.  
필요하면 추론 파트까지도 예제로 같이 만들어 줄 수 있음.  
원하는 부분 더 자세히 알려줄까?

좋음. 위 코드는 **학습이 완료된 RNN 모델을 불러와서** 전체 데이터셋에 대해 **예측 수행 + 평균 손실(MSE) 계산**하는 내용임.  
아래에 한 줄 한 줄 아주 세세하게 **모든 변수/구문/파라미터** 의미를 **보충설명**함.

---

## ✅ 1. 모델 로드
```python
loaded_model = NetflixRnn()
loaded_model.load_state_dict(torch.load('netflix.pth', map_location=device))
loaded_model.eval()
```

- `NetflixRnn()` : 학습 시 사용한 동일한 구조의 모델을 다시 생성
- `load_state_dict(...)` : 저장된 가중치(`netflix.pth`)를 모델에 불러옴
  - `map_location=device` : GPU/CPU 상황에 맞게 자동 로드
- `eval()` : 모델을 평가 모드로 설정
  - 드롭아웃, 배치 정규화 등을 비활성화
  - 예측 시 반드시 필요

---

## ✅ 2. 예측 및 손실 측정 초기화
```python
pred_lists = []
total_loss = 0.0
```

- `pred_lists` : 예측 결과값을 리스트에 저장
- `total_loss` : 전체 데이터셋에 대한 누적 손실값을 저장

---

## ✅ 3. 추론 루프
```python
with torch.no_grad():
```

- `no_grad()` :
  - 평가 시 **그래디언트(기울기)** 계산 생략 → 메모리/속도 절약
  - 추론 시 반드시 사용해야 함

---

### 🔁 루프 시작
```python
for data, label in netflix_dataloader:
```

- `netflix_dataloader` : `NetflixDataset`을 배치 단위로 제공
  - `data`: shape (batch_size, 30, 3)
  - `label`: shape (batch_size,)

---

## ✅ 4. 데이터 전처리
```python
label = label.reshape(-1,1).to(torch.float32).to(device)
data = data.clone().detach().to(torch.float32).to(device)
```

- `label.reshape(-1,1)` : (batch_size,) → (batch_size, 1)
  - 모델 출력 `predict`와 차원을 맞추기 위함
- `.to(torch.float32)` : 데이터 타입을 `float32`로 변환 (정확한 연산 보장)
- `.to(device)` : GPU 또는 CPU로 전송
- `data.clone().detach()` :
  - `clone()` : 원본 데이터 복사
  - `detach()` : 연산 그래프에서 분리 (불필요한 gradient 연결 방지)

---

## ✅ 5. 초기 은닉 상태 설정
```python
h0 = torch.zeros(5, data.shape[0], 10).to(device)
```

- `torch.zeros(...)` : 모든 값이 0인 텐서 생성
- shape: **(num_layers, batch_size, hidden_size)**
  - `5`: RNN 레이어 수 (모델에서 설정)
  - `data.shape[0]`: 현재 배치 크기
  - `10`: hidden state 크기
- `to(device)` : GPU/CPU로 전송

---

## ✅ 6. 예측 수행
```python
predict = loaded_model(data, h0)
```

- 학습된 `loaded_model`에 입력 데이터와 초기 은닉상태 `h0`를 넣어 예측값 `predict` 생성
- `predict` shape: `(batch_size, 1)` → 예측된 종가

---

## ✅ 7. 예측값 저장
```python
pred_lists.extend([p.item() for p in predict])
```

- `predict` 결과를 `.item()`으로 실수값으로 변환 후 리스트에 추가
- `.extend()` : 리스트에 여러 항목 추가

---

## ✅ 8. 손실 계산
```python
loss = loss_fn(predict, label)
total_loss += loss.item()
```

- `loss_fn`: `nn.MSELoss()` — 평균 제곱 오차
- `loss.item()` : 파이토치 텐서를 일반 숫자로 변환
- `total_loss`: 각 배치 손실값을 누적

---

## ✅ 9. 전체 평균 손실 출력
```python
print(f"loss : {total_loss / len(netflix_dataloader.dataset)}")
```

- `netflix_dataloader.dataset`: 전체 샘플 수
- 총 손실 합을 전체 샘플 수로 나눠 **평균 손실(MSE)** 계산

---

## 📌 요약 흐름도

| 단계 | 설명 |
|------|------|
| 모델 불러오기 | 학습된 가중치 로드, 평가 모드 전환 |
| `no_grad()` | 추론 중 그래디언트 계산 생략 |
| 배치 순회 | 전체 데이터셋을 순회하며 예측 |
| 전처리 | float32 변환, GPU 이동 |
| 은닉 상태 초기화 | (5, batch_size, 10) 형상 맞춰 초기화 |
| 예측 수행 | `model(data, h0)` |
| 결과 저장 | `.item()`으로 예측값 저장 |
| 손실 측정 | `MSELoss` 사용 후 누적 |
| 평균 손실 출력 | 전체 샘플 수로 나눠 출력 |

---

필요 시 예측 결과(`pred_lists`)를 실제 날짜와 함께 **시각화(plot)** 하는 것도 가능함.  
예: 실제 종가와 예측 종가를 겹쳐서 그리기.

그 부분도 원할 경우 시각화 코드도 추가해 줄 수 있음. 계속 진행할까?

좋아, 지금부터 너가 작성한 **로또 번호 예측 RNN 전체 코드**를 **한땀한땀**, 진짜 **낱낱이 설명**해줄게.  
각 파트는 다음처럼 나눌 수 있음:

---

## 🧩 1. 데이터셋 구성

```python
pd.read_excel('/content/lotto.xlsx').iloc[::-1]
```

- 엑셀 파일에서 로또 데이터 읽기
- `.iloc[::-1]`: 로또 데이터는 최신 회차가 위에 있음 → 시간 순서대로 바꾸기 위해 **뒤집기**

---

## 🧵 2. 커스텀 Dataset 클래스 정의

```python
class LottoDataSet(Dataset):
  def __init__(self, csv_path, sequence_len = 10 ):
    lott_df = pd.read_excel(csv_path).iloc[::-1]
    self.data = (lott_df.iloc[:-2,13:-1].astype(np.float32).to_numpy() - 1) / 44.0
    self.sequence_len = sequence_len
```

- `Dataset` 클래스를 상속하여 PyTorch에서 사용할 수 있는 커스텀 데이터셋 구성
- `lott_df.iloc[:-2,13:-1]`  
  → 로또 번호만 추출 (일반적으로 13~18열에 당첨번호 있음, 마지막열 보너스 번호는 제외)  
  → `[:-2]`: 마지막 2줄은 제거 (예측 시 target 확보를 위해 여유 데이터 제거)
- `-1)/44.0`  
  → 로또 번호는 1~45 → **0~1로 정규화**  
  (모델 학습 안정성 위해 반드시 정규화 필요)

```python
  def __len__(self):
    return len(self.data) - self.sequence_len
```

- 전체 길이에서 `sequence_len` 만큼 줄여줌
- 예: 총 100개가 있고, 시퀀스 길이가 10이면 → `90개`가 나옴

```python
  def __getitem__(self, index):
    data = self.data[index:index+self.sequence_len]
    label = self.data[index+self.sequence_len]
    return torch.FloatTensor(data), torch.FloatTensor(label)
```

- 입력 시퀀스: 과거 10개 회차 번호
- 정답 (label): 그 다음 회차 번호
- → 예: 1~10회차로 11회차 예측

---

## 🧠 3. RNN 모델 정의

```python
class LottoRnn(nn.Module):
  def __init__(self,input_size=6, hidden_size=64,num_layers=3):
    super().__init__()
```

- `input_size=6`: 6개의 당첨번호가 입력됨 (1줄이 6개의 숫자)
- `hidden_size=64`: RNN의 은닉 노드 수
- `num_layers=3`: RNN이 3층으로 쌓여 있음

```python
    self.rnn = nn.RNN(input_size=input_size,hidden_size=hidden_size
                      ,num_layers=num_layers,batch_first=True,dropout=0.2)
```

- RNN 계층 구성:
  - `batch_first=True`: 입력 차원이 (batch, seq_len, input_size)
  - dropout=0.2: 층 간 과적합 방지

```python
    self.fc = nn.Sequential(
        nn.Linear(in_features=hidden_size,out_features=128),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(in_features=128,out_features=64),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(in_features=64,out_features=6),
    )
```

- RNN의 마지막 출력은 (배치, hidden_size)  
- 이걸 완전연결층(fc)으로 통과시켜 → 6개의 숫자 예측

```python
  def forward(self,x):
    h0 = torch.zeros(self.num_layers, x.shape[0], self.hidden_size).to(device)
    out, _ = self.rnn(x,h0)
    out = out[:,-1,:]
    out = self.fc(out)
    return out
```

- 초기 은닉 상태 `h0` 생성 (RNN 입력용)
- `out[:,-1,:]`: 시퀀스의 마지막 시점 출력만 사용 (과거 정보를 다 모은 상태)
- 그걸 `fc`에 넣어 최종 6개의 숫자 예측

---

## 🔁 4. 학습 코드

```python
lotto_dataset = LottoDataSet('/content/lotto.xlsx')
lotto_dataloader = DataLoader(lotto_dataset,batch_size=32,shuffle=True)
```

- 커스텀 데이터셋을 배치 단위로 묶어줌
- `shuffle=True`: epoch마다 데이터 순서를 섞어 일반화 성능 향상

```python
lotto_model = LottoRnn()
loss_fn = nn.MSELoss()
lr = 1e-4
epochs = 200
optim = torch.optim.Adam(lotto_model.parameters(), lr=lr)
lotto_model = lotto_model.to(device)
```

- 모델, 손실함수, 최적화기 설정
- `MSELoss`: 숫자 회귀이기 때문에 사용
- `Adam`: 적응형 학습률 사용

---

```python
epoch_loss_list = []
for epoch in range(epochs):
  iterator = tqdm(lotto_dataloader)
  epoch_loss = 0.0
  for data, label in iterator:
    data,label = data.to(device), label.to(device)
    pred = lotto_model(data)
    loss = loss_fn(pred,label)
    loss.backward()
    optim.step()
    optim.zero_grad()
    epoch_loss += loss.item()
    iterator.set_description(f'loss : {loss.item() / len(lotto_dataloader)}')
  epoch_loss_list.append(epoch_loss / len(lotto_dataloader) )
  print(f"epoch : {epoch+1} loss : {epoch_loss / len(lotto_dataloader.dataset)}")
```

### 한 스텝씩 설명
- `data`, `label`: 배치 단위 입력/정답
- `pred = lotto_model(data)`: 예측
- `loss = loss_fn(pred,label)`: 정답과 예측 비교
- `loss.backward()`: 역전파 → gradient 계산
- `optim.step()`: 파라미터 업데이트
- `optim.zero_grad()`: 누적된 gradient 초기화
- `epoch_loss_list`: epoch별 loss 저장

---

## 📉 5. 학습 결과 시각화

```python
import matplotlib.pyplot as plt
plt.plot(epoch_loss_list)
plt.show()
```

- 학습 loss 추이 확인

---

## 🔮 6. 예측

```python
sequence_len = 10
last_number = torch.FloatTensor( lotto_dataset.data[-sequence_len:] ).unsqueeze(0).to(device)
```

- 마지막 10개 회차를 불러와 예측에 사용
- `.unsqueeze(0)`: (10,6) → (1,10,6) → 배치 1개 입력 형태

```python
lotto_model.eval()
with torch.no_grad():
  prediction = lotto_model(last_number)
  preds = list(map(int,prediction[0]*44+1))
print(preds)
```

- 예측값을 0~1 정규화 형태에서 **역변환**
  - `*44+1`: 0~1 → 1~45로 복원
- `.eval()` + `torch.no_grad()`: 추론 모드 (dropout 비활성, gradient 계산 안함)

---

## 🔚 최종 요약

| 구성 요소 | 의미 |
|-----------|------|
| `Dataset` | 시계열 데이터 구성 |
| `RNN`     | 시계열 구조 학습 |
| `fc`      | 예측값 출력 (6개 숫자) |
| `MSELoss` | 예측값과 실제값의 차이 측정 |
| `Adam`    | 모델 학습 최적화 |
| `last_number` | 최근 회차 기반 예측 입력 |
| `prediction` | 모델 추론 결과 (예측된 로또 번호) |

---

궁금한 부분 있으면 이어서 설명해줄게. 이거 기반으로 성능 개선도 가능함 (예: LSTM 변경, 앙상블 등).  
필요하면 바로 말해줘.

노이즈

원래값이 아닌 다른 값
이미지의 경우 : 초점, 통신과정에서 잡음이 섞임.
센서 및 장비 : 순간적인 오류

가우스 노이즈
모든 값이 정규분포를 따르는 노이즈
기존 픽셀에 정규분포를 따르는 노이즈가 추가
백색잡음 : 무수하게 섞인 노이즈

인코더 : 정보르 압축
디코더 : 압축된 정보를 복원
오토인코더 : 입력을 그대로 출력하도록 학습된 구조
CAE : 합성곱을 이용하는 오토인코더

장점 : 이미지에서 추출한 특징을 복원
인코더만 따로 사용해서 이미지의 특징 추출기로 사용
디코더만 따로 사용해서 새로운 이미지를 만들도록 활용

단점 : 인코더의 특징을 디코더가 사용하지 않는다.
학습이 잘 안되면.. 인코더 문제인지 디코더 문제인지 확실치 않다

좋아. 너가 요약한 내용을 기준으로 **노이즈, 오토인코더(AutoEncoder), CAE(Convolutional AutoEncoder)** 개념을 아주 **차근차근, 자세히** 설명해줄게.  
이 내용은 **딥러닝 기반 이미지 전처리, 특징 추출, 생성 모델 기초**와 관련됨.

---

## 🔊 1. 노이즈(Noise)

**노이즈**란?  
> **원래 신호에 섞여 들어온 불필요한 정보(잡음)**. → 정보 품질을 저하시키는 원인.

### 📸 예시
| 분야 | 설명 |
|------|------|
| **이미지** | 초점이 흔들리거나, 촬영 중 조명이 튐, 통신 과정 중 데이터 손상 |
| **센서** | 일시적인 전압 불안정, 센서 오작동 등으로 비정상적인 값 발생 |
| **오디오** | 백색 잡음(화이트 노이즈), 환경 잡음 등이 원래 소리에 섞임 |

---

## 🟢 2. 가우시안 노이즈 (Gaussian Noise)

**정규분포(Gaussian distribution)**를 따르는 노이즈  
→ 어떤 픽셀에 **확률적으로 평균이 0, 분산이 일정한** 잡음이 더해짐

### 수식으로 표현:
\[
I_{\text{noisy}}(x, y) = I(x, y) + N(0, \sigma^2)
\]

- \( I(x, y) \): 원래 이미지의 픽셀 값
- \( N(0, \sigma^2) \): 평균 0, 분산 \( \sigma^2 \)의 정규분포에서 샘플링된 노이즈

---

## ⚪ 3. 백색잡음 (White Noise)

- **모든 주파수 대역에 동일한 세기를 갖는 노이즈**
- **랜덤하고 예측 불가능함**
- 이미지의 경우, 시각적으로 "입자"처럼 보임 (마치 TV 흑백 화면 오류처럼)

---

## 🧠 4. 오토인코더 (AutoEncoder)

**오토인코더 = 인코더 + 디코더**

| 구조 | 설명 |
|------|------|
| **인코더 (Encoder)** | 입력 데이터를 저차원으로 압축 (특징 벡터 생성) |
| **디코더 (Decoder)** | 저차원 특징 벡터를 다시 원래 입력과 비슷하게 복원 |
| **목표** | 압축 후 복원하는 동안 **입력과 출력의 차이(MSE)**를 최소화 |

### 학습 목적
- **입력 데이터의 핵심 특징을 압축하고 재구성**하는 능력을 학습함

---

## 🧩 5. CAE (Convolutional AutoEncoder)

> **이미지 전용 오토인코더**  
> **Conv2D**를 사용하여 이미지의 지역적 특징 추출에 특화됨

### 구조

1. **Encoder**
   - Conv2D → ReLU → MaxPool
   - 점점 크기가 줄어듦 (공간 정보를 줄이며 핵심 정보만 남김)

2. **Decoder**
   - ConvTranspose2D (업샘플링)
   - 입력 이미지 복원

---

## ✅ 장점

| 항목 | 설명 |
|------|------|
| ✅ **특징 추출기 활용** | 인코더만 떼서 사용 → 이미지의 **압축 표현(Feature vector)** 생성 가능 |
| ✅ **이미지 생성기 활용** | 디코더만 떼서 사용 → 임의의 벡터로부터 새로운 이미지 생성 가능 |
| ✅ **노이즈 제거** | 노이즈가 섞인 이미지를 입력하고 원본을 출력하도록 학습 → **Denoising AutoEncoder** |

---

## ⚠️ 단점

| 문제 | 설명 |
|------|------|
| ❌ **인코더와 디코더 분리 학습** | 인코더가 추출한 정보를 디코더가 **제대로 활용하지 못할 수 있음** |
| ❌ **원인 불명 오류** | 복원이 잘 안되면, **인코더 문제인지 디코더 문제인지 구분 어려움** |
| ❌ **과적합 위험** | 입력과 출력이 같기 때문에 단순 복사만 배우게 될 위험도 있음

---

## 🧪 정리 예시: Denoising AutoEncoder 흐름

| 단계 | 설명 |
|------|------|
| 1️⃣ 노이즈가 섞인 이미지 입력 | 정규분포 잡음 추가된 이미지 |
| 2️⃣ 인코더 통과 | 이미지의 핵심 특징만 뽑음 |
| 3️⃣ 디코더 통과 | 잡음을 제거한 복원 이미지 출력 |
| 4️⃣ 손실 함수 | 원래 이미지와 복원 이미지 사이의 오차 계산 (ex. MSE) |

---

## 📌 참고 용도

| 용도 | 모델 예시 |
|------|-----------|
| 노이즈 제거 | Denoising AutoEncoder |
| 특징 추출 | 인코더만 떼어 CNN + Classifier 조합 |
| 생성 모델 | 디코더만 이용해 이미지 생성 (ex. VAE, GAN) |
| 차원 축소 | PCA 대체로 AutoEncoder 사용 가능 |

---

## 📎 요약

- **노이즈**: 원하지 않는 값. 이미지 품질 저하.
- **가우시안 노이즈**: 정규분포 따른 노이즈.
- **AutoEncoder**: 입력 → 인코더 → 디코더 → 출력 (입력 복원)
- **CAE**: Conv 기반 오토인코더. 이미지 특징 추출, 복원에 적합.
- **장점**: 이미지 압축/복원, 생성, 잡음 제거에 사용 가능.
- **단점**: 인코더-디코더 간 상호작용 부족 가능성.

---

원하면, **코드 예제(CAE + Denoising 예제)**도 만들어줄 수 있음.  
또는 오토인코더를 활용한 **이미지 노이즈 제거 프로젝트** 진행도 가능.  
어떤 방향으로 더 알아볼까?

