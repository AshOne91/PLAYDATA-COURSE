IMDB ë¦¬ë·° ë°ì´í„°ì…‹
RNN
ê¸°ìš¸ê¸° í´ë¦¬í•‘
LSTMì…€ 
ìž¥ë‹¨ê¸° ë©”ëª¨ë¦¬
ê²Œì´íŠ¸
ê¸°ì–µí•˜ë ¤ëŠ” ê²Œì´íŠ¸, ë§ê°í•˜ë ¤ëŠ” ê²Œì´íŠ¸

RNNì˜ ë¬¸ì œì 
```
ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)
 ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ì†ì‹¤ì´ ê³¼ê±°ì‹œì ìœ¼ë¡œ ì „ë‹¬ë ë•Œ ê¸°ìš¸ê¸° ê°’ì´ ì ì  ìž‘ì•„ì§€ë©´ì„œ 0ì— ê°€ê¹Œì›Œì§€ê³  ê²°êµ­ í•™ìŠµì´ ì•ˆë˜ëŠ” í˜„ìƒ
 - í•™ìŠµê³¼ì •ì—ì„œ íŒŒì•…í•˜ê¸° ì–´ë ¤ì›€
ê¸°ìš¸ê¸° í­ë°œ(Expoding Gradient)
 íŠ¹ì •ìƒí™©ì—ì„œ ê¸°ìš¸ê¸° ê°‘ì´ ë„ˆë¬´ ì»¤ì ¸ì„œ í•™ìŠµì´ ë¶ˆì•„ì •í•´ì§€ëŠ” í˜„ìƒ
 ì½”ë“œìƒìœ¼ë¡œ ê¸°ìš¸ê¸° í´ë¦¬í•‘ì´ ì‚¬ìš©
ìž¥ê¸° ì˜ì¡´ì„± í•™ìŠµì˜ ì–´ë ¤ì›€
 ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì´ˆê¸° ìž…ë ¥ì •ë³´ê°€ ì ì í¬ë¯¸í•´ì§€ëŠ” í˜„ìƒìœ¼ë¡œ ì´ˆê¸° íŠ¹ì„±ì„ ë°˜ì˜í•˜ì§€ ëª»í•¨
ë‹¨ìˆœí•œ ì •ë³´ ì €ìž¥
```
LSTM
```
 RNNê³¼ ë™ì¼í•˜ê²Œ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬, ì…€ìƒíƒœì™€ ê²Œì´íŠ¸ê°œë…ì„ ë„ìž… ë§ê° ê²Œì´íŠ¸
 ë§ê°ê²Œì´íŠ¸
  ì´ì§„ ì…€ ìƒíƒœì—ì„œ ì‚­ì œí•  ì •ë³´ë¥¼ ê²°ì •(0~1) 0ì€ ìžŠê³  1ì´ë©´ ìœ ì§€
```
```
ìž…ë ¥ê²Œì´íŠ¸
  ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ì €ìž¥í• ì§€ ê²°ì •
  1 ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì €ìž¥, 0ì´ë©´ ì €ìž¥ì•ˆ
```
```
ì¶œë ¥ê²Œì´íŠ¸
  ìµœì¢…ì ìœ¼ë¡œ ì–´ë–¤ ì •ë³´ë¥¼ ì€ë‹‰ìƒíƒœì— ë‹´ì•„ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ê¸¸ì§€ ê²°ì •í•˜ëŠ” ê²Œì´íŠ¸
```
```
lstmì˜ ì •ë³´ì²˜ë¦¬ ìˆœì„œ
  ê³¼ê±°ì •ë³´ì—ì„œ ë²„ë¦´ê²ƒì€ ë²„ë¦¬ê³  (Foget Gate)
  ìƒˆë¡œìš´ ì •ë³´ë¥¼ ë°›ì•„ë“¤ì´ê³ (Input Gate)
  ì…€ ìƒíƒœ ì—…ë°ì´íŠ¸
  ì—…ë°ì´íŠ¸ëœ ì •ë³´ë¥¼ ì €ìž¥í•œë’¤(Output Gate)
  ìµœì¢… ê²°ê³¼ë¥¼ ì¶œë ¥
```

### **ðŸ“Œ RNNì˜ ë¬¸ì œì  ë° LSTMì˜ í•´ê²° ë°©ë²•ì„ ì˜ˆì œì™€ í•¨ê»˜ ì„¤ëª…**

#### **1ï¸âƒ£ RNN(Recurrent Neural Network) ë¬¸ì œì **

âœ… **ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)**  
- ì—­ì „íŒŒ ì‹œ ê³¼ê±°ë¡œ ê°ˆìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ ì ì  ìž‘ì•„ì ¸ 0ì— ê°€ê¹Œì›Œì§€ë©´ì„œ í•™ìŠµì´ ì–´ë ¤ì›Œì§€ëŠ” ë¬¸ì œ.

âœ… **ê¸°ìš¸ê¸° í­ë°œ(Exploding Gradient)**  
- íŠ¹ì • ìƒí™©ì—ì„œ ê¸°ìš¸ê¸°ê°€ ë„ˆë¬´ ì»¤ì ¸ì„œ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§€ëŠ” ë¬¸ì œ.  
- í•´ê²° ë°©ë²•: **ê¸°ìš¸ê¸° í´ë¦¬í•‘(Gradient Clipping)** ì‚¬ìš©.

âœ… **ìž¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ì–´ë ¤ì›€**  
- ìž…ë ¥ ë°ì´í„°ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì´ˆê¸° ì •ë³´ê°€ í¬ë¯¸í•´ì ¸ ì¤‘ìš”í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œ.

âœ… **ë‹¨ìˆœí•œ ì •ë³´ ì €ìž¥**  
- RNNì€ ë‹¨ìˆœí•œ ì€ë‹‰ ìƒíƒœë§Œ ìœ ì§€í•˜ë©° ìž¥ê¸° ê¸°ì–µì„ í•˜ì§€ ëª»í•¨.

---

#### **2ï¸âƒ£ LSTM(Long Short-Term Memory)ìœ¼ë¡œ í•´ê²°**
- **LSTMì€ RNNê³¼ ë™ì¼í•˜ê²Œ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ì§€ë§Œ, ì…€ ìƒíƒœ(Cell State)ì™€ ê²Œì´íŠ¸(Gate) ê°œë…ì„ ë„ìž….**
- **ê²Œì´íŠ¸ë¥¼ í†µí•´ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ê¸°ì–µí•˜ê³ , í•„ìš” ì—†ëŠ” ì •ë³´ë¥¼ ì œê±°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ.**  

#### **ðŸ“Œ LSTMì˜ ì •ë³´ íë¦„**
1. **Forget Gate(ë§ê° ê²Œì´íŠ¸)**
   - ê³¼ê±° ì •ë³´ì—ì„œ ë²„ë¦´ ê²ƒì„ ì„ íƒ.
   - 0(ìžŠìŒ) ~ 1(ìœ ì§€)ì˜ ê°’ì„ ê°€ì§.

2. **Input Gate(ìž…ë ¥ ê²Œì´íŠ¸)**
   - ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì €ìž¥í• ì§€ ê²°ì •.
   - 0(ì €ìž¥ X) ~ 1(ì €ìž¥)ì˜ ê°’ì„ ê°€ì§.

3. **Cell State(ì…€ ìƒíƒœ) ì—…ë°ì´íŠ¸**
   - ê¸°ì¡´ ì •ë³´ë¥¼ ìƒˆ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸.

4. **Output Gate(ì¶œë ¥ ê²Œì´íŠ¸)**
   - ìµœì¢… ì€ë‹‰ ìƒíƒœë¥¼ ê²°ì •.

---

### **ðŸ“Œ RNNê³¼ LSTM ë¹„êµë¥¼ ìœ„í•œ ì‹¤ìŠµ ì˜ˆì œ**
Pythonì„ ì‚¬ìš©í•´ IMDB ë¦¬ë·° ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ RNNê³¼ LSTMì„ ë¹„êµ.

âœ” ìœ„ ì½”ë“œì—ì„œëŠ” IMDB ë¦¬ë·° ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ **RNN ëª¨ë¸ê³¼ LSTM ëª¨ë¸ì„ ê°ê° ì •ì˜**í–ˆìŒ.  
âœ” ë‘ ëª¨ë¸ì˜ ì°¨ì´ì ì„ í•™ìŠµí•˜ê³ , LSTMì´ RNNë³´ë‹¤ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©° ìž¥ê¸° ì˜ì¡´ì„±ì„ ë” ìž˜ í•™ìŠµí•˜ëŠ”ì§€ í™•ì¸ ê°€ëŠ¥í•¨.  

ì¶”ê°€ì ìœ¼ë¡œ í•™ìŠµ í›„ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ì½”ë“œë„ ì›í•œë‹¤ë©´ ìš”ì²­ ë°”ëžŒ.
### **IMDB ë¦¬ë·° ë°ì´í„°ì…‹ + RNN + LSTM ê°œë… ì˜ˆì œ**

IMDB ë¦¬ë·° ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ RNNê³¼ LSTMì„ ë¹„êµí•˜ë©´ì„œ ê°œë…ì„ í•˜ë‚˜ì”© í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ì§„í–‰í•¨.

---

### **1ï¸âƒ£ RNNì„ ì´ìš©í•œ í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„ (IMDB ë¦¬ë·° ë°ì´í„°ì…‹ ì‚¬ìš©)**

#### **ðŸ”¹ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬**
```python
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

# IMDB ë°ì´í„° ë¡œë“œ (ìƒìœ„ 10,000ê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©)
num_words = 10000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)

# íŒ¨ë”© ì²˜ë¦¬ (ë¬¸ìž¥ì˜ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§žì¶”ê¸° ìœ„í•´)
max_len = 200  # í•œ ë¬¸ìž¥ì˜ ìµœëŒ€ ê¸¸ì´
x_train = pad_sequences(x_train, maxlen=max_len)
x_test = pad_sequences(x_test, maxlen=max_len)
```
- `imdb.load_data(num_words=10000)`: ìžì£¼ ë“±ìž¥í•˜ëŠ” 10,000ê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©.
- `pad_sequences()`: ë¬¸ìž¥ì˜ ê¸¸ì´ë¥¼ `max_len=200`ìœ¼ë¡œ ë§žì¶¤.

---

#### **ðŸ”¹ RNN ëª¨ë¸ êµ¬ì¶•**
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Embedding, Dense

# RNN ëª¨ë¸ ìƒì„±
model_rnn = Sequential([
    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),  # ë‹¨ì–´ë¥¼ 128ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
    SimpleRNN(64, return_sequences=False),  # ì€ë‹‰ ìƒíƒœ í¬ê¸° 64
    Dense(1, activation='sigmoid')  # ê°ì„± ë¶„ì„ì´ë¯€ë¡œ 1ê°œì˜ ì¶œë ¥ì„ sigmoidë¡œ ë¶„ë¥˜
])

# ëª¨ë¸ ì»´íŒŒì¼
model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# ëª¨ë¸ í•™ìŠµ
history = model_rnn.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))
```
- `Embedding()`: ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜.
- `SimpleRNN()`: ì¼ë°˜ì ì¸ RNN ì¸µ ì‚¬ìš©.
- `Dense(1, activation='sigmoid')`: ì´ì§„ ê°ì„± ë¶„ë¥˜ (ê¸ì •/ë¶€ì •).

ðŸ”¹ **ë¬¸ì œì :**
- **ê¸°ìš¸ê¸° ì†Œì‹¤ (Vanishing Gradient)** â†’ ì˜¤ëž˜ëœ ìž…ë ¥ ì •ë³´ê°€ ì‚¬ë¼ì§.
- **ìž¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ** â†’ ê¸´ ë¬¸ìž¥ì„ í•™ìŠµí•  ë•Œ ì•žë¶€ë¶„ì„ ìž˜ ê¸°ì–µí•˜ì§€ ëª»í•¨.

---

### **2ï¸âƒ£ LSTMì„ í™œìš©í•˜ì—¬ ë¬¸ì œ í•´ê²°**

#### **ðŸ”¹ LSTM ëª¨ë¸ êµ¬ì¶•**
```python
from tensorflow.keras.layers import LSTM

# LSTM ëª¨ë¸ ìƒì„±
model_lstm = Sequential([
    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),
    LSTM(64, return_sequences=False),
    Dense(1, activation='sigmoid')
])

# ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ
model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history_lstm = model_lstm.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))
```
ðŸ”¹ **LSTMì´ í•´ê²°í•˜ëŠ” ë¬¸ì œ:**
- **ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ ë°©ì§€** (ìž¥ê¸° ê¸°ì–µ ê°€ëŠ¥).
- **ìž¥ê¸° ì˜ì¡´ì„± í•´ê²°** (ì´ì „ ë‹¨ì–´ì™€ ë©€ë¦¬ ë–¨ì–´ì§„ ë‹¨ì–´ë„ í•™ìŠµ ê°€ëŠ¥).

---

### **3ï¸âƒ£ LSTMì˜ ë‚´ë¶€ ë™ìž‘ ì˜ˆì œ**
LSTMì€ **Forget Gate, Input Gate, Output Gate**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ì¡°ì ˆí•¨.

#### **ðŸ”¹ í•œ ìŠ¤í…ì˜ ì—°ì‚° ê³¼ì •**
1. **Forget Gate (ë§ê° ê²Œì´íŠ¸)**  
   - ê³¼ê±° ì •ë³´ë¥¼ ë²„ë¦´ ê²ƒì¸ì§€ ê²°ì •.
   - ì˜ˆì œ: "ì´ ì˜í™”ëŠ” ì •ë§ [ì¢‹ì•˜ë‹¤]!" â†’ "ì •ë§"ì„ ê¸°ì–µí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ë²„ë¦´ ìˆ˜ë„ ìžˆìŒ.

2. **Input Gate (ìž…ë ¥ ê²Œì´íŠ¸)**
   - ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì €ìž¥í• ì§€ ê²°ì •.
   - ì˜ˆì œ: "ë°°ìš° ì—°ê¸°ê°€ [íƒì›”í•˜ë‹¤]" â†’ "íƒì›”í•˜ë‹¤"ë¥¼ ê¸°ì–µ.

3. **Output Gate (ì¶œë ¥ ê²Œì´íŠ¸)**
   - ìµœì¢…ì ìœ¼ë¡œ ì€ë‹‰ ìƒíƒœë¡œ ì „ë‹¬í•  ì •ë³´ë¥¼ ê²°ì •.
   - ì˜ˆì œ: "ìŠ¤í† ë¦¬ê°€ [ë›°ì–´ë‚¬ë‹¤]" â†’ "ë›°ì–´ë‚¬ë‹¤"ë¥¼ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬.

#### **ðŸ”¹ LSTM ê²Œì´íŠ¸ ë™ìž‘ ì½”ë“œ**
```python
import numpy as np

# ìž…ë ¥ê°’ (ë‹¨ì–´ ë²¡í„°)
x_t = np.array([0.2, 0.5, 0.1])  # ì˜ˆì œ ë‹¨ì–´ ë²¡í„°

# ê°€ì¤‘ì¹˜ (ìž„ì˜ì˜ ê°’)
Wf, Wi, Wo, Wc = np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3)
Uf, Ui, Uo, Uc = np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3)
bf, bi, bo, bc = np.random.randn(3), np.random.randn(3), np.random.randn(3), np.random.randn(3)

# ì´ì „ ìƒíƒœê°’
h_prev = np.array([0.1, 0.2, 0.3])
c_prev = np.array([0.4, 0.5, 0.6])

# ê²Œì´íŠ¸ ì—°ì‚°
forget_gate = tf.sigmoid(np.dot(Wf, x_t) + np.dot(Uf, h_prev) + bf)
input_gate = tf.sigmoid(np.dot(Wi, x_t) + np.dot(Ui, h_prev) + bi)
output_gate = tf.sigmoid(np.dot(Wo, x_t) + np.dot(Uo, h_prev) + bo)
candidate_memory = np.tanh(np.dot(Wc, x_t) + np.dot(Uc, h_prev) + bc)

# ìƒˆë¡œìš´ ì…€ ìƒíƒœ
c_t = forget_gate * c_prev + input_gate * candidate_memory

# ìƒˆë¡œìš´ ì€ë‹‰ ìƒíƒœ
h_t = output_gate * np.tanh(c_t)

print("Forget Gate Output:", forget_gate.numpy())
print("Input Gate Output:", input_gate.numpy())
print("Output Gate Output:", output_gate.numpy())
print("New Memory Cell:", c_t)
print("New Hidden State:", h_t)
```
ðŸ”¹ **ì´ ì½”ë“œì˜ ì˜ë¯¸:**
- `forget_gate` â†’ ê³¼ê±° ê¸°ì–µ ì¤‘ ì–´ë–¤ ê±¸ ìœ ì§€í• ì§€ ê²°ì •.
- `input_gate` â†’ ìƒˆë¡œìš´ ìž…ë ¥ ì¤‘ ì–´ë–¤ ê±¸ ì €ìž¥í• ì§€ ê²°ì •.
- `output_gate` â†’ ìµœì¢… ì¶œë ¥ ì •ë³´ë¥¼ ê²°ì •.

---

### **4ï¸âƒ£ ê¸°ìš¸ê¸° í´ë¦¬í•‘ (Gradient Clipping)**
ðŸ”¹ **ê¸°ìš¸ê¸° í­ë°œ ë°©ì§€ë¥¼ ìœ„í•´ ì‚¬ìš©**
```python
from tensorflow.keras.optimizers import Adam

# Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©í•˜ë©´ì„œ ê¸°ìš¸ê¸° í´ë¦¬í•‘ ì ìš©
optimizer = Adam(learning_rate=0.001, clipnorm=1.0)

model_lstm.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
```
- `clipnorm=1.0`: ê¸°ìš¸ê¸°ê°€ 1ì„ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì •í•˜ì—¬ í­ë°œ ë°©ì§€.

---

### **5ï¸âƒ£ RNNê³¼ LSTM ì„±ëŠ¥ ë¹„êµ**
```python
rnn_acc = model_rnn.evaluate(x_test, y_test, verbose=0)[1]
lstm_acc = model_lstm.evaluate(x_test, y_test, verbose=0)[1]

print(f"RNN Test Accuracy: {rnn_acc:.4f}")
print(f"LSTM Test Accuracy: {lstm_acc:.4f}")
```
**ì˜ˆìƒ ê²°ê³¼:**  
- RNN: ë‚®ì€ ì •í™•ë„ (ìž¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ)
- LSTM: ë†’ì€ ì •í™•ë„ (ìž¥ê¸° ê¸°ì–µ ê°€ëŠ¥)

---

### **ðŸ”¹ ê²°ë¡ **
- RNNì€ `ê¸°ìš¸ê¸° ì†Œì‹¤`ê³¼ `ìž¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ`ë¡œ ì¸í•´ ê¸´ ë¬¸ìž¥ ì²˜ë¦¬ì— ì–´ë ¤ì›€ì´ ìžˆìŒ.
- LSTMì€ **Forget/Input/Output ê²Œì´íŠ¸**ë¥¼ í†µí•´ ìž¥ê¸° ì˜ì¡´ì„±ì„ í•´ê²°í•˜ê³  ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ìž„.
- `ê¸°ìš¸ê¸° í´ë¦¬í•‘`ì„ í†µí•´ `ê¸°ìš¸ê¸° í­ë°œ`ì„ ë°©ì§€í•  ìˆ˜ ìžˆìŒ.

ì´ë ‡ê²Œ ê°œë…ë³„ë¡œ ì˜ˆì œì™€ í•¨ê»˜ ì‹¤ìŠµ ê°€ëŠ¥í•¨. ðŸš€

GRU(Gated Recurrent Unit) ì…€ì€ ìˆœí™˜ì‹ ê²½ë§(RNN)ì˜ í•œ ì¢…ë¥˜ë¡œ, **LSTM(Long Short-Term Memory)**ì˜ ë‹¨ìˆœí™”ëœ ë²„ì „ìž„.  

### **ðŸ”¹ GRUì˜ í•µì‹¬ ê°œë…**
GRUëŠ” **ìž¥ê¸° ì˜ì¡´ì„±(long-term dependency)** ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **ê²Œì´íŠ¸(gate) êµ¬ì¡°**ë¥¼ í™œìš©í•¨. LSTMê³¼ ë¹„êµí•˜ì—¬ **ë” ì ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ë©´ì„œë„ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ëƒ„**.  

---

### **ðŸ”¹ GRU êµ¬ì¡°**
GRU ì…€ì—ëŠ” **ë‘ ê°œì˜ ê²Œì´íŠ¸**ê°€ ìžˆìŒ.  
1. **ë¦¬ì…‹ ê²Œì´íŠ¸(Reset Gate, \( r_t \))**  
   - ì´ì „ ìƒíƒœë¥¼ ì–¼ë§ˆë‚˜ ìžŠì„ì§€ ê²°ì •í•¨.  
2. **ì—…ë°ì´íŠ¸ ê²Œì´íŠ¸(Update Gate, \( z_t \))**  
   - ìƒˆë¡œìš´ ì •ë³´ì™€ ì´ì „ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë°˜ì˜í• ì§€ ì¡°ì ˆí•¨.  

ðŸ“Œ **GRUëŠ” LSTMê³¼ ë‹¬ë¦¬ `ì…€ ìƒíƒœ(Cell State)`ê°€ ë”°ë¡œ ì—†ê³ , ì€ë‹‰ ìƒíƒœ(\( h_t \))ë§Œ ìœ ì§€í•¨.**  

---

### **ðŸ”¹ GRU ìˆ˜ì‹**
GRUì˜ ë™ìž‘ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŒ.

1. **ë¦¬ì…‹ ê²Œì´íŠ¸ ê³„ì‚°**  
   \[
   r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)
   \]
   
2. **ì—…ë°ì´íŠ¸ ê²Œì´íŠ¸ ê³„ì‚°**  
   \[
   z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)
   \]
   
3. **ìƒˆë¡œìš´ í›„ë³´ ì€ë‹‰ ìƒíƒœ ê³„ì‚°**  
   \[
   \tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)
   \]
   
4. **ìµœì¢… ì€ë‹‰ ìƒíƒœ ì—…ë°ì´íŠ¸**  
   \[
   h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
   \]

ì—¬ê¸°ì„œ,  
- \( \sigma \) : ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ (0~1 ì‚¬ì´ ê°’)  
- \( \odot \) : ìš”ì†Œë³„ ê³±ì…ˆ (element-wise multiplication)  
- \( W_r, W_z, W_h \) : í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ í–‰ë ¬  
- \( b_r, b_z, b_h \) : íŽ¸í–¥ ë²¡í„°  

---

### **ðŸ”¹ GRU vs LSTM ì°¨ì´ì **
| ë¹„êµ í•­ëª© | GRU | LSTM |
|-----------|------|------|
| ê²Œì´íŠ¸ ìˆ˜ | 2ê°œ (ë¦¬ì…‹, ì—…ë°ì´íŠ¸) | 3ê°œ (ìž…ë ¥, ì¶œë ¥, ë§ê°) |
| ì…€ ìƒíƒœ | ì—†ìŒ (ì€ë‹‰ ìƒíƒœë§Œ ì¡´ìž¬) | ì…€ ìƒíƒœì™€ ì€ë‹‰ ìƒíƒœ ë¶„ë¦¬ |
| ê³„ì‚° ë¹„ìš© | ë‚®ìŒ | ìƒëŒ€ì ìœ¼ë¡œ ë†’ìŒ |
| ì„±ëŠ¥ | ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ì¢‹ìŒ (ìž‘ì€ ë°ì´í„°ì…‹) | ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ì•ˆì •ì  |

---

### **ðŸ”¹ GRU ì‚¬ìš© ì˜ˆì œ (PyTorch)**
```python
import torch
import torch.nn as nn

# GRU ëª¨ë¸ ì •ì˜
gru = nn.GRU(input_size=10, hidden_size=20, num_layers=1, batch_first=True)

# ë”ë¯¸ ìž…ë ¥ ë°ì´í„° (ë°°ì¹˜ í¬ê¸°: 5, ì‹œí€€ìŠ¤ ê¸¸ì´: 3, ìž…ë ¥ í¬ê¸°: 10)
x = torch.randn(5, 3, 10)

# ì´ˆê¸° ì€ë‹‰ ìƒíƒœ (ë ˆì´ì–´ ìˆ˜, ë°°ì¹˜ í¬ê¸°, ì€ë‹‰ í¬ê¸°)
h0 = torch.zeros(1, 5, 20)

# GRU ì‹¤í–‰
output, hn = gru(x, h0)

print(output.shape)  # (5, 3, 20) -> ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ ë°˜í™˜
print(hn.shape)      # (1, 5, 20) -> ë§ˆì§€ë§‰ ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ
```

---

### **ðŸ”¹ GRU ìš”ì•½**
âœ… **ìž¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°**  
âœ… **LSTMë³´ë‹¤ êµ¬ì¡°ê°€ ë‹¨ìˆœí•˜ì—¬ ê³„ì‚°ëŸ‰ì´ ì ìŒ**  
âœ… **ìž‘ì€ ë°ì´í„°ì…‹ì—ì„œëŠ” LSTMë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ë„ ìžˆìŒ**  
âœ… **ê¸°ê³„ ë²ˆì—­, ìŒì„± ì¸ì‹, ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ ë“±ì— í™œìš©ë¨**  

GRUëŠ” **ì—°ì‚°ëŸ‰ì´ ì ê³  ë¹ ë¥¸ í•™ìŠµì´ í•„ìš”í•œ ê²½ìš° ì í•©**í•˜ì§€ë§Œ, **ê¸´ ì‹œí€€ìŠ¤ì—ì„œëŠ” LSTMì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ë„ ìžˆìŒ**.

**ê²°ê³¼ ê°’ì„ ì •ê·œí™”í•˜ëŠ” ë°©ë²•**ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ê°€ ìžˆìœ¼ë©°, ì£¼ìš” ë°©ë²•ìœ¼ë¡œ **L2 ì •ê·œí™”**ì™€ **ë°°ì¹˜ ì •ê·œí™”(Batch Normalization)**ê°€ ìžˆìŒ.  

---

## **ðŸ”¹ 1. L2 ì •ê·œí™” (L2 Regularization, Ridge Regularization)**
âœ… **ëª©ì :** ê°€ì¤‘ì¹˜(Weight)ê°€ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ì—¬ **ê³¼ì í•©(Overfitting) ë°©ì§€**  
âœ… **ì ìš© ìœ„ì¹˜:** ëª¨ë¸ì˜ **ê°€ì¤‘ì¹˜(Weight)ì— ì§ì ‘ ì ìš©**  
âœ… **ìˆ˜ì‹:**  
L2 ì •ê·œí™”ëŠ” ì†ì‹¤ í•¨ìˆ˜ì— **ê°€ì¤‘ì¹˜ì˜ ì œê³±í•©**ì„ ì¶”ê°€í•¨.  

\[
L_{new} = L + \lambda ||W||_2^2
\]

ì—¬ê¸°ì„œ,  
- \( L \) : ì›ëž˜ ì†ì‹¤ í•¨ìˆ˜ (ì˜ˆ: MSE, Cross-Entropy ë“±)  
- \( \lambda \) : ì •ê·œí™” ê°•ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°  
- \( ||W||_2^2 \) : ëª¨ë“  ê°€ì¤‘ì¹˜ì˜ L2 ë…¸ë¦„ (ì œê³±í•©)  

ðŸ’¡ **L2 ì •ê·œí™”ëŠ” ê°€ì¤‘ì¹˜ë¥¼ 0ì— ê°€ê¹ê²Œ ë§Œë“¤ì§€ë§Œ ì™„ì „ížˆ 0ìœ¼ë¡œ ë§Œë“¤ì§€ëŠ” ì•ŠìŒ.**  
ðŸ’¡ **ì£¼ë¡œ ì„ í˜• íšŒê·€, ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ ì •ê·œí™” ë“±ì— ì‚¬ìš©ë¨.**  

### **âœ… L2 ì •ê·œí™” ì ìš© ì˜ˆì œ (PyTorch)**
```python
import torch
import torch.nn as nn
import torch.optim as optim

# ê°„ë‹¨í•œ ì„ í˜• ëª¨ë¸
model = nn.Linear(10, 1)

# L2 ì •ê·œí™” ì ìš© (weight_decay ì‚¬ìš©)
optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)  # Î»=0.01
```

---

## **ðŸ”¹ 2. ë°°ì¹˜ ì •ê·œí™” (Batch Normalization, BN)**
âœ… **ëª©ì :** ì‹ ê²½ë§ í•™ìŠµ ì‹œ **ê° ì¸µì˜ ìž…ë ¥ ë¶„í¬ë¥¼ ì •ê·œí™”í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ì¦ê°€**  
âœ… **ì ìš© ìœ„ì¹˜:** **ì€ë‹‰ì¸µì˜ ì¶œë ¥ê°’(í™œì„±í™” í•¨ìˆ˜ ì´ì „ or ì´í›„)**  
âœ… **ìˆ˜ì‹:**  
ë°°ì¹˜ ì •ê·œí™”ëŠ” **ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ í‰ê· ê³¼ ë¶„ì‚°ì„ ì •ê·œí™”**í•¨.  

\[
\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
\]

\[
y = \gamma \hat{x} + \beta
\]

ì—¬ê¸°ì„œ,  
- \( \mu \), \( \sigma^2 \) : ë¯¸ë‹ˆë°°ì¹˜ì˜ í‰ê· ê³¼ ë¶„ì‚°  
- \( \epsilon \) : ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ ìž‘ì€ ê°’  
- \( \gamma \), \( \beta \) : í•™ìŠµ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼ë§/ì‹œí”„íŠ¸ íŒŒë¼ë¯¸í„°  

ðŸ’¡ **ë°°ì¹˜ ì •ê·œí™”ëŠ” ê¹Šì€ ì‹ ê²½ë§ì—ì„œ ì¤‘ìš”í•œ ê¸°ë²•ìœ¼ë¡œ, í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ê³  ê³¼ì í•©ì„ ì¤„ì´ëŠ” íš¨ê³¼ê°€ ìžˆìŒ.**  
ðŸ’¡ **CNN, RNN, Transformer ë“±ì˜ ë‹¤ì–‘í•œ ì‹ ê²½ë§ êµ¬ì¡°ì—ì„œ ì‚¬ìš©ë¨.**  

### **âœ… ë°°ì¹˜ ì •ê·œí™” ì ìš© ì˜ˆì œ (PyTorch)**
```python
import torch.nn as nn

# Fully Connected Layer + BatchNorm
model = nn.Sequential(
    nn.Linear(10, 20),
    nn.BatchNorm1d(20),  # 1D ë°ì´í„°ì— ëŒ€í•œ BatchNorm
    nn.ReLU()
)

# CNNì˜ ê²½ìš°
cnn_model = nn.Sequential(
    nn.Conv2d(3, 16, kernel_size=3, padding=1),
    nn.BatchNorm2d(16),  # 2D ë°ì´í„°ì— ëŒ€í•œ BatchNorm
    nn.ReLU()
)
```

---

## **ðŸ”¹ L2 ì •ê·œí™” vs ë°°ì¹˜ ì •ê·œí™”**
| ë¹„êµ í•­ëª© | L2 ì •ê·œí™” | ë°°ì¹˜ ì •ê·œí™” |
|-----------|----------|----------|
| **ëª©ì ** | ê³¼ì í•© ë°©ì§€ (ê°€ì¤‘ì¹˜ í¬ê¸° ì œí•œ) | í•™ìŠµ ì•ˆì •í™” ë° ì†ë„ í–¥ìƒ |
| **ì ìš© ìœ„ì¹˜** | ê°€ì¤‘ì¹˜ì— ì ìš© | ì€ë‹‰ì¸µì˜ ì¶œë ¥ê°’ ì •ê·œí™” |
| **ì •ê·œí™” ëŒ€ìƒ** | ê°€ì¤‘ì¹˜(Weight) | ë¯¸ë‹ˆë°°ì¹˜ì˜ ì¶œë ¥ê°’(Activation) |
| **ì¶”ë¡  ì‹œ ì ìš© ì—¬ë¶€** | X (í•™ìŠµ ì‹œì—ë§Œ ì ìš©ë¨) | O (í…ŒìŠ¤íŠ¸ ì‹œì—ë„ ì ìš©ë¨) |

---

## **ðŸ”¹ ì–¸ì œ ì–´ë–¤ ê±¸ ì‚¬ìš©í•´ì•¼ í• ê¹Œ?**
- **L2 ì •ê·œí™”:** ëª¨ë¸ì´ **ê³¼ì í•©ë  ê°€ëŠ¥ì„±ì´ ë†’ì„ ë•Œ ì‚¬ìš©**  
- **ë°°ì¹˜ ì •ê·œí™”:** **ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ì„œ í•™ìŠµì„ ë” ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©**  

L2 ì •ê·œí™”ì™€ ë°°ì¹˜ ì •ê·œí™”ëŠ” **ì„œë¡œ ë‹¤ë¥¸ ëª©ì **ì„ ê°€ì§€ë¯€ë¡œ, **í•¨ê»˜ ì‚¬ìš© ê°€ëŠ¥**í•¨.  
ì˜ˆë¥¼ ë“¤ì–´, CNNì´ë‚˜ RNN ëª¨ë¸ì—ì„œëŠ” ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•˜ê³ , ë™ì‹œì— L2 ì •ê·œí™”(weight decay)ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ìž„.

LSTM í•µì‹¬êµ¬ì„±ìš”ì†Œ
1. ë§ê° ê²Œì´íŠ¸
2. ìž…ë ¥ ê²Œì´íŠ¸
3. ì¶œë ¥ ê²Œì´íŠ¸
4. ì…€ ìƒíƒœ
ì €ëŠ” ì•„ì¹¨ì— ì±…ì„ ì½ìŠµë‹ˆë‹¤. -> ë‹¨ì–´ë‹¨ìœ„ë¡œ ì²˜ë¦¬
LSTM (ì €ëŠ”, ì•„ì¹¨ì—, ì±…ì„, ì½ìŠµë‹ˆë‹¤)

ì‹œí€€ìŠ¤ : ì €ëŠ” ì•„ì¹¨ì— ì±…ì„ (3ê°œë‹¨ì–´)
ëª©í‘œ : ì½ìŠµë‹ˆë‹¤ (ì˜ˆì¸¡)
x1 = [1, 0, 0] x2 = [0, 1, 0]  x3 = [0, 0, 1]
h0 = [0,0] c0=[0,0]
wf wi wo wc ê³ ì •ê°’ì‚¬ìš© ê°€ì • (ì‹¤ì œë¡œëŠ” ê³„ì‚°)

ë§ê°ê²Œì´íŠ¸ (ì €ëŠ”)
 ì—­í•  : ì´ì „ ì…€ìƒíƒœ coì—ì„œ ìžŠì„ ë¶€ë¶„ì„ ê²°ì •
ho=[0,0], x1=[1,0,0],wf=[[0.5,0.5],[0.1,0.2,0.3]], bf=0
f1 = a([0.5,0.5]o[0,0]+[0.1,0.2,0.3]o[1,0,0]) = a(0.1) = 0.52
f1ì€ ê³¼ê±°ì •ë³´ì˜ 52ë§Œ ìœ ì§€

ìž…ë ¥ê²Œì´íŠ¸
ex ê²°ê³¼ : 0.55
ìƒˆë¡œìš´ì •ë³´ì˜ 55%ë¥¼ ê¸°ì–µ

ë§ê°ê²Œì´íŠ¸ : 'ì €ëŠ”'ì˜ ì¼ë¶€ë§Œ ë‚¨ê¸°ê³  'ì•„ì¹¨ì—' ì™€ 'ì±…ì„'ì´ ë“¤ì–´ì˜¬ë•Œë§ˆë‹¤ ì¡°ì •
ìž…ë ¥ê²Œì´íŠ¸ : ìƒˆë‹¨ì–´('ì•„ì¹¨ì—','ì±…ì„')ë¥¼ ì…€ ìƒíƒœì— ì¡°ê¸ˆì”© ì¶”ê°€
ì…€ìƒíƒœ : ë©”ëª¨ë¦¬ì— 'ì €ëŠ” ì•„ì¹¨ì— ì±…ì„' ì •ë³´ë¥¼ ëˆ„ì 
ì¶œë ¥ê²Œì´íŠ¸ : ê° ë‹¨ê³„ì—ì„œ ë‹¤ìŒ ë‹¨ì–´ì˜ˆì¸¡ì— ì‚¬ìš©í•  ht ìƒì„±

## **ðŸ”¹ LSTM (Long Short-Term Memory) í•µì‹¬ ê°œë… ì •ë¦¬**  
LSTMì€ **ìž¥ê¸° ì˜ì¡´ì„±(Long-Term Dependency)**ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ìˆœí™˜ ì‹ ê²½ë§(RNN) êµ¬ì¡°ìž„.  

---

## **1ï¸âƒ£ LSTMì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ**  
LSTMì€ **4ê°€ì§€ í•µì‹¬ ìš”ì†Œ**ë¥¼ í†µí•´ ì •ë³´ë¥¼ ìœ ì§€í•˜ê³  ì—…ë°ì´íŠ¸í•¨.  

| ìš”ì†Œ | ì—­í•  |
|------|------|
| **ë§ê° ê²Œì´íŠ¸ (Forget Gate)** | ê³¼ê±° ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ìžŠì„ì§€ ê²°ì • |
| **ìž…ë ¥ ê²Œì´íŠ¸ (Input Gate)** | ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ê¸°ì–µí• ì§€ ê²°ì • |
| **ì…€ ìƒíƒœ (Cell State, C)** | ìž¥ê¸° ë©”ëª¨ë¦¬ ì—­í•  |
| **ì¶œë ¥ ê²Œì´íŠ¸ (Output Gate)** | í˜„ìž¬ ìƒíƒœì—ì„œ ì¶œë ¥í•  ì •ë³´ ê²°ì • |

---

## **2ï¸âƒ£ LSTMì˜ ë™ìž‘ ê³¼ì •**  

### **ðŸ”¹ (1) ìž…ë ¥ ì‹œí€€ìŠ¤ ì˜ˆì‹œ**  
**ë¬¸ìž¥:** "ì €ëŠ” ì•„ì¹¨ì— ì±…ì„ ì½ìŠµë‹ˆë‹¤."  
LSTMì€ **ë‹¨ì–´ ë‹¨ìœ„**ë¡œ ì²˜ë¦¬í•˜ë©°, ê° ë‹¨ì–´ë¥¼ **ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)** ë°©ì‹ìœ¼ë¡œ ë³€í™˜í•¨.  

\[
x_1 = [1,0,0], \quad x_2 = [0,1,0], \quad x_3 = [0,0,1]
\]

- **ëª©í‘œ:** "ì €ëŠ” ì•„ì¹¨ì— ì±…ì„" â†’ **"ì½ìŠµë‹ˆë‹¤"ë¥¼ ì˜ˆì¸¡**  
- **ì´ˆê¸° ìƒíƒœ:**  
  - \( h_0 = [0,0] \) (ì´ˆê¸° ì€ë‹‰ ìƒíƒœ)  
  - \( c_0 = [0,0] \) (ì´ˆê¸° ì…€ ìƒíƒœ)  

---

### **ðŸ”¹ (2) ë§ê° ê²Œì´íŠ¸ (Forget Gate)**  
- **ì´ì „ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ìžŠì„ì§€ ê²°ì •**  
- **ìž…ë ¥ ê°’:** ì´ì „ ì€ë‹‰ ìƒíƒœ \( h_{t-1} \), í˜„ìž¬ ë‹¨ì–´ \( x_t \)  
- **ì—°ì‚° ê³¼ì •:**  
  \[
  f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
  \]
- **ì˜ˆì œ ê³„ì‚°**  
  - \( h_0 = [0,0] \), \( x_1 = [1,0,0] \)  
  - \( W_f = [[0.5,0.5],[0.1,0.2,0.3]] \), \( b_f = 0 \)  
  - \( f_1 = a([0.5,0.5] \circ [0,0] + [0.1,0.2,0.3] \circ [1,0,0]) = a(0.1) = 0.52 \)  
  - **52%ì˜ ê³¼ê±° ì •ë³´ ìœ ì§€**  

---

### **ðŸ”¹ (3) ìž…ë ¥ ê²Œì´íŠ¸ (Input Gate)**  
- **ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ê¸°ì–µí• ì§€ ê²°ì •**  
- **ì—°ì‚° ê³¼ì •:**  
  \[
  i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
  \]
- **ì˜ˆì œ ê³„ì‚° ê²°ê³¼:**  
  \[
  i_1 = 0.55
  \]
  - **ìƒˆë¡œìš´ ì •ë³´ì˜ 55%ë¥¼ ê¸°ì–µ**  

---

### **ðŸ”¹ (4) ì…€ ìƒíƒœ ì—…ë°ì´íŠ¸**  
- **ë§ê° ê²Œì´íŠ¸ì™€ ìž…ë ¥ ê²Œì´íŠ¸ë¥¼ ë°˜ì˜í•˜ì—¬ ìƒˆë¡œìš´ ì…€ ìƒíƒœ \( C_t \) ê°±ì‹ **  
- **ì—°ì‚° ê³¼ì •:**  
  \[
  C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t
  \]
- **ì…€ ìƒíƒœì˜ ì—­í• **  
  - **ë§ê° ê²Œì´íŠ¸**: "ì €ëŠ”"ì˜ ì¼ë¶€ë§Œ ë‚¨ê¸°ê³  "ì•„ì¹¨ì—", "ì±…ì„"ì´ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ì¡°ì •  
  - **ìž…ë ¥ ê²Œì´íŠ¸**: ìƒˆ ë‹¨ì–´("ì•„ì¹¨ì—", "ì±…ì„")ë¥¼ ì…€ ìƒíƒœì— ì¶”ê°€  

---

### **ðŸ”¹ (5) ì¶œë ¥ ê²Œì´íŠ¸ (Output Gate) & ì€ë‹‰ ìƒíƒœ \( h_t \) ìƒì„±**  
- **ì¶œë ¥ ê²Œì´íŠ¸**ëŠ” ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  **ì€ë‹‰ ìƒíƒœ \( h_t \)**ë¥¼ ê²°ì •í•¨.  
- **ì—°ì‚° ê³¼ì •:**  
  \[
  o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
  \]
  \[
  h_t = o_t \cdot \tanh(C_t)
  \]
- **ì¶œë ¥ ê²Œì´íŠ¸ ì—­í• **  
  - í˜„ìž¬ê¹Œì§€ì˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ìœ„í•œ ë²¡í„° \( h_t \)** ìƒì„±  

---

## **ðŸ”¹ 3. ìš”ì•½**
1. **ë§ê° ê²Œì´íŠ¸**: ê³¼ê±° ì •ë³´ ì¤‘ ìœ ì§€í•  ë¶€ë¶„ ì„ íƒ  
2. **ìž…ë ¥ ê²Œì´íŠ¸**: ìƒˆë¡œìš´ ì •ë³´ë¥¼ ê¸°ì–µí•  ë¹„ìœ¨ ê²°ì •  
3. **ì…€ ìƒíƒœ**: ê¸°ì–µì„ ì§€ì†ì ìœ¼ë¡œ ê°±ì‹ í•˜ëŠ” ë©”ëª¨ë¦¬ ì—­í•   
4. **ì¶œë ¥ ê²Œì´íŠ¸**: ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  \( h_t \) ê²°ì •  

**âœ… LSTMì€ ìž¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ê´€ë¦¬í•˜ì—¬, ê¸´ ë¬¸ìž¥ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ìžƒì§€ ì•Šê³  íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìžˆìŒ!**

ê³ ê°ì˜ˆì¸¡
ë„ë©”ì¸ì§€ì‹ íŒŒì•…
ë°ì´í„°ì˜ ì •ë³´ë¥¼ ë³´ê¸°
ê²°ì¸¡ì¹˜ íŒŒì•…
ì–´ë–¤ì „ëžµìœ¼ë¡œ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•  ê²ƒì¸ì§€
ë°ì´í„°ì˜ ë¶„í¬ë¡œ íŒŒì•…
ì´ìƒì¹˜ íŒŒì•…

ì í•©í•œ ëª¨ë¸ì„ íŒŒì•…
ë‹¤ì–‘í•œ ëª¨ë¸ ì„ íƒ
í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
AUTUNA ë¼ì´ë¸ŒëŸ¬ë¦¬

feature ì—”ì§€ë‹ˆì–´ë§?
ë°ì´í„° ì¦ê°•ê¸°ë²•?
ìƒˆë¡œìš´ íŠ¹ì„± ì°¾ê¸°?

streamlitì„ ì´ìš©í•´ì„œ í™”ë©´ì„ í‘œì‹œ

# **ê³ ê° ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ í”„ë¡œì„¸ìŠ¤ (Step-by-Step)**  
ê³ ê° ì˜ˆì¸¡ ëª¨ë¸ì„ ê°œë°œí•˜ê¸° ìœ„í•´ì„œëŠ” **ë„ë©”ì¸ ì§€ì‹ íŒŒì•… â†’ ë°ì´í„° ì „ì²˜ë¦¬ â†’ ëª¨ë¸ ì„ íƒ â†’ ëª¨ë¸ ìµœì í™” â†’ ê²°ê³¼ ì‹œê°í™”**ì˜ ê³¼ì •ì„ ê±°ì³ì•¼ í•¨.  

---

## **1ï¸âƒ£ ë„ë©”ì¸ ì§€ì‹ íŒŒì•…**  
- **ëª©í‘œ:** ë°ì´í„°ë¥¼ ì œëŒ€ë¡œ í•´ì„í•˜ê³  ì ì ˆí•œ í”¼ì²˜(feature)ë¥¼ ì„ ì •í•˜ê¸° ìœ„í•´, **í•´ë‹¹ ì‚°ì—…(ë„ë©”ì¸)ì— ëŒ€í•œ ì´í•´**ê°€ í•„ìš”í•¨.  
- **ì£¼ìš” ì§ˆë¬¸:**  
  - ì˜ˆì¸¡í•˜ë ¤ëŠ” ê³ ê° í–‰ë™ì€ ë¬´ì—‡ì¸ê°€? (ì˜ˆ: êµ¬ë§¤ ì—¬ë¶€, ì´íƒˆ ê°€ëŠ¥ì„±, í´ë¦­ í™•ë¥  ë“±)  
  - ì¤‘ìš”í•œ ë³€ìˆ˜ëŠ” ë¬´ì—‡ì¸ê°€? (ì˜ˆ: ê³ ê° ë‚˜ì´, ë°©ë¬¸ ë¹ˆë„, êµ¬ë§¤ ì´ë ¥ ë“±)  
  - ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œëŠ” ë¬´ì—‡ì¸ê°€? (ì˜ˆ: ë§¤ì¶œ ì¦ê°€, ê³ ê° ìœ ì§€ìœ¨ í–¥ìƒ ë“±)  

---

## **2ï¸âƒ£ ë°ì´í„° íƒìƒ‰ ë° ì „ì²˜ë¦¬**  

### **ðŸ”¹ (1) ë°ì´í„°ì˜ ì •ë³´ë¥¼ ë³´ê¸°**  
- ë°ì´í„°ë¥¼ ë¡œë“œí•œ í›„, **ì „ì²´ì ì¸ êµ¬ì¡°ì™€ íŠ¹ì§•ì„ íŒŒì•…**í•´ì•¼ í•¨.  
- **ì£¼ìš” í™•ì¸ ì‚¬í•­**  
  - ë°ì´í„° í¬ê¸° (í–‰, ì—´ ê°œìˆ˜)  
  - ë³€ìˆ˜(ì»¬ëŸ¼)ë³„ ë°ì´í„° íƒ€ìž…  
  - ê²°ì¸¡ì¹˜ ë° ì´ìƒì¹˜ ì¡´ìž¬ ì—¬ë¶€  
  - íƒ€ê¹ƒ ë³€ìˆ˜ ë¶„í¬ (í´ëž˜ìŠ¤ ë¶ˆê· í˜• ì—¬ë¶€)  

```python
import pandas as pd

df = pd.read_csv("customer_data.csv")  
print(df.info())  # ë°ì´í„° íƒ€ìž… ë° ê²°ì¸¡ì¹˜ í™•ì¸
print(df.describe())  # ìˆ˜ì¹˜í˜• ë°ì´í„°ì˜ ê¸°ë³¸ í†µê³„ í™•ì¸
```

---

### **ðŸ”¹ (2) ê²°ì¸¡ì¹˜ íŒŒì•… ë° ì²˜ë¦¬**  
- ê²°ì¸¡ì¹˜(Missing Values)ëŠ” ëª¨ë¸ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìžˆìŒ.  
- ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•:  
  - **ì‚­ì œ**: ê²°ì¸¡ì¹˜ê°€ ì ë‹¤ë©´ í•´ë‹¹ í–‰(row) ë˜ëŠ” ì—´(column)ì„ ì œê±°  
  - **ëŒ€ì²´**: í‰ê· , ì¤‘ì•™ê°’, ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´í•˜ê±°ë‚˜, KNN ë˜ëŠ” íšŒê·€ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ëŒ€ì²´  
  - **ì˜ˆì¸¡ ë³€ìˆ˜í™”**: ê²°ì¸¡ì¹˜ë¥¼ í•˜ë‚˜ì˜ ìƒˆë¡œìš´ íŠ¹ì„±(feature)ìœ¼ë¡œ ì¶”ê°€  

```python
print(df.isnull().sum())  # ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸
df.fillna(df.median(), inplace=True)  # ê²°ì¸¡ì¹˜ë¥¼ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
```

---

### **ðŸ”¹ (3) ë°ì´í„° ë¶„í¬ ë° ì´ìƒì¹˜ íŒŒì•…**  
- ë°ì´í„° ë¶„í¬ë¥¼ í™•ì¸í•˜ì—¬ **ì™œê³¡(skewness)** ì—¬ë¶€ë¥¼ íŒŒì•…  
- ì´ìƒì¹˜(Outliers) ì²˜ë¦¬ ë°©ë²•  
  - **IQR (Interquartile Range) ë°©ë²•**: ì‚¬ë¶„ìœ„ìˆ˜ë¥¼ ì‚¬ìš©í•´ ê·¹ë‹¨ì ì¸ ê°’ ì œê±°  
  - **Z-score**ë¥¼ ì´ìš©í•œ ì´ìƒì¹˜ íƒì§€  

```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(df["customer_age"], bins=30, kde=True)  # ë‚˜ì´ ë¶„í¬ í™•ì¸
plt.show()

# IQRì„ ì´ìš©í•œ ì´ìƒì¹˜ ì œê±°
Q1 = df["purchase_amount"].quantile(0.25)
Q3 = df["purchase_amount"].quantile(0.75)
IQR = Q3 - Q1

df = df[(df["purchase_amount"] >= Q1 - 1.5 * IQR) & (df["purchase_amount"] <= Q3 + 1.5 * IQR)]
```

---

## **3ï¸âƒ£ ëª¨ë¸ ì„ ì • ë° í•™ìŠµ**  

### **ðŸ”¹ (1) ì í•©í•œ ëª¨ë¸ íŒŒì•…**  
- **íšŒê·€(Regression) ëª¨ë¸**: ì—°ì†ëœ ìˆ˜ì¹˜(ì˜ˆ: ê³ ê° ì§€ì¶œ ê¸ˆì•¡) ì˜ˆì¸¡  
- **ë¶„ë¥˜(Classification) ëª¨ë¸**: íŠ¹ì • í´ëž˜ìŠ¤(ì˜ˆ: êµ¬ë§¤í• ì§€ ì•ˆ í• ì§€) ì˜ˆì¸¡  

| ëª¨ë¸ | íŠ¹ì§• |
|------|------|
| **ë¡œì§€ìŠ¤í‹± íšŒê·€** | ê°„ë‹¨í•œ ë¶„ë¥˜ ë¬¸ì œì— ì í•© |
| **ëžœë¤ í¬ë ˆìŠ¤íŠ¸** | ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„ ë° ê°•ê±´í•œ ì„±ëŠ¥ |
| **XGBoost / LightGBM** | ë¹ ë¥¸ ê³„ì‚° ì†ë„ì™€ ë†’ì€ ì„±ëŠ¥ |
| **ë”¥ëŸ¬ë‹ (LSTM, CNN)** | ì‹œê³„ì—´ ë°ì´í„° ë° ë³µìž¡í•œ íŒ¨í„´ ë¶„ì„ì— ê°•í•¨ |

---

### **ðŸ”¹ (2) ë‹¤ì–‘í•œ ëª¨ë¸ ì„ íƒ ë° ë¹„êµ**  
- ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , **ì„±ëŠ¥ ë¹„êµ í›„ ìµœì  ëª¨ë¸ì„ ì„ íƒ**  
- ì˜ˆì œ: ë¡œì§€ìŠ¤í‹± íšŒê·€ vs ëžœë¤ í¬ë ˆìŠ¤íŠ¸ vs XGBoost  

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=["target"]), df["target"], test_size=0.2, random_state=42)

models = {
    "Random Forest": RandomForestClassifier(),
    "XGBoost": XGBClassifier()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"{name} ì •í™•ë„: {accuracy_score(y_test, y_pred):.4f}")
```

---

### **ðŸ”¹ (3) í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (AutoML í™œìš© ê°€ëŠ¥)**  
- **AUTOTUNA**: ìžë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬  
- **GridSearchCV ë˜ëŠ” Bayesian Optimization**ì„ ì‚¬ìš©í•˜ì—¬ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°  

```python
from autogluon.tabular import TabularPredictor

predictor = TabularPredictor(label="target").fit(df)
predictor.leaderboard(df)
```

---

## **4ï¸âƒ£ Feature Engineering & ë°ì´í„° ì¦ê°•**  
- ëª¨ë¸ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§(Feature Engineering)** í•„ìš”  

### **ðŸ”¹ (1) ìƒˆë¡œìš´ íŠ¹ì„± ì°¾ê¸° (Feature Engineering)**  
- ì˜ˆ: ê³ ê° ë°©ë¬¸ ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ê³ ê° ë“±ê¸‰ ìƒì„±**  
- **ê¸°ì¡´ ë³€ìˆ˜ ì¡°í•©**ì„ í†µí•´ ìƒˆë¡œìš´ íŠ¹ì§• ìƒì„±  

```python
df["visit_per_spend"] = df["total_spend"] / (df["visit_count"] + 1)
```

---

### **ðŸ”¹ (2) ë°ì´í„° ì¦ê°• (Data Augmentation)**  
- ë°ì´í„° ì–‘ì´ ì ì„ ê²½ìš°, ë‹¤ì–‘í•œ ê¸°ë²•ì„ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ **ìƒì„±**  
- **SMOTE ê¸°ë²•**ì„ ì´ìš©í•œ í´ëž˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°  

```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy='auto')
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```

---

## **5ï¸âƒ£ Streamlitì„ ì´ìš©í•œ ê²°ê³¼ ì‹œê°í™”**  
- **Streamlitì„ í™œìš©í•˜ì—¬ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›¹ í™”ë©´ì— í‘œì‹œ**  

```python
import streamlit as st

st.title("ê³ ê° ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ")

customer_id = st.text_input("ê³ ê° IDë¥¼ ìž…ë ¥í•˜ì„¸ìš”:")
if st.button("ì˜ˆì¸¡ ì‹¤í–‰"):
    pred = predictor.predict(df[df["customer_id"] == customer_id])
    st.write(f"ì´ ê³ ê°ì˜ êµ¬ë§¤ ê°€ëŠ¥ì„±: {pred}")
```

---

## **âœ… ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬**  
1ï¸âƒ£ **ë„ë©”ì¸ ì§€ì‹ íŒŒì•…** â†’ ì–´ë–¤ ë°ì´í„°ë¥¼ ì‚¬ìš©í• ì§€ ì´í•´  
2ï¸âƒ£ **ë°ì´í„° íƒìƒ‰ ë° ì „ì²˜ë¦¬** â†’ ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ë°ì´í„° ë¶„í¬ í™•ì¸  
3ï¸âƒ£ **ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ** â†’ ë‹¤ì–‘í•œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ìµœì í™”  
4ï¸âƒ£ **Feature Engineering & ë°ì´í„° ì¦ê°•** â†’ ìƒˆë¡œìš´ íŠ¹ì„± ì¶”ê°€ ë° ë°ì´í„° ë³´ê°•  
5ï¸âƒ£ **ê²°ê³¼ ì‹œê°í™” (Streamlit í™œìš©)** â†’ ëŒ€ì‹œë³´ë“œ êµ¬í˜„  

**ì´ í”„ë¡œì„¸ìŠ¤ë¥¼ ë”°ë¼ê°€ë©´ ê³ ê° ì˜ˆì¸¡ ëª¨ë¸ì„ ì²´ê³„ì ìœ¼ë¡œ êµ¬ì¶• ê°€ëŠ¥!** ðŸš€
