IMDB 리뷰 데이터셋
RNN
기울기 클리핑
LSTM셀 
장단기 메모리
게이트
기억하려는 게이트, 망각하려는 게이트

RNN의 문제점
```
기울기 소실(Vanishing Gradient)
 역전파 과정에서 손실이 과거시점으로 전달될때 기울기 값이 점점 작아지면서 0에 가까워지고 결국 학습이 안되는 현상
 - 학습과정에서 파악하기 어려움
기울기 폭발(Expoding Gradient)
 특정상황에서 기울기 갑이 너무 커져서 학습이 불아정해지는 현상
 코드상으로 기울기 클리핑이 사용
장기 의존성 학습의 어려움
 데이터의 시퀀스가 길어질수록 초기 입력정보가 점점희미해지는 현상으로 초기 특성을 반영하지 못함
단순한 정보 저장
```
LSTM
```
 RNN과 동일하게 시퀀스를 처리, 셀상태와 게이트개념을 도입 망각 게이트
 망각게이트
  이진 셀 상태에서 삭제할 정보를 결정(0~1) 0은 잊고 1이면 유지
```
```
입력게이트
  새로운 정보를 얼마나 저장할지 결정
  1 새로운 정보를 저장, 0이면 저장안
```
```
출력게이트
  최종적으로 어떤 정보를 은닉상태에 담아 다음 단계로 넘길지 결정하는 게이트
```
```
lstm의 정보처리 순서
  과거정보에서 버릴것은 버리고 (Foget Gate)
  새로운 정보를 받아들이고(Input Gate)
  셀 상태 업데이트
  업데이트된 정보를 저장한뒤(Output Gate)
  최종 결과를 출력
```

### **📌 RNN의 문제점 및 LSTM의 해결 방법을 예제와 함께 설명**

#### **1️⃣ RNN(Recurrent Neural Network) 문제점**

✅ **기울기 소실(Vanishing Gradient)**  
- 역전파 시 과거로 갈수록 기울기가 점점 작아져 0에 가까워지면서 학습이 어려워지는 문제.

✅ **기울기 폭발(Exploding Gradient)**  
- 특정 상황에서 기울기가 너무 커져서 학습이 불안정해지는 문제.  
- 해결 방법: **기울기 클리핑(Gradient Clipping)** 사용.

✅ **장기 의존성 학습 어려움**  
- 입력 데이터가 길어질수록 초기 정보가 희미해져 중요한 패턴을 학습하지 못하는 문제.

✅ **단순한 정보 저장**  
- RNN은 단순한 은닉 상태만 유지하며 장기 기억을 하지 못함.

---

#### **2️⃣ LSTM(Long Short-Term Memory)으로 해결**
- **LSTM은 RNN과 동일하게 시퀀스를 처리하지만, 셀 상태(Cell State)와 게이트(Gate) 개념을 도입.**
- **게이트를 통해 중요한 정보를 기억하고, 필요 없는 정보를 제거하는 방식으로 학습.**  

#### **📌 LSTM의 정보 흐름**
1. **Forget Gate(망각 게이트)**
   - 과거 정보에서 버릴 것을 선택.
   - 0(잊음) ~ 1(유지)의 값을 가짐.

2. **Input Gate(입력 게이트)**
   - 새로운 정보를 저장할지 결정.
   - 0(저장 X) ~ 1(저장)의 값을 가짐.

3. **Cell State(셀 상태) 업데이트**
   - 기존 정보를 새 정보로 업데이트.

4. **Output Gate(출력 게이트)**
   - 최종 은닉 상태를 결정.

---

### **📌 RNN과 LSTM 비교를 위한 실습 예제**
Python을 사용해 IMDB 리뷰 데이터셋을 기반으로 RNN과 LSTM을 비교.

✔ 위 코드에서는 IMDB 리뷰 데이터셋을 사용해 **RNN 모델과 LSTM 모델을 각각 정의**했음.  
✔ 두 모델의 차이점을 학습하고, LSTM이 RNN보다 기울기 소실 문제를 해결하며 장기 의존성을 더 잘 학습하는지 확인 가능함.  

추가적으로 학습 후 성능을 비교하는 코드도 원한다면 요청 바람.
### **IMDB 리뷰 데이터셋 + RNN + LSTM 개념 예제**

IMDB 리뷰 데이터셋을 사용하여 RNN과 LSTM을 비교하면서 개념을 하나씩 학습하는 과정을 진행함.

---

### **1️⃣ RNN을 이용한 텍스트 감성 분석 (IMDB 리뷰 데이터셋 사용)**

#### **🔹 데이터 로드 및 전처리**
```python
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

# IMDB 데이터 로드 (상위 10,000개의 단어만 사용)
num_words = 10000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)

# 패딩 처리 (문장의 길이를 동일하게 맞추기 위해)
max_len = 200  # 한 문장의 최대 길이
x_train = pad_sequences(x_train, maxlen=max_len)
x_test = pad_sequences(x_test, maxlen=max_len)
```
- `imdb.load_data(num_words=10000)`: 자주 등장하는 10,000개의 단어만 사용.
- `pad_sequences()`: 문장의 길이를 `max_len=200`으로 맞춤.

---

#### **🔹 RNN 모델 구축**
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Embedding, Dense

# RNN 모델 생성
model_rnn = Sequential([
    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),  # 단어를 128차원 벡터로 변환
    SimpleRNN(64, return_sequences=False),  # 은닉 상태 크기 64
    Dense(1, activation='sigmoid')  # 감성 분석이므로 1개의 출력을 sigmoid로 분류
])

# 모델 컴파일
model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# 모델 학습
history = model_rnn.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))
```
- `Embedding()`: 단어를 벡터로 변환.
- `SimpleRNN()`: 일반적인 RNN 층 사용.
- `Dense(1, activation='sigmoid')`: 이진 감성 분류 (긍정/부정).

🔹 **문제점:**
- **기울기 소실 (Vanishing Gradient)** → 오래된 입력 정보가 사라짐.
- **장기 의존성 문제** → 긴 문장을 학습할 때 앞부분을 잘 기억하지 못함.

---

### **2️⃣ LSTM을 활용하여 문제 해결**

#### **🔹 LSTM 모델 구축**
```python
from tensorflow.keras.layers import LSTM

# LSTM 모델 생성
model_lstm = Sequential([
    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),
    LSTM(64, return_sequences=False),
    Dense(1, activation='sigmoid')
])

# 모델 컴파일 및 학습
model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history_lstm = model_lstm.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))
```
🔹 **LSTM이 해결하는 문제:**
- **기울기 소실 문제 방지** (장기 기억 가능).
- **장기 의존성 해결** (이전 단어와 멀리 떨어진 단어도 학습 가능).

---

### **3️⃣ LSTM의 내부 동작 예제**
LSTM은 **Forget Gate, Input Gate, Output Gate**를 사용하여 정보를 조절함.

#### **🔹 한 스텝의 연산 과정**
1. **Forget Gate (망각 게이트)**  
   - 과거 정보를 버릴 것인지 결정.
   - 예제: "이 영화는 정말 [좋았다]!" → "정말"을 기억하고 나머지는 버릴 수도 있음.

2. **Input Gate (입력 게이트)**
   - 새로운 정보를 저장할지 결정.
   - 예제: "배우 연기가 [탁월하다]" → "탁월하다"를 기억.

3. **Output Gate (출력 게이트)**
   - 최종적으로 은닉 상태로 전달할 정보를 결정.
   - 예제: "스토리가 [뛰어났다]" → "뛰어났다"를 다음 단계로 전달.

#### **🔹 LSTM 게이트 동작 코드**
```python
import numpy as np

# 입력값 (단어 벡터)
x_t = np.array([0.2, 0.5, 0.1])  # 예제 단어 벡터

# 가중치 (임의의 값)
Wf, Wi, Wo, Wc = np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3)
Uf, Ui, Uo, Uc = np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3)
bf, bi, bo, bc = np.random.randn(3), np.random.randn(3), np.random.randn(3), np.random.randn(3)

# 이전 상태값
h_prev = np.array([0.1, 0.2, 0.3])
c_prev = np.array([0.4, 0.5, 0.6])

# 게이트 연산
forget_gate = tf.sigmoid(np.dot(Wf, x_t) + np.dot(Uf, h_prev) + bf)
input_gate = tf.sigmoid(np.dot(Wi, x_t) + np.dot(Ui, h_prev) + bi)
output_gate = tf.sigmoid(np.dot(Wo, x_t) + np.dot(Uo, h_prev) + bo)
candidate_memory = np.tanh(np.dot(Wc, x_t) + np.dot(Uc, h_prev) + bc)

# 새로운 셀 상태
c_t = forget_gate * c_prev + input_gate * candidate_memory

# 새로운 은닉 상태
h_t = output_gate * np.tanh(c_t)

print("Forget Gate Output:", forget_gate.numpy())
print("Input Gate Output:", input_gate.numpy())
print("Output Gate Output:", output_gate.numpy())
print("New Memory Cell:", c_t)
print("New Hidden State:", h_t)
```
🔹 **이 코드의 의미:**
- `forget_gate` → 과거 기억 중 어떤 걸 유지할지 결정.
- `input_gate` → 새로운 입력 중 어떤 걸 저장할지 결정.
- `output_gate` → 최종 출력 정보를 결정.

---

### **4️⃣ 기울기 클리핑 (Gradient Clipping)**
🔹 **기울기 폭발 방지를 위해 사용**
```python
from tensorflow.keras.optimizers import Adam

# Adam 옵티마이저 사용하면서 기울기 클리핑 적용
optimizer = Adam(learning_rate=0.001, clipnorm=1.0)

model_lstm.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
```
- `clipnorm=1.0`: 기울기가 1을 넘지 않도록 조정하여 폭발 방지.

---

### **5️⃣ RNN과 LSTM 성능 비교**
```python
rnn_acc = model_rnn.evaluate(x_test, y_test, verbose=0)[1]
lstm_acc = model_lstm.evaluate(x_test, y_test, verbose=0)[1]

print(f"RNN Test Accuracy: {rnn_acc:.4f}")
print(f"LSTM Test Accuracy: {lstm_acc:.4f}")
```
**예상 결과:**  
- RNN: 낮은 정확도 (장기 의존성 문제)
- LSTM: 높은 정확도 (장기 기억 가능)

---

### **🔹 결론**
- RNN은 `기울기 소실`과 `장기 의존성 문제`로 인해 긴 문장 처리에 어려움이 있음.
- LSTM은 **Forget/Input/Output 게이트**를 통해 장기 의존성을 해결하고 더 나은 성능을 보임.
- `기울기 클리핑`을 통해 `기울기 폭발`을 방지할 수 있음.

이렇게 개념별로 예제와 함께 실습 가능함. 🚀

GRU(Gated Recurrent Unit) 셀은 순환신경망(RNN)의 한 종류로, **LSTM(Long Short-Term Memory)**의 단순화된 버전임.  

### **🔹 GRU의 핵심 개념**
GRU는 **장기 의존성(long-term dependency)** 문제를 해결하기 위해 **게이트(gate) 구조**를 활용함. LSTM과 비교하여 **더 적은 파라미터를 사용하면서도 유사한 성능을 냄**.  

---

### **🔹 GRU 구조**
GRU 셀에는 **두 개의 게이트**가 있음.  
1. **리셋 게이트(Reset Gate, \( r_t \))**  
   - 이전 상태를 얼마나 잊을지 결정함.  
2. **업데이트 게이트(Update Gate, \( z_t \))**  
   - 새로운 정보와 이전 정보를 얼마나 반영할지 조절함.  

📌 **GRU는 LSTM과 달리 `셀 상태(Cell State)`가 따로 없고, 은닉 상태(\( h_t \))만 유지함.**  

---

### **🔹 GRU 수식**
GRU의 동작 방식은 다음과 같음.

1. **리셋 게이트 계산**  
   \[
   r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)
   \]
   
2. **업데이트 게이트 계산**  
   \[
   z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)
   \]
   
3. **새로운 후보 은닉 상태 계산**  
   \[
   \tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)
   \]
   
4. **최종 은닉 상태 업데이트**  
   \[
   h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
   \]

여기서,  
- \( \sigma \) : 시그모이드 함수 (0~1 사이 값)  
- \( \odot \) : 요소별 곱셈 (element-wise multiplication)  
- \( W_r, W_z, W_h \) : 학습 가능한 가중치 행렬  
- \( b_r, b_z, b_h \) : 편향 벡터  

---

### **🔹 GRU vs LSTM 차이점**
| 비교 항목 | GRU | LSTM |
|-----------|------|------|
| 게이트 수 | 2개 (리셋, 업데이트) | 3개 (입력, 출력, 망각) |
| 셀 상태 | 없음 (은닉 상태만 존재) | 셀 상태와 은닉 상태 분리 |
| 계산 비용 | 낮음 | 상대적으로 높음 |
| 성능 | 비슷하거나 더 좋음 (작은 데이터셋) | 긴 시퀀스에서 안정적 |

---

### **🔹 GRU 사용 예제 (PyTorch)**
```python
import torch
import torch.nn as nn

# GRU 모델 정의
gru = nn.GRU(input_size=10, hidden_size=20, num_layers=1, batch_first=True)

# 더미 입력 데이터 (배치 크기: 5, 시퀀스 길이: 3, 입력 크기: 10)
x = torch.randn(5, 3, 10)

# 초기 은닉 상태 (레이어 수, 배치 크기, 은닉 크기)
h0 = torch.zeros(1, 5, 20)

# GRU 실행
output, hn = gru(x, h0)

print(output.shape)  # (5, 3, 20) -> 마지막 은닉 상태 반환
print(hn.shape)      # (1, 5, 20) -> 마지막 시점의 은닉 상태
```

---

### **🔹 GRU 요약**
✅ **장기 의존성 문제 해결**  
✅ **LSTM보다 구조가 단순하여 계산량이 적음**  
✅ **작은 데이터셋에서는 LSTM보다 더 나은 성능을 낼 수도 있음**  
✅ **기계 번역, 음성 인식, 시계열 데이터 분석 등에 활용됨**  

GRU는 **연산량이 적고 빠른 학습이 필요한 경우 적합**하지만, **긴 시퀀스에서는 LSTM이 더 나은 성능을 낼 수도 있음**.

**결과 값을 정규화하는 방법**에는 여러 가지가 있으며, 주요 방법으로 **L2 정규화**와 **배치 정규화(Batch Normalization)**가 있음.  

---

## **🔹 1. L2 정규화 (L2 Regularization, Ridge Regularization)**
✅ **목적:** 가중치(Weight)가 너무 커지는 것을 방지하여 **과적합(Overfitting) 방지**  
✅ **적용 위치:** 모델의 **가중치(Weight)에 직접 적용**  
✅ **수식:**  
L2 정규화는 손실 함수에 **가중치의 제곱합**을 추가함.  

\[
L_{new} = L + \lambda ||W||_2^2
\]

여기서,  
- \( L \) : 원래 손실 함수 (예: MSE, Cross-Entropy 등)  
- \( \lambda \) : 정규화 강도를 조절하는 하이퍼파라미터  
- \( ||W||_2^2 \) : 모든 가중치의 L2 노름 (제곱합)  

💡 **L2 정규화는 가중치를 0에 가깝게 만들지만 완전히 0으로 만들지는 않음.**  
💡 **주로 선형 회귀, 신경망의 가중치 정규화 등에 사용됨.**  

### **✅ L2 정규화 적용 예제 (PyTorch)**
```python
import torch
import torch.nn as nn
import torch.optim as optim

# 간단한 선형 모델
model = nn.Linear(10, 1)

# L2 정규화 적용 (weight_decay 사용)
optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)  # λ=0.01
```

---

## **🔹 2. 배치 정규화 (Batch Normalization, BN)**
✅ **목적:** 신경망 학습 시 **각 층의 입력 분포를 정규화하여 학습 속도를 증가**  
✅ **적용 위치:** **은닉층의 출력값(활성화 함수 이전 or 이후)**  
✅ **수식:**  
배치 정규화는 **미니배치 단위로 평균과 분산을 정규화**함.  

\[
\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
\]

\[
y = \gamma \hat{x} + \beta
\]

여기서,  
- \( \mu \), \( \sigma^2 \) : 미니배치의 평균과 분산  
- \( \epsilon \) : 수치적 안정성을 위한 작은 값  
- \( \gamma \), \( \beta \) : 학습 가능한 스케일링/시프트 파라미터  

💡 **배치 정규화는 깊은 신경망에서 중요한 기법으로, 학습 속도를 높이고 과적합을 줄이는 효과가 있음.**  
💡 **CNN, RNN, Transformer 등의 다양한 신경망 구조에서 사용됨.**  

### **✅ 배치 정규화 적용 예제 (PyTorch)**
```python
import torch.nn as nn

# Fully Connected Layer + BatchNorm
model = nn.Sequential(
    nn.Linear(10, 20),
    nn.BatchNorm1d(20),  # 1D 데이터에 대한 BatchNorm
    nn.ReLU()
)

# CNN의 경우
cnn_model = nn.Sequential(
    nn.Conv2d(3, 16, kernel_size=3, padding=1),
    nn.BatchNorm2d(16),  # 2D 데이터에 대한 BatchNorm
    nn.ReLU()
)
```

---

## **🔹 L2 정규화 vs 배치 정규화**
| 비교 항목 | L2 정규화 | 배치 정규화 |
|-----------|----------|----------|
| **목적** | 과적합 방지 (가중치 크기 제한) | 학습 안정화 및 속도 향상 |
| **적용 위치** | 가중치에 적용 | 은닉층의 출력값 정규화 |
| **정규화 대상** | 가중치(Weight) | 미니배치의 출력값(Activation) |
| **추론 시 적용 여부** | X (학습 시에만 적용됨) | O (테스트 시에도 적용됨) |

---

## **🔹 언제 어떤 걸 사용해야 할까?**
- **L2 정규화:** 모델이 **과적합될 가능성이 높을 때 사용**  
- **배치 정규화:** **딥러닝 모델에서 학습을 더 빠르고 안정적으로 진행하고 싶을 때 사용**  

L2 정규화와 배치 정규화는 **서로 다른 목적**을 가지므로, **함께 사용 가능**함.  
예를 들어, CNN이나 RNN 모델에서는 배치 정규화를 적용하고, 동시에 L2 정규화(weight decay)를 추가하는 것이 일반적임.

LSTM 핵심구성요소
1. 망각 게이트
2. 입력 게이트
3. 출력 게이트
4. 셀 상태
저는 아침에 책을 읽습니다. -> 단어단위로 처리
LSTM (저는, 아침에, 책을, 읽습니다)

시퀀스 : 저는 아침에 책을 (3개단어)
목표 : 읽습니다 (예측)
x1 = [1, 0, 0] x2 = [0, 1, 0]  x3 = [0, 0, 1]
h0 = [0,0] c0=[0,0]
wf wi wo wc 고정값사용 가정 (실제로는 계산)

망각게이트 (저는)
 역할 : 이전 셀상태 co에서 잊을 부분을 결정
ho=[0,0], x1=[1,0,0],wf=[[0.5,0.5],[0.1,0.2,0.3]], bf=0
f1 = a([0.5,0.5]o[0,0]+[0.1,0.2,0.3]o[1,0,0]) = a(0.1) = 0.52
f1은 과거정보의 52만 유지

입력게이트
ex 결과 : 0.55
새로운정보의 55%를 기억

망각게이트 : '저는'의 일부만 남기고 '아침에' 와 '책을'이 들어올때마다 조정
입력게이트 : 새단어('아침에','책을')를 셀 상태에 조금씩 추가
셀상태 : 메모리에 '저는 아침에 책을' 정보를 누적
출력게이트 : 각 단계에서 다음 단어예측에 사용할 ht 생성

## **🔹 LSTM (Long Short-Term Memory) 핵심 개념 정리**  
LSTM은 **장기 의존성(Long-Term Dependency)**을 효과적으로 학습하는 순환 신경망(RNN) 구조임.  

---

## **1️⃣ LSTM의 주요 구성 요소**  
LSTM은 **4가지 핵심 요소**를 통해 정보를 유지하고 업데이트함.  

| 요소 | 역할 |
|------|------|
| **망각 게이트 (Forget Gate)** | 과거 정보를 얼마나 잊을지 결정 |
| **입력 게이트 (Input Gate)** | 새로운 정보를 얼마나 기억할지 결정 |
| **셀 상태 (Cell State, C)** | 장기 메모리 역할 |
| **출력 게이트 (Output Gate)** | 현재 상태에서 출력할 정보 결정 |

---

## **2️⃣ LSTM의 동작 과정**  

### **🔹 (1) 입력 시퀀스 예시**  
**문장:** "저는 아침에 책을 읽습니다."  
LSTM은 **단어 단위**로 처리하며, 각 단어를 **원-핫 인코딩(One-Hot Encoding)** 방식으로 변환함.  

\[
x_1 = [1,0,0], \quad x_2 = [0,1,0], \quad x_3 = [0,0,1]
\]

- **목표:** "저는 아침에 책을" → **"읽습니다"를 예측**  
- **초기 상태:**  
  - \( h_0 = [0,0] \) (초기 은닉 상태)  
  - \( c_0 = [0,0] \) (초기 셀 상태)  

---

### **🔹 (2) 망각 게이트 (Forget Gate)**  
- **이전 정보를 얼마나 잊을지 결정**  
- **입력 값:** 이전 은닉 상태 \( h_{t-1} \), 현재 단어 \( x_t \)  
- **연산 과정:**  
  \[
  f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
  \]
- **예제 계산**  
  - \( h_0 = [0,0] \), \( x_1 = [1,0,0] \)  
  - \( W_f = [[0.5,0.5],[0.1,0.2,0.3]] \), \( b_f = 0 \)  
  - \( f_1 = a([0.5,0.5] \circ [0,0] + [0.1,0.2,0.3] \circ [1,0,0]) = a(0.1) = 0.52 \)  
  - **52%의 과거 정보 유지**  

---

### **🔹 (3) 입력 게이트 (Input Gate)**  
- **새로운 정보를 얼마나 기억할지 결정**  
- **연산 과정:**  
  \[
  i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
  \]
- **예제 계산 결과:**  
  \[
  i_1 = 0.55
  \]
  - **새로운 정보의 55%를 기억**  

---

### **🔹 (4) 셀 상태 업데이트**  
- **망각 게이트와 입력 게이트를 반영하여 새로운 셀 상태 \( C_t \) 갱신**  
- **연산 과정:**  
  \[
  C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t
  \]
- **셀 상태의 역할**  
  - **망각 게이트**: "저는"의 일부만 남기고 "아침에", "책을"이 들어올 때마다 조정  
  - **입력 게이트**: 새 단어("아침에", "책을")를 셀 상태에 추가  

---

### **🔹 (5) 출력 게이트 (Output Gate) & 은닉 상태 \( h_t \) 생성**  
- **출력 게이트**는 다음 단계에서 사용할 **은닉 상태 \( h_t \)**를 결정함.  
- **연산 과정:**  
  \[
  o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
  \]
  \[
  h_t = o_t \cdot \tanh(C_t)
  \]
- **출력 게이트 역할**  
  - 현재까지의 정보를 기반으로 **다음 단어 예측을 위한 벡터 \( h_t \)** 생성  

---

## **🔹 3. 요약**
1. **망각 게이트**: 과거 정보 중 유지할 부분 선택  
2. **입력 게이트**: 새로운 정보를 기억할 비율 결정  
3. **셀 상태**: 기억을 지속적으로 갱신하는 메모리 역할  
4. **출력 게이트**: 다음 단계에서 사용할 \( h_t \) 결정  

**✅ LSTM은 장기 메모리를 관리하여, 긴 문장에서 중요한 정보를 잃지 않고 효과적으로 처리할 수 있음!**

고객예측
도메인지식 파악
데이터의 정보를 보기
결측치 파악
어떤전략으로 결측치를 처리할 것인지
데이터의 분포로 파악
이상치 파악

적합한 모델을 파악
다양한 모델 선택
하이퍼파라미터 튜닝
AUTUNA 라이브러리

feature 엔지니어링?
데이터 증강기법?
새로운 특성 찾기?

streamlit을 이용해서 화면을 표시

# **고객 예측 모델 개발 프로세스 (Step-by-Step)**  
고객 예측 모델을 개발하기 위해서는 **도메인 지식 파악 → 데이터 전처리 → 모델 선택 → 모델 최적화 → 결과 시각화**의 과정을 거쳐야 함.  

---

## **1️⃣ 도메인 지식 파악**  
- **목표:** 데이터를 제대로 해석하고 적절한 피처(feature)를 선정하기 위해, **해당 산업(도메인)에 대한 이해**가 필요함.  
- **주요 질문:**  
  - 예측하려는 고객 행동은 무엇인가? (예: 구매 여부, 이탈 가능성, 클릭 확률 등)  
  - 중요한 변수는 무엇인가? (예: 고객 나이, 방문 빈도, 구매 이력 등)  
  - 비즈니스 목표는 무엇인가? (예: 매출 증가, 고객 유지율 향상 등)  

---

## **2️⃣ 데이터 탐색 및 전처리**  

### **🔹 (1) 데이터의 정보를 보기**  
- 데이터를 로드한 후, **전체적인 구조와 특징을 파악**해야 함.  
- **주요 확인 사항**  
  - 데이터 크기 (행, 열 개수)  
  - 변수(컬럼)별 데이터 타입  
  - 결측치 및 이상치 존재 여부  
  - 타깃 변수 분포 (클래스 불균형 여부)  

```python
import pandas as pd

df = pd.read_csv("customer_data.csv")  
print(df.info())  # 데이터 타입 및 결측치 확인
print(df.describe())  # 수치형 데이터의 기본 통계 확인
```

---

### **🔹 (2) 결측치 파악 및 처리**  
- 결측치(Missing Values)는 모델 성능에 큰 영향을 미칠 수 있음.  
- 결측치 처리 방법:  
  - **삭제**: 결측치가 적다면 해당 행(row) 또는 열(column)을 제거  
  - **대체**: 평균, 중앙값, 최빈값으로 대체하거나, KNN 또는 회귀 모델을 이용하여 대체  
  - **예측 변수화**: 결측치를 하나의 새로운 특성(feature)으로 추가  

```python
print(df.isnull().sum())  # 컬럼별 결측치 개수 확인
df.fillna(df.median(), inplace=True)  # 결측치를 중앙값으로 대체
```

---

### **🔹 (3) 데이터 분포 및 이상치 파악**  
- 데이터 분포를 확인하여 **왜곡(skewness)** 여부를 파악  
- 이상치(Outliers) 처리 방법  
  - **IQR (Interquartile Range) 방법**: 사분위수를 사용해 극단적인 값 제거  
  - **Z-score**를 이용한 이상치 탐지  

```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(df["customer_age"], bins=30, kde=True)  # 나이 분포 확인
plt.show()

# IQR을 이용한 이상치 제거
Q1 = df["purchase_amount"].quantile(0.25)
Q3 = df["purchase_amount"].quantile(0.75)
IQR = Q3 - Q1

df = df[(df["purchase_amount"] >= Q1 - 1.5 * IQR) & (df["purchase_amount"] <= Q3 + 1.5 * IQR)]
```

---

## **3️⃣ 모델 선정 및 학습**  

### **🔹 (1) 적합한 모델 파악**  
- **회귀(Regression) 모델**: 연속된 수치(예: 고객 지출 금액) 예측  
- **분류(Classification) 모델**: 특정 클래스(예: 구매할지 안 할지) 예측  

| 모델 | 특징 |
|------|------|
| **로지스틱 회귀** | 간단한 분류 문제에 적합 |
| **랜덤 포레스트** | 변수 중요도 분석 및 강건한 성능 |
| **XGBoost / LightGBM** | 빠른 계산 속도와 높은 성능 |
| **딥러닝 (LSTM, CNN)** | 시계열 데이터 및 복잡한 패턴 분석에 강함 |

---

### **🔹 (2) 다양한 모델 선택 및 비교**  
- 여러 개의 모델을 학습시키고, **성능 비교 후 최적 모델을 선택**  
- 예제: 로지스틱 회귀 vs 랜덤 포레스트 vs XGBoost  

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=["target"]), df["target"], test_size=0.2, random_state=42)

models = {
    "Random Forest": RandomForestClassifier(),
    "XGBoost": XGBClassifier()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"{name} 정확도: {accuracy_score(y_test, y_pred):.4f}")
```

---

### **🔹 (3) 하이퍼파라미터 튜닝 (AutoML 활용 가능)**  
- **AUTOTUNA**: 자동 하이퍼파라미터 튜닝 라이브러리  
- **GridSearchCV 또는 Bayesian Optimization**을 사용하여 최적 파라미터 찾기  

```python
from autogluon.tabular import TabularPredictor

predictor = TabularPredictor(label="target").fit(df)
predictor.leaderboard(df)
```

---

## **4️⃣ Feature Engineering & 데이터 증강**  
- 모델 성능을 높이기 위해 **특성 엔지니어링(Feature Engineering)** 필요  

### **🔹 (1) 새로운 특성 찾기 (Feature Engineering)**  
- 예: 고객 방문 빈도를 기반으로 **고객 등급 생성**  
- **기존 변수 조합**을 통해 새로운 특징 생성  

```python
df["visit_per_spend"] = df["total_spend"] / (df["visit_count"] + 1)
```

---

### **🔹 (2) 데이터 증강 (Data Augmentation)**  
- 데이터 양이 적을 경우, 다양한 기법을 사용해 데이터를 **생성**  
- **SMOTE 기법**을 이용한 클래스 불균형 해결  

```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy='auto')
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```

---

## **5️⃣ Streamlit을 이용한 결과 시각화**  
- **Streamlit을 활용하여 모델 예측 결과를 웹 화면에 표시**  

```python
import streamlit as st

st.title("고객 예측 대시보드")

customer_id = st.text_input("고객 ID를 입력하세요:")
if st.button("예측 실행"):
    pred = predictor.predict(df[df["customer_id"] == customer_id])
    st.write(f"이 고객의 구매 가능성: {pred}")
```

---

## **✅ 전체 프로세스 정리**  
1️⃣ **도메인 지식 파악** → 어떤 데이터를 사용할지 이해  
2️⃣ **데이터 탐색 및 전처리** → 결측치, 이상치, 데이터 분포 확인  
3️⃣ **모델 선택 및 학습** → 다양한 모델을 테스트하고 최적화  
4️⃣ **Feature Engineering & 데이터 증강** → 새로운 특성 추가 및 데이터 보강  
5️⃣ **결과 시각화 (Streamlit 활용)** → 대시보드 구현  

**이 프로세스를 따라가면 고객 예측 모델을 체계적으로 구축 가능!** 🚀
