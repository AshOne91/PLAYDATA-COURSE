IMDB ë¦¬ë·° ë°ì´í„°ì…‹
RNN
ê¸°ìš¸ê¸° í´ë¦¬í•‘
LSTMì…€ 
ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬
ê²Œì´íŠ¸
ê¸°ì–µí•˜ë ¤ëŠ” ê²Œì´íŠ¸, ë§ê°í•˜ë ¤ëŠ” ê²Œì´íŠ¸

RNNì˜ ë¬¸ì œì 
```
ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)
 ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ì†ì‹¤ì´ ê³¼ê±°ì‹œì ìœ¼ë¡œ ì „ë‹¬ë ë•Œ ê¸°ìš¸ê¸° ê°’ì´ ì ì  ì‘ì•„ì§€ë©´ì„œ 0ì— ê°€ê¹Œì›Œì§€ê³  ê²°êµ­ í•™ìŠµì´ ì•ˆë˜ëŠ” í˜„ìƒ
 - í•™ìŠµê³¼ì •ì—ì„œ íŒŒì•…í•˜ê¸° ì–´ë ¤ì›€
ê¸°ìš¸ê¸° í­ë°œ(Expoding Gradient)
 íŠ¹ì •ìƒí™©ì—ì„œ ê¸°ìš¸ê¸° ê°‘ì´ ë„ˆë¬´ ì»¤ì ¸ì„œ í•™ìŠµì´ ë¶ˆì•„ì •í•´ì§€ëŠ” í˜„ìƒ
 ì½”ë“œìƒìœ¼ë¡œ ê¸°ìš¸ê¸° í´ë¦¬í•‘ì´ ì‚¬ìš©
ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµì˜ ì–´ë ¤ì›€
 ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì´ˆê¸° ì…ë ¥ì •ë³´ê°€ ì ì í¬ë¯¸í•´ì§€ëŠ” í˜„ìƒìœ¼ë¡œ ì´ˆê¸° íŠ¹ì„±ì„ ë°˜ì˜í•˜ì§€ ëª»í•¨
ë‹¨ìˆœí•œ ì •ë³´ ì €ì¥
```
LSTM
```
 RNNê³¼ ë™ì¼í•˜ê²Œ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬, ì…€ìƒíƒœì™€ ê²Œì´íŠ¸ê°œë…ì„ ë„ì… ë§ê° ê²Œì´íŠ¸
 ë§ê°ê²Œì´íŠ¸
  ì´ì§„ ì…€ ìƒíƒœì—ì„œ ì‚­ì œí•  ì •ë³´ë¥¼ ê²°ì •(0~1) 0ì€ ìŠê³  1ì´ë©´ ìœ ì§€
```
```
ì…ë ¥ê²Œì´íŠ¸
  ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ì €ì¥í• ì§€ ê²°ì •
  1 ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì €ì¥, 0ì´ë©´ ì €ì¥ì•ˆ
```
```
ì¶œë ¥ê²Œì´íŠ¸
  ìµœì¢…ì ìœ¼ë¡œ ì–´ë–¤ ì •ë³´ë¥¼ ì€ë‹‰ìƒíƒœì— ë‹´ì•„ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ê¸¸ì§€ ê²°ì •í•˜ëŠ” ê²Œì´íŠ¸
```
```
lstmì˜ ì •ë³´ì²˜ë¦¬ ìˆœì„œ
  ê³¼ê±°ì •ë³´ì—ì„œ ë²„ë¦´ê²ƒì€ ë²„ë¦¬ê³  (Foget Gate)
  ìƒˆë¡œìš´ ì •ë³´ë¥¼ ë°›ì•„ë“¤ì´ê³ (Input Gate)
  ì…€ ìƒíƒœ ì—…ë°ì´íŠ¸
  ì—…ë°ì´íŠ¸ëœ ì •ë³´ë¥¼ ì €ì¥í•œë’¤(Output Gate)
  ìµœì¢… ê²°ê³¼ë¥¼ ì¶œë ¥
```

### **ğŸ“Œ RNNì˜ ë¬¸ì œì  ë° LSTMì˜ í•´ê²° ë°©ë²•ì„ ì˜ˆì œì™€ í•¨ê»˜ ì„¤ëª…**

#### **1ï¸âƒ£ RNN(Recurrent Neural Network) ë¬¸ì œì **

âœ… **ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)**  
- ì—­ì „íŒŒ ì‹œ ê³¼ê±°ë¡œ ê°ˆìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ ì ì  ì‘ì•„ì ¸ 0ì— ê°€ê¹Œì›Œì§€ë©´ì„œ í•™ìŠµì´ ì–´ë ¤ì›Œì§€ëŠ” ë¬¸ì œ.

âœ… **ê¸°ìš¸ê¸° í­ë°œ(Exploding Gradient)**  
- íŠ¹ì • ìƒí™©ì—ì„œ ê¸°ìš¸ê¸°ê°€ ë„ˆë¬´ ì»¤ì ¸ì„œ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§€ëŠ” ë¬¸ì œ.  
- í•´ê²° ë°©ë²•: **ê¸°ìš¸ê¸° í´ë¦¬í•‘(Gradient Clipping)** ì‚¬ìš©.

âœ… **ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ì–´ë ¤ì›€**  
- ì…ë ¥ ë°ì´í„°ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì´ˆê¸° ì •ë³´ê°€ í¬ë¯¸í•´ì ¸ ì¤‘ìš”í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ì§€ ëª»í•˜ëŠ” ë¬¸ì œ.

âœ… **ë‹¨ìˆœí•œ ì •ë³´ ì €ì¥**  
- RNNì€ ë‹¨ìˆœí•œ ì€ë‹‰ ìƒíƒœë§Œ ìœ ì§€í•˜ë©° ì¥ê¸° ê¸°ì–µì„ í•˜ì§€ ëª»í•¨.

---

#### **2ï¸âƒ£ LSTM(Long Short-Term Memory)ìœ¼ë¡œ í•´ê²°**
- **LSTMì€ RNNê³¼ ë™ì¼í•˜ê²Œ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ì§€ë§Œ, ì…€ ìƒíƒœ(Cell State)ì™€ ê²Œì´íŠ¸(Gate) ê°œë…ì„ ë„ì….**
- **ê²Œì´íŠ¸ë¥¼ í†µí•´ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ê¸°ì–µí•˜ê³ , í•„ìš” ì—†ëŠ” ì •ë³´ë¥¼ ì œê±°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ.**  

#### **ğŸ“Œ LSTMì˜ ì •ë³´ íë¦„**
1. **Forget Gate(ë§ê° ê²Œì´íŠ¸)**
   - ê³¼ê±° ì •ë³´ì—ì„œ ë²„ë¦´ ê²ƒì„ ì„ íƒ.
   - 0(ìŠìŒ) ~ 1(ìœ ì§€)ì˜ ê°’ì„ ê°€ì§.

2. **Input Gate(ì…ë ¥ ê²Œì´íŠ¸)**
   - ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì €ì¥í• ì§€ ê²°ì •.
   - 0(ì €ì¥ X) ~ 1(ì €ì¥)ì˜ ê°’ì„ ê°€ì§.

3. **Cell State(ì…€ ìƒíƒœ) ì—…ë°ì´íŠ¸**
   - ê¸°ì¡´ ì •ë³´ë¥¼ ìƒˆ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸.

4. **Output Gate(ì¶œë ¥ ê²Œì´íŠ¸)**
   - ìµœì¢… ì€ë‹‰ ìƒíƒœë¥¼ ê²°ì •.

---

### **ğŸ“Œ RNNê³¼ LSTM ë¹„êµë¥¼ ìœ„í•œ ì‹¤ìŠµ ì˜ˆì œ**
Pythonì„ ì‚¬ìš©í•´ IMDB ë¦¬ë·° ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ RNNê³¼ LSTMì„ ë¹„êµ.

âœ” ìœ„ ì½”ë“œì—ì„œëŠ” IMDB ë¦¬ë·° ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ **RNN ëª¨ë¸ê³¼ LSTM ëª¨ë¸ì„ ê°ê° ì •ì˜**í–ˆìŒ.  
âœ” ë‘ ëª¨ë¸ì˜ ì°¨ì´ì ì„ í•™ìŠµí•˜ê³ , LSTMì´ RNNë³´ë‹¤ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©° ì¥ê¸° ì˜ì¡´ì„±ì„ ë” ì˜ í•™ìŠµí•˜ëŠ”ì§€ í™•ì¸ ê°€ëŠ¥í•¨.  

ì¶”ê°€ì ìœ¼ë¡œ í•™ìŠµ í›„ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ì½”ë“œë„ ì›í•œë‹¤ë©´ ìš”ì²­ ë°”ëŒ.
### **IMDB ë¦¬ë·° ë°ì´í„°ì…‹ + RNN + LSTM ê°œë… ì˜ˆì œ**

IMDB ë¦¬ë·° ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ RNNê³¼ LSTMì„ ë¹„êµí•˜ë©´ì„œ ê°œë…ì„ í•˜ë‚˜ì”© í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ì§„í–‰í•¨.

---

### **1ï¸âƒ£ RNNì„ ì´ìš©í•œ í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„ (IMDB ë¦¬ë·° ë°ì´í„°ì…‹ ì‚¬ìš©)**

#### **ğŸ”¹ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬**
```python
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

# IMDB ë°ì´í„° ë¡œë“œ (ìƒìœ„ 10,000ê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©)
num_words = 10000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)

# íŒ¨ë”© ì²˜ë¦¬ (ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶”ê¸° ìœ„í•´)
max_len = 200  # í•œ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´
x_train = pad_sequences(x_train, maxlen=max_len)
x_test = pad_sequences(x_test, maxlen=max_len)
```
- `imdb.load_data(num_words=10000)`: ìì£¼ ë“±ì¥í•˜ëŠ” 10,000ê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©.
- `pad_sequences()`: ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ `max_len=200`ìœ¼ë¡œ ë§ì¶¤.

---

#### **ğŸ”¹ RNN ëª¨ë¸ êµ¬ì¶•**
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Embedding, Dense

# RNN ëª¨ë¸ ìƒì„±
model_rnn = Sequential([
    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),  # ë‹¨ì–´ë¥¼ 128ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
    SimpleRNN(64, return_sequences=False),  # ì€ë‹‰ ìƒíƒœ í¬ê¸° 64
    Dense(1, activation='sigmoid')  # ê°ì„± ë¶„ì„ì´ë¯€ë¡œ 1ê°œì˜ ì¶œë ¥ì„ sigmoidë¡œ ë¶„ë¥˜
])

# ëª¨ë¸ ì»´íŒŒì¼
model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# ëª¨ë¸ í•™ìŠµ
history = model_rnn.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))
```
- `Embedding()`: ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜.
- `SimpleRNN()`: ì¼ë°˜ì ì¸ RNN ì¸µ ì‚¬ìš©.
- `Dense(1, activation='sigmoid')`: ì´ì§„ ê°ì„± ë¶„ë¥˜ (ê¸ì •/ë¶€ì •).

ğŸ”¹ **ë¬¸ì œì :**
- **ê¸°ìš¸ê¸° ì†Œì‹¤ (Vanishing Gradient)** â†’ ì˜¤ë˜ëœ ì…ë ¥ ì •ë³´ê°€ ì‚¬ë¼ì§.
- **ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ** â†’ ê¸´ ë¬¸ì¥ì„ í•™ìŠµí•  ë•Œ ì•ë¶€ë¶„ì„ ì˜ ê¸°ì–µí•˜ì§€ ëª»í•¨.

---

### **2ï¸âƒ£ LSTMì„ í™œìš©í•˜ì—¬ ë¬¸ì œ í•´ê²°**

#### **ğŸ”¹ LSTM ëª¨ë¸ êµ¬ì¶•**
```python
from tensorflow.keras.layers import LSTM

# LSTM ëª¨ë¸ ìƒì„±
model_lstm = Sequential([
    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),
    LSTM(64, return_sequences=False),
    Dense(1, activation='sigmoid')
])

# ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ
model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history_lstm = model_lstm.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))
```
ğŸ”¹ **LSTMì´ í•´ê²°í•˜ëŠ” ë¬¸ì œ:**
- **ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ ë°©ì§€** (ì¥ê¸° ê¸°ì–µ ê°€ëŠ¥).
- **ì¥ê¸° ì˜ì¡´ì„± í•´ê²°** (ì´ì „ ë‹¨ì–´ì™€ ë©€ë¦¬ ë–¨ì–´ì§„ ë‹¨ì–´ë„ í•™ìŠµ ê°€ëŠ¥).

---

### **3ï¸âƒ£ LSTMì˜ ë‚´ë¶€ ë™ì‘ ì˜ˆì œ**
LSTMì€ **Forget Gate, Input Gate, Output Gate**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ì¡°ì ˆí•¨.

#### **ğŸ”¹ í•œ ìŠ¤í…ì˜ ì—°ì‚° ê³¼ì •**
1. **Forget Gate (ë§ê° ê²Œì´íŠ¸)**  
   - ê³¼ê±° ì •ë³´ë¥¼ ë²„ë¦´ ê²ƒì¸ì§€ ê²°ì •.
   - ì˜ˆì œ: "ì´ ì˜í™”ëŠ” ì •ë§ [ì¢‹ì•˜ë‹¤]!" â†’ "ì •ë§"ì„ ê¸°ì–µí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ë²„ë¦´ ìˆ˜ë„ ìˆìŒ.

2. **Input Gate (ì…ë ¥ ê²Œì´íŠ¸)**
   - ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì €ì¥í• ì§€ ê²°ì •.
   - ì˜ˆì œ: "ë°°ìš° ì—°ê¸°ê°€ [íƒì›”í•˜ë‹¤]" â†’ "íƒì›”í•˜ë‹¤"ë¥¼ ê¸°ì–µ.

3. **Output Gate (ì¶œë ¥ ê²Œì´íŠ¸)**
   - ìµœì¢…ì ìœ¼ë¡œ ì€ë‹‰ ìƒíƒœë¡œ ì „ë‹¬í•  ì •ë³´ë¥¼ ê²°ì •.
   - ì˜ˆì œ: "ìŠ¤í† ë¦¬ê°€ [ë›°ì–´ë‚¬ë‹¤]" â†’ "ë›°ì–´ë‚¬ë‹¤"ë¥¼ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬.

#### **ğŸ”¹ LSTM ê²Œì´íŠ¸ ë™ì‘ ì½”ë“œ**
```python
import numpy as np

# ì…ë ¥ê°’ (ë‹¨ì–´ ë²¡í„°)
x_t = np.array([0.2, 0.5, 0.1])  # ì˜ˆì œ ë‹¨ì–´ ë²¡í„°

# ê°€ì¤‘ì¹˜ (ì„ì˜ì˜ ê°’)
Wf, Wi, Wo, Wc = np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3)
Uf, Ui, Uo, Uc = np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3)
bf, bi, bo, bc = np.random.randn(3), np.random.randn(3), np.random.randn(3), np.random.randn(3)

# ì´ì „ ìƒíƒœê°’
h_prev = np.array([0.1, 0.2, 0.3])
c_prev = np.array([0.4, 0.5, 0.6])

# ê²Œì´íŠ¸ ì—°ì‚°
forget_gate = tf.sigmoid(np.dot(Wf, x_t) + np.dot(Uf, h_prev) + bf)
input_gate = tf.sigmoid(np.dot(Wi, x_t) + np.dot(Ui, h_prev) + bi)
output_gate = tf.sigmoid(np.dot(Wo, x_t) + np.dot(Uo, h_prev) + bo)
candidate_memory = np.tanh(np.dot(Wc, x_t) + np.dot(Uc, h_prev) + bc)

# ìƒˆë¡œìš´ ì…€ ìƒíƒœ
c_t = forget_gate * c_prev + input_gate * candidate_memory

# ìƒˆë¡œìš´ ì€ë‹‰ ìƒíƒœ
h_t = output_gate * np.tanh(c_t)

print("Forget Gate Output:", forget_gate.numpy())
print("Input Gate Output:", input_gate.numpy())
print("Output Gate Output:", output_gate.numpy())
print("New Memory Cell:", c_t)
print("New Hidden State:", h_t)
```
ğŸ”¹ **ì´ ì½”ë“œì˜ ì˜ë¯¸:**
- `forget_gate` â†’ ê³¼ê±° ê¸°ì–µ ì¤‘ ì–´ë–¤ ê±¸ ìœ ì§€í• ì§€ ê²°ì •.
- `input_gate` â†’ ìƒˆë¡œìš´ ì…ë ¥ ì¤‘ ì–´ë–¤ ê±¸ ì €ì¥í• ì§€ ê²°ì •.
- `output_gate` â†’ ìµœì¢… ì¶œë ¥ ì •ë³´ë¥¼ ê²°ì •.

---

### **4ï¸âƒ£ ê¸°ìš¸ê¸° í´ë¦¬í•‘ (Gradient Clipping)**
ğŸ”¹ **ê¸°ìš¸ê¸° í­ë°œ ë°©ì§€ë¥¼ ìœ„í•´ ì‚¬ìš©**
```python
from tensorflow.keras.optimizers import Adam

# Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©í•˜ë©´ì„œ ê¸°ìš¸ê¸° í´ë¦¬í•‘ ì ìš©
optimizer = Adam(learning_rate=0.001, clipnorm=1.0)

model_lstm.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
```
- `clipnorm=1.0`: ê¸°ìš¸ê¸°ê°€ 1ì„ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì •í•˜ì—¬ í­ë°œ ë°©ì§€.

---

### **5ï¸âƒ£ RNNê³¼ LSTM ì„±ëŠ¥ ë¹„êµ**
```python
rnn_acc = model_rnn.evaluate(x_test, y_test, verbose=0)[1]
lstm_acc = model_lstm.evaluate(x_test, y_test, verbose=0)[1]

print(f"RNN Test Accuracy: {rnn_acc:.4f}")
print(f"LSTM Test Accuracy: {lstm_acc:.4f}")
```
**ì˜ˆìƒ ê²°ê³¼:**  
- RNN: ë‚®ì€ ì •í™•ë„ (ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ)
- LSTM: ë†’ì€ ì •í™•ë„ (ì¥ê¸° ê¸°ì–µ ê°€ëŠ¥)

---

### **ğŸ”¹ ê²°ë¡ **
- RNNì€ `ê¸°ìš¸ê¸° ì†Œì‹¤`ê³¼ `ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ`ë¡œ ì¸í•´ ê¸´ ë¬¸ì¥ ì²˜ë¦¬ì— ì–´ë ¤ì›€ì´ ìˆìŒ.
- LSTMì€ **Forget/Input/Output ê²Œì´íŠ¸**ë¥¼ í†µí•´ ì¥ê¸° ì˜ì¡´ì„±ì„ í•´ê²°í•˜ê³  ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„.
- `ê¸°ìš¸ê¸° í´ë¦¬í•‘`ì„ í†µí•´ `ê¸°ìš¸ê¸° í­ë°œ`ì„ ë°©ì§€í•  ìˆ˜ ìˆìŒ.

ì´ë ‡ê²Œ ê°œë…ë³„ë¡œ ì˜ˆì œì™€ í•¨ê»˜ ì‹¤ìŠµ ê°€ëŠ¥í•¨. ğŸš€

