IMDB 리뷰 데이터셋
RNN
기울기 클리핑
LSTM셀 
장단기 메모리
게이트
기억하려는 게이트, 망각하려는 게이트

RNN의 문제점
```
기울기 소실(Vanishing Gradient)
 역전파 과정에서 손실이 과거시점으로 전달될때 기울기 값이 점점 작아지면서 0에 가까워지고 결국 학습이 안되는 현상
 - 학습과정에서 파악하기 어려움
기울기 폭발(Expoding Gradient)
 특정상황에서 기울기 갑이 너무 커져서 학습이 불아정해지는 현상
 코드상으로 기울기 클리핑이 사용
장기 의존성 학습의 어려움
 데이터의 시퀀스가 길어질수록 초기 입력정보가 점점희미해지는 현상으로 초기 특성을 반영하지 못함
단순한 정보 저장
```
LSTM
```
 RNN과 동일하게 시퀀스를 처리, 셀상태와 게이트개념을 도입 망각 게이트
 망각게이트
  이진 셀 상태에서 삭제할 정보를 결정(0~1) 0은 잊고 1이면 유지
```
```
입력게이트
  새로운 정보를 얼마나 저장할지 결정
  1 새로운 정보를 저장, 0이면 저장안
```
```
출력게이트
  최종적으로 어떤 정보를 은닉상태에 담아 다음 단계로 넘길지 결정하는 게이트
```
```
lstm의 정보처리 순서
  과거정보에서 버릴것은 버리고 (Foget Gate)
  새로운 정보를 받아들이고(Input Gate)
  셀 상태 업데이트
  업데이트된 정보를 저장한뒤(Output Gate)
  최종 결과를 출력
```

### **📌 RNN의 문제점 및 LSTM의 해결 방법을 예제와 함께 설명**

#### **1️⃣ RNN(Recurrent Neural Network) 문제점**

✅ **기울기 소실(Vanishing Gradient)**  
- 역전파 시 과거로 갈수록 기울기가 점점 작아져 0에 가까워지면서 학습이 어려워지는 문제.

✅ **기울기 폭발(Exploding Gradient)**  
- 특정 상황에서 기울기가 너무 커져서 학습이 불안정해지는 문제.  
- 해결 방법: **기울기 클리핑(Gradient Clipping)** 사용.

✅ **장기 의존성 학습 어려움**  
- 입력 데이터가 길어질수록 초기 정보가 희미해져 중요한 패턴을 학습하지 못하는 문제.

✅ **단순한 정보 저장**  
- RNN은 단순한 은닉 상태만 유지하며 장기 기억을 하지 못함.

---

#### **2️⃣ LSTM(Long Short-Term Memory)으로 해결**
- **LSTM은 RNN과 동일하게 시퀀스를 처리하지만, 셀 상태(Cell State)와 게이트(Gate) 개념을 도입.**
- **게이트를 통해 중요한 정보를 기억하고, 필요 없는 정보를 제거하는 방식으로 학습.**  

#### **📌 LSTM의 정보 흐름**
1. **Forget Gate(망각 게이트)**
   - 과거 정보에서 버릴 것을 선택.
   - 0(잊음) ~ 1(유지)의 값을 가짐.

2. **Input Gate(입력 게이트)**
   - 새로운 정보를 저장할지 결정.
   - 0(저장 X) ~ 1(저장)의 값을 가짐.

3. **Cell State(셀 상태) 업데이트**
   - 기존 정보를 새 정보로 업데이트.

4. **Output Gate(출력 게이트)**
   - 최종 은닉 상태를 결정.

---

### **📌 RNN과 LSTM 비교를 위한 실습 예제**
Python을 사용해 IMDB 리뷰 데이터셋을 기반으로 RNN과 LSTM을 비교.

✔ 위 코드에서는 IMDB 리뷰 데이터셋을 사용해 **RNN 모델과 LSTM 모델을 각각 정의**했음.  
✔ 두 모델의 차이점을 학습하고, LSTM이 RNN보다 기울기 소실 문제를 해결하며 장기 의존성을 더 잘 학습하는지 확인 가능함.  

추가적으로 학습 후 성능을 비교하는 코드도 원한다면 요청 바람.
### **IMDB 리뷰 데이터셋 + RNN + LSTM 개념 예제**

IMDB 리뷰 데이터셋을 사용하여 RNN과 LSTM을 비교하면서 개념을 하나씩 학습하는 과정을 진행함.

---

### **1️⃣ RNN을 이용한 텍스트 감성 분석 (IMDB 리뷰 데이터셋 사용)**

#### **🔹 데이터 로드 및 전처리**
```python
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

# IMDB 데이터 로드 (상위 10,000개의 단어만 사용)
num_words = 10000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)

# 패딩 처리 (문장의 길이를 동일하게 맞추기 위해)
max_len = 200  # 한 문장의 최대 길이
x_train = pad_sequences(x_train, maxlen=max_len)
x_test = pad_sequences(x_test, maxlen=max_len)
```
- `imdb.load_data(num_words=10000)`: 자주 등장하는 10,000개의 단어만 사용.
- `pad_sequences()`: 문장의 길이를 `max_len=200`으로 맞춤.

---

#### **🔹 RNN 모델 구축**
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Embedding, Dense

# RNN 모델 생성
model_rnn = Sequential([
    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),  # 단어를 128차원 벡터로 변환
    SimpleRNN(64, return_sequences=False),  # 은닉 상태 크기 64
    Dense(1, activation='sigmoid')  # 감성 분석이므로 1개의 출력을 sigmoid로 분류
])

# 모델 컴파일
model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# 모델 학습
history = model_rnn.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))
```
- `Embedding()`: 단어를 벡터로 변환.
- `SimpleRNN()`: 일반적인 RNN 층 사용.
- `Dense(1, activation='sigmoid')`: 이진 감성 분류 (긍정/부정).

🔹 **문제점:**
- **기울기 소실 (Vanishing Gradient)** → 오래된 입력 정보가 사라짐.
- **장기 의존성 문제** → 긴 문장을 학습할 때 앞부분을 잘 기억하지 못함.

---

### **2️⃣ LSTM을 활용하여 문제 해결**

#### **🔹 LSTM 모델 구축**
```python
from tensorflow.keras.layers import LSTM

# LSTM 모델 생성
model_lstm = Sequential([
    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),
    LSTM(64, return_sequences=False),
    Dense(1, activation='sigmoid')
])

# 모델 컴파일 및 학습
model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history_lstm = model_lstm.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))
```
🔹 **LSTM이 해결하는 문제:**
- **기울기 소실 문제 방지** (장기 기억 가능).
- **장기 의존성 해결** (이전 단어와 멀리 떨어진 단어도 학습 가능).

---

### **3️⃣ LSTM의 내부 동작 예제**
LSTM은 **Forget Gate, Input Gate, Output Gate**를 사용하여 정보를 조절함.

#### **🔹 한 스텝의 연산 과정**
1. **Forget Gate (망각 게이트)**  
   - 과거 정보를 버릴 것인지 결정.
   - 예제: "이 영화는 정말 [좋았다]!" → "정말"을 기억하고 나머지는 버릴 수도 있음.

2. **Input Gate (입력 게이트)**
   - 새로운 정보를 저장할지 결정.
   - 예제: "배우 연기가 [탁월하다]" → "탁월하다"를 기억.

3. **Output Gate (출력 게이트)**
   - 최종적으로 은닉 상태로 전달할 정보를 결정.
   - 예제: "스토리가 [뛰어났다]" → "뛰어났다"를 다음 단계로 전달.

#### **🔹 LSTM 게이트 동작 코드**
```python
import numpy as np

# 입력값 (단어 벡터)
x_t = np.array([0.2, 0.5, 0.1])  # 예제 단어 벡터

# 가중치 (임의의 값)
Wf, Wi, Wo, Wc = np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3)
Uf, Ui, Uo, Uc = np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3), np.random.randn(3, 3)
bf, bi, bo, bc = np.random.randn(3), np.random.randn(3), np.random.randn(3), np.random.randn(3)

# 이전 상태값
h_prev = np.array([0.1, 0.2, 0.3])
c_prev = np.array([0.4, 0.5, 0.6])

# 게이트 연산
forget_gate = tf.sigmoid(np.dot(Wf, x_t) + np.dot(Uf, h_prev) + bf)
input_gate = tf.sigmoid(np.dot(Wi, x_t) + np.dot(Ui, h_prev) + bi)
output_gate = tf.sigmoid(np.dot(Wo, x_t) + np.dot(Uo, h_prev) + bo)
candidate_memory = np.tanh(np.dot(Wc, x_t) + np.dot(Uc, h_prev) + bc)

# 새로운 셀 상태
c_t = forget_gate * c_prev + input_gate * candidate_memory

# 새로운 은닉 상태
h_t = output_gate * np.tanh(c_t)

print("Forget Gate Output:", forget_gate.numpy())
print("Input Gate Output:", input_gate.numpy())
print("Output Gate Output:", output_gate.numpy())
print("New Memory Cell:", c_t)
print("New Hidden State:", h_t)
```
🔹 **이 코드의 의미:**
- `forget_gate` → 과거 기억 중 어떤 걸 유지할지 결정.
- `input_gate` → 새로운 입력 중 어떤 걸 저장할지 결정.
- `output_gate` → 최종 출력 정보를 결정.

---

### **4️⃣ 기울기 클리핑 (Gradient Clipping)**
🔹 **기울기 폭발 방지를 위해 사용**
```python
from tensorflow.keras.optimizers import Adam

# Adam 옵티마이저 사용하면서 기울기 클리핑 적용
optimizer = Adam(learning_rate=0.001, clipnorm=1.0)

model_lstm.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
```
- `clipnorm=1.0`: 기울기가 1을 넘지 않도록 조정하여 폭발 방지.

---

### **5️⃣ RNN과 LSTM 성능 비교**
```python
rnn_acc = model_rnn.evaluate(x_test, y_test, verbose=0)[1]
lstm_acc = model_lstm.evaluate(x_test, y_test, verbose=0)[1]

print(f"RNN Test Accuracy: {rnn_acc:.4f}")
print(f"LSTM Test Accuracy: {lstm_acc:.4f}")
```
**예상 결과:**  
- RNN: 낮은 정확도 (장기 의존성 문제)
- LSTM: 높은 정확도 (장기 기억 가능)

---

### **🔹 결론**
- RNN은 `기울기 소실`과 `장기 의존성 문제`로 인해 긴 문장 처리에 어려움이 있음.
- LSTM은 **Forget/Input/Output 게이트**를 통해 장기 의존성을 해결하고 더 나은 성능을 보임.
- `기울기 클리핑`을 통해 `기울기 폭발`을 방지할 수 있음.

이렇게 개념별로 예제와 함께 실습 가능함. 🚀

