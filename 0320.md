### **📌 결측치(Missing Values) 처리: 제거 vs. 대체**  

데이터의 결측치 처리는 데이터 타입(수치형 vs. 범주형)과 데이터의 특성에 따라 **제거(drop)** 또는 **대체(impute)** 방식을 선택해야 합니다.  

---

## **1️⃣ 결측치 제거 (Drop)**
💡 **언제 제거해야 할까?**  
- 결측치가 포함된 행(row)이 전체 데이터에서 **차지하는 비율이 적을 때** (예: 5% 미만)  
- 해당 컬럼이 **분석에 큰 영향을 미치지 않을 때**  
- 결측치가 특정 패턴 없이 **무작위로 발생했을 때 (MCAR; Missing Completely at Random)**  
- **대체할 적절한 값이 없거나** 대체 시 **데이터 왜곡 가능성이 높을 때**  

🔹 **예제 (Pandas)**
```python
df.dropna(subset=['컬럼명'], inplace=True)  # 특정 컬럼 기준으로 결측치가 있는 행 삭제
```

---

## **2️⃣ 결측치 대체 (Impute)**
💡 **언제 대체해야 할까?**  
- **결측치가 많은 경우** (행을 제거하면 데이터 손실이 크기 때문)  
- 해당 컬럼이 **중요한 정보**를 포함하고 있을 때  
- 결측치가 특정 패턴을 보일 가능성이 있을 때 (MAR, MNAR)  
- 데이터 손실을 최소화하고 모델 성능을 유지하고 싶을 때  

---

# **📌 데이터 타입별 결측치 처리 방법**
## **✅ 수치형 데이터 (int, float)**
### **1️⃣ 제거 (Drop)**
- 결측 데이터 비율이 낮고, 제거해도 데이터 손실이 크지 않을 때  
```python
df.dropna(subset=['수치형컬럼'], inplace=True)  
```

### **2️⃣ 대체 (Impute)**
#### **✔ 평균(mean) 대체**
📌 **정규 분포를 따르는 데이터에 적합**  
```python
df['수치형컬럼'].fillna(df['수치형컬럼'].mean(), inplace=True)
```

#### **✔ 중앙값(median) 대체**
📌 **이상치(outlier)가 많은 경우 적합 (ex: 소득, 주택 가격)**  
```python
df['수치형컬럼'].fillna(df['수치형컬럼'].median(), inplace=True)
```

#### **✔ 최빈값(mode) 대체**
📌 **값이 특정 값에 집중되어 있을 때 사용 가능**  
```python
df['수치형컬럼'].fillna(df['수치형컬럼'].mode()[0], inplace=True)
```

#### **✔ KNN 또는 머신러닝을 이용한 예측 대체**
📌 **다른 변수들과의 관계를 활용해 결측치를 보완할 때 사용**  
```python
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)  # 가장 가까운 5개의 값 평균 사용
df[['수치형컬럼']] = imputer.fit_transform(df[['수치형컬럼']])
```

#### **✔ 앞/뒤 값 채우기 (시계열 데이터)**
📌 **시간 순서가 있는 데이터 (ex: 주가, 기온, 센서 데이터)에서 사용**  
```python
df['수치형컬럼'].fillna(method='ffill', inplace=True)  # 이전 값으로 채우기
df['수치형컬럼'].fillna(method='bfill', inplace=True)  # 다음 값으로 채우기
```

---

## **✅ 범주형 데이터 (object, category)**
### **1️⃣ 제거 (Drop)**
- 결측치 비율이 적고, 해당 컬럼이 분석에 크게 영향을 미치지 않을 때  
```python
df.dropna(subset=['범주형컬럼'], inplace=True)
```

### **2️⃣ 대체 (Impute)**
#### **✔ 최빈값(mode) 대체**
📌 **범주형 데이터에서 가장 많이 등장한 값으로 채움**  
```python
df['범주형컬럼'].fillna(df['범주형컬럼'].mode()[0], inplace=True)
```

#### **✔ ‘Unknown’ 또는 ‘Other’ 등 특정 값 대체**
📌 **카테고리가 명확하지 않거나, 분석에서 큰 영향이 없을 때**  
```python
df['범주형컬럼'].fillna('Unknown', inplace=True)
```

#### **✔ 그룹별 최빈값 대체**
📌 **특정 그룹(예: 지역, 성별 등)에 따라 결측치를 보완해야 할 때**  
```python
df['범주형컬럼'] = df.groupby('기준컬럼')['범주형컬럼'].transform(lambda x: x.fillna(x.mode()[0]))
```

#### **✔ 예측 모델 활용 (ML Imputation)**
📌 **다른 변수들과의 관계를 활용해 결측치를 보완할 때 사용**  
```python
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="most_frequent")
df[['범주형컬럼']] = imputer.fit_transform(df[['범주형컬럼']])
```

---

# **3️⃣ 결론: 수치형 vs 범주형 비교 정리**
| 데이터 타입 | 제거 (Drop) | 대체 (Impute) |
|------------|------------|---------------|
| **수치형** | 결측치 비율이 낮거나 데이터 왜곡 우려가 클 때 | 평균, 중앙값, KNN, 시계열 보간 사용 |
| **범주형** | 결측치 비율이 낮거나 의미 없는 컬럼일 때 | 최빈값, 'Unknown' 대체, 그룹별 보완 |

✅ **수치형은 평균·중앙값·KNN 등 수치적 방법 활용**  
✅ **범주형은 최빈값, 특정 값 ('Unknown') 활용**  
✅ **데이터 특성을 고려해 최적의 방법 선택**  

---

## **📌 결측치 처리 방법 선택 기준**
| 상황 | 제거 (Drop) | 대체 (Impute) |
|------|------------|---------------|
| 결측 데이터 비율 | 적을 때 (5% 미만) | 많을 때 (5% 이상) |
| 데이터 손실 우려 | 적음 | 큼 |
| 해당 컬럼 중요도 | 낮음 | 높음 |
| 데이터 패턴 고려 | 무작위 (MCAR) | 특정 패턴 (MAR, MNAR) |
| 머신러닝 모델 적용 | 필요 없음 | 필요할 수 있음 |

---

## **📌 타이타닉 데이터셋 이상치 분석 – 수학적 개념 정리**  
이상치 분석에서 핵심이 되는 **IQR (사분위수 범위)**, **Z-score (표준점수)** 개념을 수학적으로 설명하고, 코드와 함께 이해해보자.  

---

# **1️⃣ 이상치란?**  
이상치는 **데이터 분포에서 비정상적으로 크거나 작은 값**을 의미해.  
이상치는 분석을 방해하거나 잘못된 결과를 초래할 수 있기 때문에 **제거 또는 변환이 필요**해.  

이상치를 판별하는 대표적인 방법은 두 가지야:  
1️⃣ **IQR (Interquartile Range, 사분위수 범위)**  
2️⃣ **Z-score (표준점수, 정규화된 거리)**  

---

# **2️⃣ IQR (사분위수 범위) 방법 – 상위/하위 1.5배 판별법**  
## **✔ IQR 개념**  
IQR(Interquartile Range)은 **데이터의 중앙 50%를 포함하는 범위**야.  
이걸 활용해서 이상치를 판별하는 기준은 아래와 같아.

### **📌 IQR 계산 공식**
\[
IQR = Q3 - Q1
\]
- **Q1 (제1사분위, 25%)**: 데이터의 하위 25% 지점 값  
- **Q3 (제3사분위, 75%)**: 데이터의 상위 25% 지점 값  
- **IQR (사분위수 범위)**: Q3 - Q1  

### **📌 이상치 기준**
이상치는 **Q1 - 1.5 × IQR** 미만이거나 **Q3 + 1.5 × IQR** 초과하는 값으로 정해.  
\[
\text{Lower Bound} = Q1 - 1.5 \times IQR
\]
\[
\text{Upper Bound} = Q3 + 1.5 \times IQR
\]

---

## **✔ IQR 코드 예제**
```python
import pandas as pd
import numpy as np

# 데이터 불러오기
df = pd.read_csv("titanic.csv")

# IQR을 이용한 이상치 탐색 함수
def find_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)  # 제1사분위
    Q3 = df[column].quantile(0.75)  # 제3사분위
    IQR = Q3 - Q1  # IQR 계산
    lower_bound = Q1 - 1.5 * IQR  # 하한
    upper_bound = Q3 + 1.5 * IQR  # 상한
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]  # 이상치 찾기
    
    print(f"📌 [{column}의 이상치 개수]: {len(outliers)}개")
    return outliers

# 이상치 확인 (나이, 요금 등)
find_outliers_iqr(df, "Fare")
```
✅ **해석:**  
- `Fare` 같은 경우 높은 티켓 가격이 많아서 이상치로 판별될 가능성이 커!  
- `Age` 같은 경우 상대적으로 이상치가 적을 수도 있어.  

---

# **3️⃣ Z-score (표준점수) 방법 – 평균과 표준편차 기준**
## **✔ Z-score 개념**  
Z-score는 **데이터 값이 평균으로부터 얼마나 떨어져 있는지를 표준편차 기준으로 측정하는 값**이야.

### **📌 Z-score 공식**
\[
Z = \frac{X - \mu}{\sigma}
\]
- \(X\) : 특정 데이터 값  
- \(\mu\) : 데이터의 평균  
- \(\sigma\) : 데이터의 표준편차  

### **📌 이상치 기준**
이상치는 **Z-score가 ±3을 초과하는 값**으로 간주해.  
\[
|Z| > 3
\]
즉, 평균에서 3 표준편차 이상 벗어난 값들은 이상치일 확률이 높아.

---

## **✔ Z-score 코드 예제**
```python
from scipy import stats

# Z-score 기반 이상치 탐색
def find_outliers_zscore(df, column):
    z_scores = np.abs(stats.zscore(df[column].dropna()))  # Z-score 계산
    outliers = df[z_scores > 3]  # |Z| > 3인 값 찾기
    print(f"📌 [{column}의 이상치 개수]: {len(outliers)}개")
    return outliers

# 이상치 확인
find_outliers_zscore(df, "Fare")
```
✅ **해석:**  
- `Fare`의 경우, 평균에서 3배 이상 벗어난 값들은 이상치로 판별됨.  
- `Age`처럼 정규분포를 따르는 컬럼에서는 이상치가 적을 수도 있어.

---

# **4️⃣ 이상치 처리 방법**
이제 **이상치를 어떻게 처리할지** 결정해야 해.

| 방법 | 설명 | 코드 |
|------|------|------|
| **제거 (Remove)** | 이상치가 너무 많거나 중요하지 않다면 삭제 | `df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]` |
| **대체 (Replace)** | 이상치를 평균, 중앙값 등으로 변경 | `df[col] = np.clip(df[col], lower_bound, upper_bound)` |
| **로그 변환 (Log Transform)** | 이상치가 큰 경우 로그 변환으로 완화 | `df['col'] = np.log1p(df['col'])` |

---

## **① 이상치 제거 (IQR 기준)**
```python
# IQR 기반 이상치 제거
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

df_clean = remove_outliers(df, 'Fare')
print(f"✅ 이상치 제거 후 데이터 크기: {df_clean.shape}")
```
✅ **해석:**  
- `Fare`의 이상치를 제거하여 데이터 크기가 줄어들었음을 확인할 수 있음.

---

## **② 이상치 대체 (클리핑)**
```python
# 이상치를 상한/하한 값으로 변환
df['Fare'] = np.clip(df['Fare'], df['Fare'].quantile(0.05), df['Fare'].quantile(0.95))
```
✅ **해석:**  
- `Fare`에서 극단적인 이상치를 **상위 95%, 하위 5% 백분위 값으로 제한**하여 왜곡을 줄임.

---

## **③ 로그 변환 (분포 조정)**
```python
# Fare 로그 변환
df['Fare_log'] = np.log1p(df['Fare'])

# 히스토그램 시각화
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
sns.histplot(df['Fare'], bins=50, kde=True)
plt.title("Before Log Transformation")

plt.subplot(1, 2, 2)
sns.histplot(df['Fare_log'], bins=50, kde=True)
plt.title("After Log Transformation")

plt.show()
```
✅ **해석:**  
- 로그 변환을 적용하면 데이터의 분포가 **더 정규분포에 가깝게 조정됨**.

---

# **📌 최종 정리**
1️⃣ **이상치 탐색 방법**  
- **IQR (1.5배 기준) → Q1 - 1.5×IQR, Q3 + 1.5×IQR**  
- **Z-score (표준점수) → |Z| > 3**  
- **시각화 (박스플롯, 히스토그램)**  

2️⃣ **이상치 처리 방법**  
- **제거 (Remove)**
- **상한/하한 대체 (Clip)**
- **로그 변환 (Log Transform)**  

---

📢 **이제 이상치를 다루는 방법이 더 명확해졌을 거야!** 🚀  
더 궁금한 게 있으면 질문해줘! 😊
## **🎯 최적의 결측치 처리 전략**
1️⃣ **결측치 비율이 낮다면?** → **제거(drop)**  
2️⃣ **중요한 컬럼이라면?** → **평균(mean), 중앙값(median), 최빈값(mode) 대체**  
3️⃣ **시간순 데이터라면?** → **앞/뒤 값 채우기 (ffill, bfill)**  
4️⃣ **고급 기법을 사용하려면?** → **KNN, 머신러닝 기반 예측 대체**  

👉 **데이터의 특성과 분석 목적에 맞게 적절한 방법을 선택하는 것이 중요합니다!** 😊

상관관계와 카이제곱 검정을 **수학적인 측면**도 포함하여 쉽게 설명해드릴게요!

---

### **1️⃣ 피어슨 상관계수 (Pearson Correlation Coefficient)**

**피어슨 상관계수**는 **두 숫자형 변수** 간의 **선형적인 관계**가 얼마나 강한지, 즉 두 변수 간의 변화가 얼마나 비례하는지 측정하는 값입니다.  
이 값은 **-1**에서 **1** 사이의 값을 가지며, 두 변수 간의 관계를 수학적으로 나타냅니다.

#### **수학적 정의:**
피어슨 상관계수 \( r \)은 두 변수 \( X \)와 \( Y \)에 대해 다음과 같이 계산됩니다:
\[
r = \frac{{\sum (X_i - \bar{X})(Y_i - \bar{Y})}}{{\sqrt{{\sum (X_i - \bar{X})^2 \cdot \sum (Y_i - \bar{Y})^2}}}}
\]
- **\( X_i \)**, **\( Y_i \)**: 각각의 관측값
- **\( \bar{X} \)**, **\( \bar{Y} \)**: 각각의 평균 값

#### **피어슨 상관계수 해석:**
- \( r = 1 \): 두 변수 간에 완벽한 양의 선형 관계가 있음 (같은 방향으로 변함)
- \( r = -1 \): 두 변수 간에 완벽한 음의 선형 관계가 있음 (한 변수가 증가하면 다른 변수는 감소)
- \( r = 0 \): 두 변수 간에 선형적인 관계가 없거나 매우 약함
- \( 0 < r < 1 \): 양의 선형 관계 (한 변수가 증가하면 다른 변수도 증가)
- \( -1 < r < 0 \): 음의 선형 관계 (한 변수가 증가하면 다른 변수는 감소)

#### **예시 (나이와 생존율):**
- **나이(Age)**와 **생존 여부(Survived)** 간의 상관관계를 피어슨 상관계수로 분석한다고 가정합니다.
  - **나이**와 **생존 여부**는 연속형 데이터와 이진 데이터이므로, 피어슨 상관계수로 분석 가능합니다.
  - 피어슨 상관계수가 **0.2**라면, 나이가 증가하면 생존율도 조금 증가하는 경향이 있다는 의미일 수 있습니다.

---

### **2️⃣ 카이제곱 검정 (Chi-squared Test)**

**카이제곱 검정**은 두 범주형 변수 간의 **독립성**을 테스트하는 방법입니다. 즉, 두 변수 간에 **상관관계가 있는지** 확인하는 방법입니다. 카이제곱 검정은 **관측값**과 **기대값** 사이의 차이를 이용해 검정을 수행합니다.

#### **수학적 정의:**
카이제곱 통계량 \( \chi^2 \)은 **관측값**과 **기대값**의 차이를 바탕으로 계산됩니다:
\[
\chi^2 = \sum \frac{{(O_i - E_i)^2}}{{E_i}}
\]
- **\( O_i \)**: 관측값 (실제 데이터에서 얻은 값)
- **\( E_i \)**: 기대값 (귀무가설 하에서 예상되는 값)

#### **기대값 계산:**
기대값 \( E_i \)는 각 카테고리별로 **전체 데이터에 대한 비율**을 반영하여 계산합니다:
\[
E_i = \frac{{\text{{(행의 합)}} \times \text{{(열의 합)}}}}{{\text{{전체 데이터 수}}}}
\]
이렇게 계산한 기대값과 실제 관측값의 차이를 통해 두 변수 간의 **독립성**을 검증합니다.

#### **카이제곱 검정 해석:**
- **귀무가설(H0)**: 두 변수 간에 **독립적**이다. 즉, **상관관계가 없다**.
- **대립가설(H1)**: 두 변수 간에 **독립적이지 않다**. 즉, **상관관계가 있다**.
- **p-value**가 0.05보다 작으면 **귀무가설을 기각**하고, 두 변수 간에 관계가 있다고 결론 내릴 수 있습니다.

#### **예시 (성별과 생존 여부):**
- **성별(Sex)**과 **생존 여부(Survived)** 간의 관계를 카이제곱 검정으로 분석하려면, **성별**과 **생존 여부**의 교차표를 작성하고 카이제곱 검정을 수행합니다.
  - 성별과 생존 여부 간에 **관계가 있다면** (p-value가 작으면), **성별**이 **생존 여부에 영향을 미친다**고 결론 내릴 수 있습니다.
  
---

### **3️⃣ 차이점 정리**
- **피어슨 상관계수**:
  - **수치형 변수**들 간의 **선형적 관계**를 파악하는 데 사용.
  - **범위**: -1에서 1 사이.
  - 두 변수 간에 **선형 관계가 있을 때** 유용.

- **카이제곱 검정**:
  - **범주형 변수** 간의 **독립성**을 테스트하는 데 사용.
  - 두 변수 간에 **상관관계가 있는지** 여부를 파악.

---

### **📌 결론**
- **피어슨 상관계수**는 **숫자형**(연속형) 변수 간의 관계를 분석하고, 두 변수 간의 선형 관계를 수학적으로 계산합니다.
- **카이제곱 검정**은 **범주형** 변수 간의 관계를 분석하고, 두 변수 간에 **독립성**이 존재하는지 여부를 검증합니다.

**상관관계**와 **독립성 검정**은 모두 데이터 분석에서 중요한 역할을 하며, 상황에 따라 적절한 방법을 선택해야 합니다. 😊

혹시 더 궁금한 점이 있으면 언제든지 물어보세요!

### **1️⃣ 피어슨 상관계수 vs 포인트-비세리얼 상관계수 (Point-Biserial Correlation)**

**피어슨 상관계수 (Pearson Correlation Coefficient)**와 **포인트-비세리얼 상관계수 (Point-Biserial Correlation)**는 모두 두 변수 간의 관계를 측정하는 지표입니다. 하지만 사용되는 경우가 다릅니다.

#### **피어슨 상관계수 (Pearson Correlation Coefficient)**:
- **사용 조건**: 두 변수 모두 **연속형** (숫자형)일 때 사용됩니다.
- **값 범위**: -1 ~ 1
- **의미**:
  - **1**: 완전한 양의 선형 관계 (두 변수가 완벽하게 비례함)
  - **-1**: 완전한 음의 선형 관계 (한 변수가 증가하면 다른 변수는 감소)
  - **0**: 선형 관계가 없음 (변수 간에 아무런 관계가 없음)

#### **포인트-비세리얼 상관계수 (Point-Biserial Correlation)**:
- **사용 조건**: 한 변수는 **연속형** (숫자형)이고, 다른 변수는 **이진형** (두 가지 값만 가질 수 있는 범주형)일 때 사용됩니다.
  - 예: **생존 여부 (Survived)**와 **나이 (Age)** 간의 관계 분석에서 **생존 여부**는 이진형 (0: 사망, 1: 생존), **나이**는 연속형 변수입니다.
- **값 범위**: -1 ~ 1 (피어슨 상관계수와 동일)
- **의미**:
  - **1**: 생존 여부와 나이가 완벽하게 양의 선형 관계
  - **-1**: 생존 여부와 나이가 완벽하게 음의 선형 관계
  - **0**: 생존 여부와 나이 간에 선형 관계가 없음

---

### **2️⃣ 코드 설명**

```python
from scipy.stats import pointbiserialr
df1 = df.copy()
numeric_cols = df1.describe().columns[3:]  # 수치형 변수들만 선택

# 숫자형 변수와 생존 간의 상관계수를 계산
for col in numeric_cols:
  # 피어슨 상관계수 계산
  corr = df1['Survived'].corr(df1[col])  # 피어슨 상관계수
  print(f'corr : {col}과의 상관계수 : {corr:.3f}')
  
  # 포인트-비세리얼 상관계수 계산
  corr, p_value = pointbiserialr(df1['Survived'], df1[col])
  print(f'pointbiserialr : {col}과의 상관계수 : {corr:.3f}, p-value : {p_value:.3f}')
```

#### **1. 피어슨 상관계수**
- `df1['Survived'].corr(df1[col])`는 **피어슨 상관계수**를 계산합니다.
- 이는 **'Survived'** (이진형)과 **연속형 변수** 간의 선형 관계를 측정합니다.
  - 예: 나이가 증가할수록 생존 확률이 증가하는지, 또는 반대의 관계가 있는지를 파악할 수 있습니다.

#### **2. 포인트-비세리얼 상관계수**
- `pointbiserialr(df1['Survived'], df1[col])`는 **포인트-비세리얼 상관계수**와 **p-value**를 계산합니다.
  - **p-value**는 상관관계가 우연에 의한 것인지 아닌지를 판단하는 지표입니다.
    - p-value가 **0.05 미만**이면 **유의미한 상관관계**가 있다는 뜻입니다.
    - p-value가 **0.05 이상**이면 **상관관계가 없다**고 볼 수 있습니다.
  
#### **3. 출력 예시**
```plaintext
corr : Age과의 상관계수 : 0.070
pointbiserialr : Age과의 상관계수 : 0.070, p-value : 0.318
```

- 여기서 `corr`은 **0.070**으로 **양의 상관관계**를 보여주지만, **p-value**가 **0.318**로 **0.05보다 크므로** 이 상관관계는 **통계적으로 유의미하지 않다**고 해석할 수 있습니다. 즉, 나이가 생존 여부에 큰 영향을 주지 않다는 결론을 내릴 수 있습니다.

---

### **3️⃣ 해석**
- **상관계수 (corr)**:
  - 양의 상관관계일 경우 (corr > 0), 한 변수가 증가하면 다른 변수도 증가하는 경향이 있음을 의미합니다.
  - 음의 상관관계일 경우 (corr < 0), 한 변수가 증가하면 다른 변수는 감소하는 경향이 있음을 의미합니다.
  - 상관계수가 **0에 가까운 값**이라면, 두 변수 간에는 선형적인 관계가 없다는 의미입니다.
  
- **p-value**:
  - **p-value**는 두 변수 간의 관계가 **우연에 의한 것인지 아닌지**를 판단하는 데 사용됩니다.
  - 일반적으로 **p-value가 0.05 미만**이면 관계가 **유의미**하다고 판단합니다.

### **결론**
- **피어슨 상관계수**는 **두 숫자형 변수** 간의 **선형 관계**를 측정하는 데 유용합니다.
- **포인트-비세리얼 상관계수**는 **하나의 변수**가 **연속형**이고, 다른 **이진형 변수**와의 관계를 측정하는 데 유용합니다.
- 이 두 상관계수를 사용하여 **숫자형 변수**와 **생존 여부** 간의 관계를 분석하고, 그 관계의 강도 및 유의미성을 판단할 수 있습니다.

**유의 확률 (p-value)**은 통계적 검정에서 **귀무가설(null hypothesis)**을 기각할 수 있는 근거를 제공하는 값입니다. 이는 실험이나 데이터 분석을 통해 얻은 **관측된 결과가 우연히 발생했을 가능성**을 나타냅니다. 일반적으로 **p-value**는 **귀무가설을 기각할지 말지를 결정하는 기준**으로 사용됩니다.

---

### **1️⃣ 유의확률(p-value) 정의**

**p-value**는 실험이나 분석에서 **귀무가설**이 참일 때, **관측된 데이터 또는 그보다 극단적인 데이터**가 발생할 확률입니다.  
쉽게 말해, **귀무가설이 맞다는 전제 하에서** 우리가 얻은 데이터가 발생할 확률을 나타냅니다.

- **p-value가 작으면 (0.05보다 작을 때)**, **귀무가설을 기각**할 수 있습니다. 즉, 얻은 결과가 우연히 발생했을 확률이 매우 낮아 **새로운 가설을 채택**할 만한 이유가 생깁니다.
- **p-value가 크면 (0.05보다 클 때)**, **귀무가설을 기각할 수 없다**는 결론을 내립니다. 즉, 결과가 우연히 발생했을 가능성이 크므로 귀무가설을 유지합니다.

### **2️⃣ 유의확률 (p-value) 해석**
- **p-value < 0.01**: 매우 강한 증거 (귀무가설을 기각할 수 있음)
- **0.01 ≤ p-value < 0.05**: 약한 증거 (귀무가설을 기각할 수 있음)
- **0.05 ≤ p-value < 0.1**: 매우 약한 증거 (귀무가설을 기각할 수 없지만, 추가적인 조사가 필요함)
- **p-value ≥ 0.1**: **귀무가설을 기각할 충분한 증거가 없다**.

### **3️⃣ p-value와 신뢰수준 (Significance Level)**

**신뢰수준 (α)**은 귀무가설을 기각할 기준이 되는 값입니다. 일반적으로 0.05 (5%)가 많이 사용됩니다.
- **p-value < α**일 경우 **귀무가설을 기각**합니다.
- **p-value ≥ α**일 경우 **귀무가설을 기각하지 않는다**는 결론을 내립니다.

즉, **0.05의 유의수준**을 기준으로 **p-value가 0.05보다 작으면 귀무가설을 기각**, **p-value가 0.05보다 크면 귀무가설을 기각하지 않음**.

### **4️⃣ 예시**

#### **귀무가설 (H₀)**: "이 약물은 효과가 없다"  
#### **대립가설 (H₁)**: "이 약물은 효과가 있다"

- **p-value = 0.03**: 3% 확률로 이 약물이 효과가 있다는 결과가 우연히 발생했다고 가정할 수 있습니다. (0.03 < 0.05)
  - **귀무가설 기각**: 약물이 효과가 있다는 증거가 충분히 강합니다.
  
- **p-value = 0.12**: 12% 확률로 약물이 효과가 있다는 결과가 우연히 발생했다고 할 수 있습니다. (0.12 > 0.05)
  - **귀무가설 기각 불가**: 약물이 효과가 있다고 주장할 수 있는 충분한 증거가 없습니다.

### **5️⃣ p-value 계산 예시**
p-value를 계산하는 방법은 **통계적 검정**에 따라 다릅니다. 예를 들어, **t-test**, **카이제곱 검정**, **ANOVA** 등 다양한 방법으로 p-value를 계산할 수 있습니다.

- **예시**: `pointbiserialr`를 사용하여 **연속형 변수**와 **이진형 변수** 간의 상관관계를 계산한 경우
  ```python
  from scipy.stats import pointbiserialr
  corr, p_value = pointbiserialr(df1['Survived'], df1['Age'])
  print(f'Point-biserial 상관계수: {corr:.3f}, p-value: {p_value:.3f}')
  ```
  이 경우, **p-value**는 두 변수 간의 상관관계가 우연에 의한 것인지 아닌지 검증하는 데 사용됩니다.

---

### **6️⃣ 결론**

- **p-value**는 귀무가설을 **기각**할지 **기각하지 않을지** 결정하는 중요한 기준입니다.
- **p-value가 0.05 미만**이면 **귀무가설을 기각**하고, **0.05 이상**이면 **귀무가설을 기각할 수 없다**고 결론을 내립니다.
- **p-value**가 작을수록 **귀무가설을 기각할 가능성**이 높고, **데이터의 통계적 유의미성**을 나타냅니다.

---

추가적으로 더 궁금한 점 있으면 언제든지 물어보세요! 😊

### **카이제곱 검정 (Chi-Square Test)**

카이제곱 검정은 **범주형 데이터**에서 두 변수 간의 관계를 분석하거나, **표본이 기대한 분포**와 일치하는지를 검정하는 데 사용되는 통계적 방법입니다.

주로 다음 두 가지 경우에 사용됩니다:

1. **적합도 검정 (Goodness of Fit Test)**: 관찰된 데이터가 **예상한 분포와 일치하는지** 확인.
2. **독립성 검정 (Test of Independence)**: 두 범주형 변수 간에 **독립적인 관계**가 있는지 확인.

---

### **1️⃣ 카이제곱 검정 종류**

#### **1.1 적합도 검정 (Goodness of Fit Test)**:
- **목적**: 주어진 데이터가 예상한 분포와 일치하는지 확인.
- 예: 주사위 던지기 실험에서 6면이 균등하게 나오는지 확인하려 할 때.
  
#### **1.2 독립성 검정 (Test of Independence)**:
- **목적**: 두 범주형 변수 간에 **독립적인 관계**가 있는지 확인.
- 예: 성별과 흡연 여부가 독립적인지(성별에 따라 흡연을 하는지 여부가 영향을 받는지) 확인할 때 사용됩니다.

---

### **2️⃣ 카이제곱 검정의 가설 설정**

#### **귀무가설 (H₀)**:
- 두 변수는 **독립적이다** (두 변수 간에는 관계가 없다).
  
#### **대립가설 (H₁)**:
- 두 변수는 **독립적이지 않다** (두 변수 간에는 관계가 있다).

---

### **3️⃣ 카이제곱 통계량 계산**

카이제곱 통계량은 다음의 공식으로 계산됩니다:

\[
\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
\]

- **O_i**: 관찰된 빈도 (Observed Frequency)
- **E_i**: 기대된 빈도 (Expected Frequency) — 귀무가설이 참일 때 예상되는 빈도
- **Σ**: 각 카테고리에 대해 계산된 값을 합산

---

### **4️⃣ p-value와 카이제곱 검정**

- 카이제곱 통계량(χ²)을 계산한 후, 해당 값을 자유도(degrees of freedom)에 맞는 **카이제곱 분포표**에서 확인하여 **p-value**를 계산합니다.
- **p-value**가 **0.05보다 작으면** 귀무가설을 기각하고, 두 변수 간에 **의미 있는 관계**가 있다고 판단합니다.

---

### **5️⃣ 카이제곱 검정 사용 예시**

다음은 **`pandas`**와 **`scipy.stats`** 라이브러리를 사용하여 **독립성 검정**을 수행하는 예시입니다.

```python
import pandas as pd
from scipy.stats import chi2_contingency

# 예시 데이터: 성별과 생존 여부
data = {'Survived': ['Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes'],
        'Gender': ['Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Male', 'Female']}
df = pd.DataFrame(data)

# 데이터의 교차 테이블 생성 (빈도 계산)
contingency_table = pd.crosstab(df['Gender'], df['Survived'])

# 카이제곱 검정 수행
chi2, p, dof, expected = chi2_contingency(contingency_table)

# 결과 출력
print(f"카이제곱 통계량: {chi2}")
print(f"p-value: {p}")
print(f"자유도: {dof}")
print(f"기대 빈도: \n{expected}")

# p-value에 따라 귀무가설을 기각할지 결정
if p < 0.05:
    print("귀무가설을 기각합니다. 성별과 생존 여부 간에는 관계가 있습니다.")
else:
    print("귀무가설을 기각할 수 없습니다. 성별과 생존 여부 간에는 관계가 없습니다.")
```

### **6️⃣ 코드 설명**

- **`contingency_table`**: `pd.crosstab()`을 사용하여 두 범주형 변수의 교차 빈도를 계산합니다. 예를 들어, 성별과 생존 여부에 대한 교차 테이블을 만들 수 있습니다.
  
- **`chi2_contingency()`**: 이 함수는 카이제곱 검정을 수행합니다. 반환되는 값은 다음과 같습니다:
  - `chi2`: 카이제곱 통계량
  - `p`: p-value
  - `dof`: 자유도 (degree of freedom)
  - `expected`: 기대 빈도
  
- **결과 해석**:
  - **p-value < 0.05**: 귀무가설을 기각합니다. 즉, 성별과 생존 여부는 **독립적이지 않다**는 결론을 내립니다.
  - **p-value ≥ 0.05**: 귀무가설을 기각하지 못합니다. 즉, 성별과 생존 여부는 **독립적이다**는 결론을 내립니다.

---

### **7️⃣ 결론**

- **카이제곱 검정**은 **범주형 변수** 간의 **독립성**을 검증하는 데 매우 유용한 방법입니다.
- **독립성 검정**을 통해 두 변수 간에 관계가 있는지 확인할 수 있습니다.
- **적합도 검정**을 통해 데이터가 예상한 분포와 일치하는지를 확인할 수 있습니다.

추가적으로 더 궁금한 점 있으면 언제든지 물어보세요! 😊

이 코드는 **Cramér's V** 값을 이용해 **범주형 변수** (`categorical_cols` 리스트에 포함된 컬럼들)와 **생존 여부(Survived)** 간의 관계를 평가하는 예시입니다. **Cramér's V**는 **범주형 변수** 간의 연관성 정도를 측정하는 지표로, **0**과 **1** 사이의 값을 가집니다.

### **Cramér's V의 개념**
**Cramér's V**는 두 범주형 변수 간의 연관성을 평가하는 통계량으로, **피어슨 카이제곱 검정**을 기반으로 계산됩니다. 이 값은 **0**에서 **1** 사이의 값을 가지며, 값이 클수록 두 변수 간의 연관성이 강하다는 것을 의미합니다.

- **Cramér's V = 0**: 두 변수 간에 연관성이 없음.
- **Cramér's V = 1**: 두 변수 간에 완벽한 연관성이 있음.
- **0 < Cramér's V < 1**: 두 변수 간에 부분적인 연관성이 있음.

### **Cramér's V 계산 방법**

Cramér's V는 아래의 공식으로 계산됩니다:

\[
V = \sqrt{\frac{\chi^2}{n \times (\min(k_1, k_2) - 1)}}
\]

- \( \chi^2 \): 카이제곱 통계량
- \( n \): 샘플 크기 (전체 샘플 수)
- \( k_1 \)과 \( k_2 \): 각각 두 범주형 변수의 수준(카테고리)의 개수

### **설명**

#### 1. **데이터 준비**
```python
df1 = df.copy()
df1.describe(include='object').columns
```
이 코드에서는 `df`를 복사하여 `df1`에 저장하고, **범주형 변수**만 선택하여 사용하기 위해 `df.describe(include='object')`를 이용해 `object` 타입(문자열 또는 범주형) 컬럼들만 추출합니다. 그 후, `.columns`로 그 컬럼들의 이름을 가져옵니다.

#### 2. **범주형 컬럼 리스트 설정**
```python
categorical_cols = ['Pclass','Sex','Embarked']
```
`categorical_cols`는 분석에 사용할 범주형 변수들을 포함하는 리스트입니다. 여기서는 `Pclass`, `Sex`, `Embarked` 세 개의 변수입니다.

#### 3. **Cramér's V 계산 및 출력**
```python
for col in categorical_cols:
  cramer_v = cramers_v(df1[col],df1['Survived'])
  print(f'{col}과의 Cramer\'s V : {cramer_v:.3f}')
```
- 이 부분에서는 `categorical_cols` 리스트에 있는 각 범주형 변수와 **생존 여부(Survived)** 간의 **Cramér's V** 값을 계산합니다.
- `cramers_v`는 Cramér's V를 계산하는 함수로, 각 범주형 변수(`col`)와 생존 여부(`df1['Survived']`) 간의 연관성을 계산하여 출력합니다.
- `cramer_v`의 값은 소수점 셋째 자리까지 출력됩니다.

### **Cramér's V 값을 해석하는 방법**
- **Cramér's V 값이 0에 가까운 경우**: 두 변수 간의 연관성이 거의 없다는 것을 의미합니다. 예를 들어, 성별(Sex)과 생존 여부(Survived)가 서로 연관성이 없다면 Cramér's V 값이 매우 작을 것입니다.
- **Cramér's V 값이 1에 가까운 경우**: 두 변수 간에 강한 연관성이 있다는 것을 의미합니다. 예를 들어, 특정 변수와 생존 여부 사이에 완벽한 연관성이 있을 때 Cramér's V는 1에 가까운 값을 가질 것입니다.
- **Cramér's V 값이 0.1 ~ 0.3 사이일 경우**: 두 변수 간에 약한 연관성 정도가 있을 수 있습니다.
- **Cramér's V 값이 0.3 ~ 0.5 사이일 경우**: 두 변수 간에 중간 정도의 연관성이 있다고 볼 수 있습니다.
- **Cramér's V 값이 0.5 이상일 경우**: 두 변수 간에 강한 연관성이 있다는 것을 나타냅니다.

### **주의할 점**
- **Cramér's V**는 **범주형 변수** 간의 연관성을 평가하는 지표로, 두 변수 모두 범주형이어야 합니다.
- 연속형 변수와 범주형 변수의 연관성을 분석하려면 **Point-Biserial Correlation** 또는 **ANOVA** 등을 사용해야 합니다.

### **결론**
이 코드는 범주형 변수(`Pclass`, `Sex`, `Embarked`)와 생존 여부(`Survived`) 간의 관계를 Cramér's V 값을 사용해 평가하는 방법을 보여줍니다. Cramér's V 값을 통해 각 변수 간의 연관성 강도를 파악할 수 있으며, 연관성이 강한 변수는 모델링에서 중요한 역할을 할 수 있습니다.

### **EDA (Exploratory Data Analysis, 탐색적 데이터 분석)란?**  
EDA는 데이터 분석을 시작할 때 **데이터의 특성을 이해하고 패턴을 찾기 위해 수행하는 과정**입니다. 즉, **데이터를 시각화하고 요약하여 의미 있는 정보나 패턴을 발견하는 과정**입니다.  

---

### **💡 왜 EDA가 중요한가?**
1. **데이터의 분포 파악** → 이상치(Outliers)나 결측치(Missing Values) 발견  
2. **변수 간 관계 분석** → 상관 관계나 패턴 확인  
3. **데이터 전처리 방향 결정** → 스케일링, 정규화, 결측치 처리 등  
4. **모델링 전략 수립** → 어떤 변수를 사용할지, 어떻게 전처리할지 결정  

---

### **🔍 EDA 주요 기법**
EDA는 여러 기법을 활용하여 데이터의 특성을 분석합니다.

#### **1️⃣ 기본적인 데이터 요약**
- `df.info()` → 데이터 타입 및 결측치 확인
- `df.describe()` → 수치형 데이터의 통계값 요약 (평균, 중앙값, IQR 등)
- `df.isnull().sum()` → 결측치 개수 확인

#### **2️⃣ 데이터 시각화**
- **히스토그램 (`histplot()`)** → 변수의 분포 확인  
- **박스플롯 (`boxplot()`)** → 이상치 탐색  
- **산점도 (`scatterplot()`)** → 두 변수 간의 관계 확인  
- **히트맵 (`heatmap()`)** → 상관관계 분석  

#### **3️⃣ 다변량 분석**
- **Pair Plot (`pairplot()`)** → 여러 변수 간 관계 시각화  
- **피어슨 상관계수 (`corr()`)** → 변수 간 상관관계 수치화  
- **카이제곱 검정 (`chi2_contingency()`)** → 범주형 변수 간 독립성 검정  

---

### **📌 예제: 타이타닉 데이터셋 EDA**
```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 데이터 로드
df = sns.load_dataset("titanic")

# 1️⃣ 기본 정보 확인
print(df.info())
print(df.describe())

# 2️⃣ 결측치 확인
print(df.isnull().sum())

# 3️⃣ 시각화 (연속형 변수 분포 확인)
sns.histplot(df['age'], bins=30, kde=True)
plt.show()

# 4️⃣ 생존자와 변수 관계 분석
sns.boxplot(x='survived', y='age', data=df)
plt.show()

# 5️⃣ 상관관계 분석
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.show()
```

---

### **🔎 정리**
EDA는 데이터의 특성을 파악하고, 분석 및 모델링에 필요한 전처리 작업을 결정하는 중요한 과정입니다. 기본적인 통계값 확인부터 시각화 기법을 활용한 변수 간 관계 분석까지 다양한 방법을 사용합니다.  

**EDA 없이 모델링을 진행하면?**  
→ 데이터의 문제를 제대로 파악하지 못해 **성능이 낮거나 왜곡된 모델**이 만들어질 가능성이 높아집니다.  

**즉, EDA는 데이터 과학의 필수적인 첫 단계! 🚀**

### **📌 머신러닝에서 모델 성능 평가 지표란?**  

머신러닝 모델이 얼마나 잘 작동하는지 판단하기 위해 **평가 지표(Evaluation Metrics)** 를 사용합니다.  
어떤 평가 지표를 사용하느냐에 따라 **"정확도가 높다"** 라는 말의 의미가 달라질 수 있습니다.  

---

## **1️⃣ 주요 평가 지표 (Classification: 분류 문제)**
**분류 문제(Classification)** 는 데이터를 특정 카테고리로 분류하는 작업입니다.  
예를 들어, 타이타닉 데이터에서 **"사망(0) / 생존(1)"** 을 예측하는 문제입니다.

### **✅ (1) 정확도 (Accuracy)**
> **정확도(Accuracy) = (올바르게 예측한 샘플 수) / (전체 샘플 수)**  
```python
from sklearn.metrics import accuracy_score

y_true = [0, 1, 1, 0, 1, 0, 1, 0, 1, 1]  # 실제 정답
y_pred = [0, 1, 1, 0, 0, 0, 1, 1, 1, 1]  # 모델 예측

accuracy = accuracy_score(y_true, y_pred)
print(f'정확도: {accuracy:.3f}')  # 출력 예: 0.9 (90% 정확도)
```
**⚠️ 문제점:** 데이터가 **불균형(imbalanced)** 하면 부적절한 지표  
예: 타이타닉에서 생존자가 30%뿐이라면, **모든 승객을 사망(0)으로 예측해도 70% 정확도** 달성 가능.  

---

### **✅ (2) 정밀도 (Precision)**
> **정밀도(Precision) = (True Positive) / (True Positive + False Positive)**  
즉, 모델이 **"생존(1)"** 이라고 예측한 사람들 중 **실제 생존한 비율**  

```python
from sklearn.metrics import precision_score

precision = precision_score(y_true, y_pred)
print(f'정밀도: {precision:.3f}')
```
**🔹 언제 중요한가?**  
- 스팸 필터: **정확하지 않은 메일을 스팸 처리하면 안 됨!**  
- 암 진단: **"암(양성)"으로 예측한 경우, 반드시 실제 암 환자여야 함!**  

---

### **✅ (3) 재현율 (Recall)**
> **재현율(Recall) = (True Positive) / (True Positive + False Negative)**  
즉, **실제 생존자 중에서 모델이 생존(1)이라고 예측한 비율**  
```python
from sklearn.metrics import recall_score

recall = recall_score(y_true, y_pred)
print(f'재현율: {recall:.3f}')
```
**🔹 언제 중요한가?**  
- **암 진단 모델**: 암 환자를 놓치면 안 되므로 재현율 중요!  
- **타이타닉 생존자 예측**: 생존자를 놓치면 생명을 잃을 수 있음.  

---

### **✅ (4) F1 점수 (F1 Score)**
> **F1 Score = 2 * (정밀도 * 재현율) / (정밀도 + 재현율)**  
정밀도와 재현율 간의 **균형을 맞추는 평가 지표**  
```python
from sklearn.metrics import f1_score

f1 = f1_score(y_true, y_pred)
print(f'F1 Score: {f1:.3f}')
```
**🔹 F1 Score가 중요한 이유**  
- 정확도만 높이고 싶은지, **잘못된 예측을 줄이는 것**이 더 중요한지에 따라 선택  
- 예: **의료, 금융, 보안 분야에서는 F1 Score를 자주 사용!**  

---

### **✅ (5) ROC-AUC (Area Under Curve)**
- **ROC Curve**: 모델이 분류 기준(Threshold)을 변화시키면서 **TPR(재현율)과 FPR(거짓 긍정 비율)** 을 나타낸 그래프  
- **AUC (Area Under Curve)**: ROC 곡선 아래 면적. **1에 가까울수록 성능이 좋음**  
```python
from sklearn.metrics import roc_auc_score

auc = roc_auc_score(y_true, y_pred)
print(f'ROC-AUC Score: {auc:.3f}')
```
**🔹 언제 중요한가?**  
- **이진 분류(Binary Classification)** 에서 많이 사용  
- 예: 신용카드 사기 탐지 (사기 거래를 정확히 검출하는 것이 중요함)  

---

## **2️⃣ 주요 평가 지표 (Regression: 회귀 문제)**
회귀 문제(Regression)는 **연속적인 숫자를 예측** 하는 문제입니다.  
예: 집값 예측, 타이타닉 승객의 생존 확률 예측.

### **✅ (1) 평균 절대 오차 (MAE, Mean Absolute Error)**
> **MAE = |실제값 - 예측값|의 평균**
```python
from sklearn.metrics import mean_absolute_error

y_true = [100, 200, 300, 400, 500]
y_pred = [110, 190, 310, 390, 480]

mae = mean_absolute_error(y_true, y_pred)
print(f'MAE: {mae:.3f}')
```
- 예측값이 실제값과 **얼마나 차이가 나는지** 절댓값 기준으로 평가  
- **해석이 직관적**(예: MAE = 10이면 평균적으로 10만 원 차이)

---

### **✅ (2) 평균 제곱 오차 (MSE, Mean Squared Error)**
> **MSE = (실제값 - 예측값)의 제곱 평균**  
```python
from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_true, y_pred)
print(f'MSE: {mse:.3f}')
```
- MAE와 유사하지만, **큰 오차에 더 민감** (제곱을 하기 때문)
- 이상치(Outlier)의 영향을 더 크게 받음  

---

### **✅ (3) 결정계수 (R² Score)**
> **R² = 1 - (SSE / SST)**  
- SSE (Sum of Squared Errors): 예측값과 실제값의 차이  
- SST (Total Sum of Squares): 전체 분산  
```python
from sklearn.metrics import r2_score

r2 = r2_score(y_true, y_pred)
print(f'R² Score: {r2:.3f}')
```
**🔹 R² Score의 해석**  
- **1에 가까울수록 좋음 (최대 1)**
- **0이면 모델이 쓸모없음** (평균값으로 예측한 것과 다름없음)  

---

## **3️⃣ 결론: 어떤 평가 지표를 선택할까?**
**👉 분류(Classification) 문제**  
- 데이터가 **균형 잡힌 경우** → `Accuracy`
- 데이터가 **불균형한 경우** → `Precision`, `Recall`, `F1 Score`
- **확률 기반 모델** → `ROC-AUC`

**👉 회귀(Regression) 문제**  
- 단순한 오차 확인 → `MAE`
- 이상치에 민감 → `MSE`
- 모델의 설명력 → `R² Score`

---
### **🔎 요약**
| 문제 유형  | 주요 평가 지표 | 설명 |
|-----------|--------------|---------------------|
| 분류 (Classification) | Accuracy | 단순한 정답률 |
|  | Precision | 예측한 Positive 중 진짜 Positive 비율 |
|  | Recall | 실제 Positive 중 모델이 찾은 비율 |
|  | F1 Score | Precision과 Recall의 균형 |
|  | ROC-AUC | 모델의 판별 성능을 나타내는 곡선 |
| 회귀 (Regression) | MAE | 절대 오차 평균 |
|  | MSE | 제곱 오차 평균 (이상치에 민감) |
|  | R² Score | 모델이 데이터를 설명하는 정도 |

---
📌 **결론:**  
- **통계학적 분석과 머신러닝 평가 지표는 서로 다름**  
- 머신러닝에서는 **"통계적으로 의미가 없는 특성"** 이라도 **모델이 학습할 때 성능을 높이는 경우가 많음**  
- 따라서 **"정확도가 높다"** 라는 말은 **어떤 평가 지표를 기준으로 판단하는지 먼저 확인**해야 함! 🚀

### **📌 머신러닝에서 스케일링 (Scaling)**
스케일링은 **특성(Feature) 값의 크기를 조정하는 과정**입니다. 머신러닝 모델에서 각 특성이 갖는 값의 크기 차이가 클 경우, 학습이 제대로 이루어지지 않거나 특정 특성이 모델의 가중치 결정에 불균형한 영향을 미칠 수 있습니다. 이를 방지하기 위해 데이터의 범위를 조정하는 것이 **스케일링**입니다.

---

## **🔹 스케일링이 필요한 이유**
1. **특성 간 크기 차이 해결**  
   - 예: 키(160~190cm), 몸무게(50~100kg), 연봉(2,000만~10억 원)  
   - 연봉처럼 큰 값을 가진 특성이 모델에 더 큰 영향을 미칠 수 있음 → 정규화 필요
2. **경사 하강법(Gradient Descent) 속도 향상**  
   - 큰 값이 존재하면 학습 속도가 느려지고, 최적점 탐색이 어려울 수 있음  
3. **거리 기반 알고리즘 성능 향상**  
   - KNN, K-means, PCA 등의 알고리즘은 유클리드 거리(Euclidean Distance)를 사용  
   - 거리 계산 시 특성이 큰 값 위주로 영향을 줌 → 스케일링 필요  

---

## **🔹 주요 스케일링 기법**
### **1️⃣ Min-Max Scaling (최소-최대 정규화)**
- 데이터 값을 **0~1 범위**로 변환
- 공식:  
  \[
  X' = \frac{X - X_{\min}}{X_{\max} - X_{\min}}
  \]
- 장점: 모든 데이터가 같은 범위를 가지므로 거리 기반 알고리즘에 유리  
- 단점: 이상치(Outlier)의 영향을 많이 받음  

#### **🔹 코드 예제**
```python
from sklearn.preprocessing import MinMaxScaler
import numpy as np

data = np.array([[100], [200], [300], [400], [500]])
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)
print(scaled_data)
```

---

### **2️⃣ Standardization (Z-score 정규화)**
- **평균 0, 표준편차 1**로 변환하여 정규 분포 형태로 만듦
- 공식:  
  \[
  X' = \frac{X - \mu}{\sigma}
  \]
  (여기서 \( \mu \)는 평균, \( \sigma \)는 표준편차)
- 장점: 이상치에 덜 민감하며, 대부분의 머신러닝 알고리즘에서 성능이 좋음  
- 단점: 0~1 범위로 고정되지 않음  

#### **🔹 코드 예제**
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)
print(scaled_data)
```

---

### **3️⃣ Robust Scaling (이상치에 강한 스케일링)**
- 중앙값(median)과 사분위 범위(IQR)를 사용하여 변환
- 공식:  
  \[
  X' = \frac{X - \text{Median}}{\text{IQR}}
  \]
  (여기서 **IQR** = Q3 - Q1)
- 장점: **이상치(Outlier)**에 강함  
- 단점: 데이터 분포에 따라 성능이 다를 수 있음  

#### **🔹 코드 예제**
```python
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
scaled_data = scaler.fit_transform(data)
print(scaled_data)
```

---

### **4️⃣ Log Scaling (로그 변환)**
- 데이터 값이 극단적으로 클 경우, **로그 변환**을 적용하여 정규 분포에 가깝게 변형  
- 공식:  
  \[
  X' = \log(X + 1)
  \]
- 장점: 이상치 완화, 데이터 분포를 정규 분포에 가깝게 조정  
- 단점: 데이터가 음수일 경우 적용하기 어려움  

#### **🔹 코드 예제**
```python
import numpy as np

data = np.array([[1], [10], [100], [1000], [10000]])
log_data = np.log1p(data)  # log(x+1) 적용
print(log_data)
```

---

## **🔹 스케일링 적용 기준**
| 기법 | 사용 조건 | 장점 | 단점 |
|------|----------|------|------|
| **Min-Max Scaling** | 값이 0~1 범위 내로 있어야 하는 경우 | 직관적이고 해석 쉬움 | 이상치에 민감 |
| **Standard Scaling (Z-score)** | 데이터가 정규 분포를 따를 때 | 이상치 영향 적음 | 0~1 범위로 변환되지 않음 |
| **Robust Scaling** | 이상치가 많은 경우 | 이상치 영향 적음 | 데이터가 균등하게 분포하지 않으면 성능 저하 |
| **Log Scaling** | 데이터의 값 범위가 크고, 정규 분포로 만들고 싶은 경우 | 데이터 분포를 조정 가능 | 음수 데이터 처리 어려움 |

---

## **🔹 정리**
- **선형 모델(Logistic Regression, Linear Regression)** → StandardScaler 추천
- **트리 기반 모델(Decision Tree, Random Forest)** → 스케일링 필요 없음
- **거리 기반 모델(KNN, K-means, SVM)** → MinMaxScaler 또는 StandardScaler 추천
- **이상치가 많은 경우** → RobustScaler 추천

스케일링을 적절히 적용하면 머신러닝 모델의 성능을 개선할 수 있습니다! 🚀
