# 가입고객 이탈 예측
판다스
```
엑셀처럼 데이터를 다룰수 있는 작은 프레임웍
시리즈
 1차원 배열
데이터 프레임
 2차원 배열

판다스의 인덱스의 개념은 키의 개념이 아니다.
```

슬라이싱 정수는 start : end - 1
슬라이싱 문자열은 start : end

판다스
```
시리즈 들의 집합
딕셔너리, 리스트, 넘파이 배열 등으로 생성가능
csv 파일이나 text, xlsx 등의 데이터를 판다스 객체로 생
```

Axis개념
(0, 1, 2, 3, 4) -> vector라 생각하기
axis = 0, axis = 1, axis = 2, axis = 3, axis = 4

데이터에서 NaN -> 결측(데이터가 없다)

df.info()시 중간에 쓰레기 값이 있을 수 있다.

수치형 데이터 리스트 뽑기
df.describe()

## **📌 평균과 중위수의 차이 + 이상치(Outlier) 고려**  

평균(Mean)과 중위수(Median)은 데이터의 중심 경향을 나타내는 대표적인 통계 지표지만, **이상치(Outlier)**가 포함되었을 때 두 값이 달라질 수 있습니다.  

---

## **1️⃣ 평균(Mean)과 중위수(Median)의 개념 정리**  
### ✅ **평균 (Mean)**
- 데이터 값들의 **총합을 데이터 개수로 나눈 값**  
\[
\text{Mean} = \frac{\sum X}{N}
\]
- 모든 데이터를 고려하기 때문에 **극단값(이상치)의 영향을 크게 받음**  
- 정규분포(대칭적인 분포)에서 대표값으로 많이 사용됨  

### ✅ **중위수 (Median)**
- 데이터를 **크기순으로 정렬했을 때 중앙에 위치한 값**  
- 데이터 개수가 **홀수**면 정가운데 값, **짝수**면 가운데 두 개의 평균  
- **이상치의 영향을 거의 받지 않음**, 데이터가 비대칭일 때 유용  

---

## **2️⃣ 이상치(Outlier)란?**
이상치는 **다른 데이터와 현저히 차이나는 극단적인 값**을 의미합니다.  
- **측정 오류**(데이터 입력 오류 등)  
- **극단적인 사건**(예: 특정 기업 CEO의 연봉)  
- **자연스러운 변동성**(예: 키, 몸무게 데이터에서 유독 큰 사람)  

🔹 **이상치의 예제**
```
[5, 7, 8, 9, 10, 15, 200]
```
→ 대부분 값이 5~15 범위에 있지만, **200**이 이상치.

---

## **3️⃣ 평균과 중위수의 차이 (이상치 포함 예제)**  

### **🔹 예제 1: 이상치가 없는 경우**
```
데이터: [1, 2, 3, 4, 5]
```
- 평균 = (1+2+3+4+5) / 5 = **3**  
- 중위수 = **3** (가운데 값)  
✅ **평균과 중위수가 동일**  

---

### **🔹 예제 2: 이상치가 있는 경우**
```
데이터: [1, 2, 3, 4, 100]
```
- 평균 = (1+2+3+4+100) / 5 = **22**  
- 중위수 = **3**  

❌ **평균이 22로 크게 왜곡됨!**  
✅ **중위수(3)는 이상치 영향을 거의 받지 않음**  

---

## **4️⃣ 평균 vs. 중위수 차이 및 활용 (이상치 포함)**  
|  | 평균(Mean) | 중위수(Median) |
|---|---|---|
| **계산 방식** | 모든 값을 더하고 개수로 나눔 | 데이터를 정렬한 후 중앙값 선택 |
| **이상치(Outlier) 영향** | 매우 큼 (왜곡 가능) | 거의 없음 (안정적) |
| **데이터 분포** | 정규분포(대칭)에 적합 | 비대칭 분포에도 적합 |
| **활용 사례** | 평균 연봉, 평균 점수, 온도 | 소득, 부동산 가격, 중앙 연봉 |

### **📍 실생활 예제: 연봉 데이터**
```
연봉 데이터: [3천만 원, 3천5백만 원, 4천만 원, 5천만 원, 10억 원]
```
- 평균 연봉: (0.3 + 0.35 + 0.4 + 0.5 + 10) / 5 = **2.31억 원**  
- 중위수 연봉: **4천만 원** (실제 대부분의 연봉과 유사)  

➡ **평균은 극단적인 부자(10억 원) 때문에 현실과 동떨어짐**  
➡ **이런 경우 중위수가 더 신뢰할 수 있는 지표**  

---

## **5️⃣ 결론: 평균과 중위수, 언제 사용할까?**
✅ **정규분포(대칭적인 데이터) → 평균 사용**  
✅ **이상치가 많거나, 데이터가 비대칭적일 때 → 중위수 사용**  
✅ **소득, 부동산, 의료 데이터 분석 → 중위수 활용 추천**  

💡 **즉, 평균은 ‘전체적인 경향’을, 중위수는 ‘실제 대표값’을 보여줍니다!**

`df.describe(include='object')`는 **문자열(범주형) 데이터**에 대한 요약 통계를 제공합니다.  
각 항목의 의미는 다음과 같습니다.  

| 항목 | 의미 |
|------|------|
| **count** | 해당 열의 **총 데이터 개수** (결측값 제외) |
| **unique** | **유일한 값의 개수** (중복 제외) |
| **top** | **가장 많이 등장한 값** (최빈값) |
| **freq** | **top 값이 등장한 횟수** |

---

### **🔹 예제**
```python
import pandas as pd

data = {'Category': ['A', 'B', 'A', 'C', 'B', 'A', 'A']}
df = pd.DataFrame(data)

print(df.describe(include='object'))
```
**📌 출력 결과**
```
       Category
count        7
unique       3
top          A
freq         4
```
📌 **해석:**  
- `count = 7` → 총 7개의 데이터  
- `unique = 3` → 'A', 'B', 'C' 총 3가지  
- `top = A` → 가장 많이 등장한 값  
- `freq = 4` → 'A'는 4번 등장  

✔ **문자열 데이터의 빈도 분석 시 유용함!**

# 데이터프레임
```
info()
  1. 데이터 타입 정보
  2. 결측치 여부
  3. 결측치가 없어도 수치형인데, 타입이 object이면, 수치형외에 다른데이터가 포함되어 이다.
  4. 인덱스가 정렬되어 있는지 여부
describe() 통계적 분석 4분위수 및 최대 최소 카운트
  -> 수치형 데이터
  -> 범주형 include = 'object'
      최빈값, 유니크, 빈도수,
  -> 위의 두가지를 다 출력 include='all'
isna() : 결측치 여부 확인
isna().sum() or isna().mean()
temp = df.isna().sum()
temp[temo>0] : 결측치가 있는 데이터의 컬럼명과 합을 시리즈로
```
### **📌 데이터프레임 분석 함수 요약**

1. **`info()`**  
   - **데이터 타입 정보**: 각 컬럼의 데이터 타입 확인  
   - **결측치 여부**: 각 컬럼의 결측치 개수 및 데이터 수량 확인  
   - **수치형 데이터의 object 타입 문제**: 수치형 데이터가 `object` 타입으로 표시되면, 비수치형 데이터가 포함된 경우가 있음  
   - **인덱스 정렬 여부**: 인덱스가 정렬되어 있는지 여부 확인  

   ```python
   df.info()
   ```

2. **`describe()`**  
   - **수치형 데이터**: 4분위수, 평균, 표준편차, 최소값, 최대값 등 통계적 정보 출력  
   - **범주형 데이터**: `include='object'`로 설정하여 **최빈값**, **유니크 값**, **빈도수** 출력  
   - **모든 데이터 출력**: `include='all'`을 사용하여 수치형과 범주형 데이터를 모두 출력  

   ```python
   df.describe()             # 기본 수치형 데이터
   df.describe(include='all')  # 수치형 + 범주형 데이터
   ```

3. **`isna()`**  
   - **결측치 여부**: 각 셀의 결측치 여부를 True/False로 반환  

   ```python
   df.isna()  # 결측치 여부 확인
   ```

4. **`isna().sum()`**  
   - 각 컬럼의 결측치 개수를 반환  

   ```python
   df.isna().sum()  # 결측치 개수
   ```

5. **`isna().mean()`**  
   - 각 컬럼의 결측치 비율을 반환  

   ```python
   df.isna().mean()  # 결측치 비율
   ```

6. **결측치가 있는 컬럼 확인**  
   - 결측치가 있는 컬럼명과 해당 컬럼의 결측치 개수를 시리즈로 반환  

   ```python
   temp = df.isna().sum()
   temp[temp > 0]  # 결측치가 있는 컬럼 및 개수
   ```

---

### **🚀 정리**  
- **`info()`**: 데이터 타입, 결측치 여부, 인덱스 정렬 여부 확인  
- **`describe()`**: 수치형/범주형 데이터의 통계적 요약  
- **`isna()`**: 결측치 여부 확인  
- **`isna().sum()`**: 결측치 개수  
- **`isna().mean()`**: 결측치 비율  
- **`isna().sum() > 0`**: 결측치가 있는 컬럼 확인

좋은 질문입니다! **`pd.DataFrame()`에서 `df`의 컬럼이 행으로 변하는 이유**에 대해 자세히 설명드리겠습니다.

### 📌 `DataFrame`에서 행과 열의 개념

- **행 (Row)**: 데이터의 수평적 항목. 각 행은 데이터베이스의 한 레코드, 혹은 `DataFrame`의 하나의 관측값을 나타냅니다.
- **열 (Column)**: 데이터의 수직적 항목. 각 열은 **특정 특성**이나 **속성**을 나타냅니다. 예를 들어, 나이, 성별, 수입 등.

따라서, **기본적으로 `DataFrame`은 `컬럼`을 세로로 보고, `행`은 가로로 봅니다.**

### 📌 `DataFrame`을 생성할 때, **딕셔너리**로 데이터를 넣는 경우:

```python
pd.DataFrame({
    'Data_Type': df.dtypes,
    'Missing_count': df.isna().sum(),
    'Missing_Ratio(%)': df.isna().mean() * 100
})
```

이 경우, **딕셔너리**의 **키**는 **열 이름**이 되고, **값**은 **각 컬럼에 대한 통계 값**입니다.

---

### 📌 왜 `df`의 **컬럼**이 **행**이 되는지?

이 코드를 작성했을 때, `df`의 **컬럼 이름**이 **행 인덱스**로 설정되는 이유는, `pd.DataFrame()` 안에서 **딕셔너리**의 **값**을 **열(column)**로 설정하고, **키**를 **행(index)**로 설정하기 때문입니다.

**딕셔너리**의 키를 **행 이름(index)**로 설정하고, 딕셔너리의 **값**을 **열(column)**로 취급하는 방식입니다.

즉, 다음과 같이 `pd.DataFrame()` 내부에서 **`df.columns`**를 행 이름으로 사용하고 있는 것입니다:

- `df.dtypes`, `df.isna().sum()`, `df.isna().mean() * 100`의 값들이 **열(column)**로 들어갑니다.
- **`df.columns`**에 들어 있는 **컬럼 이름**이 **행(index)**으로 자동 설정됩니다.

---

### 📌 예시로 살펴보자

#### 원본 DataFrame (`df`):

```python
import pandas as pd

df = pd.DataFrame({
    'age': [25, 30, 35, None, 45],
    'salary': [50000, 60000, None, 80000, 90000],
    'gender': ['M', 'F', 'M', 'F', None],
    'department': ['HR', 'IT', 'IT', 'HR', 'Finance']
})
```

#### 딕셔너리를 이용한 `summary` DataFrame 생성

```python
summary = pd.DataFrame({
    'Data_Type': df.dtypes,
    'Missing_count': df.isna().sum(),
    'Missing_Ratio(%)': df.isna().mean() * 100
})

print(summary)
```

**출력 결과:**

```
            Data_Type  Missing_count  Missing_Ratio(%)
age               float64              1               20.0
salary            float64              1               20.0
gender            object               1               20.0
department        object               0                0.0
```

#### **설명:**

- **열(Column)**: `Data_Type`, `Missing_count`, `Missing_Ratio(%)`는 딕셔너리의 **값**에 해당하는 열들입니다.
- **행(Row)**: `age`, `salary`, `gender`, `department`는 딕셔너리의 **키**입니다. 이 키들이 **행**으로 들어갑니다.

---

### 📌 요약

- **딕셔너리**에서 **값**들은 `DataFrame`의 **열**로 배치됩니다.
- **딕셔너리의 키**는 **행 이름(index)**로 사용됩니다.
- `df`의 **컬럼 이름**들은 **행 이름(index)**로 사용되어 결과적으로 `summary`에서 **행**으로 출력됩니다.

이렇게 **열**과 **행**을 정의하는 방식은 **딕셔너리의 키와 값**이 어떻게 배치되는지에 따라 결정됩니다.

### 📌 `pd.crosstab()` 언제 사용할까?

`pd.crosstab()`은 **두 개 이상의 범주형 변수(카테고리 데이터)** 간의 관계를 분석할 때 사용합니다.  
주로 **빈도수를 계산**하거나 **비율을 확인**할 때 유용합니다.

---

## 1️⃣ `crosstab()` 기본 사용법: **빈도수 계산**
```python
import pandas as pd

# 예제 데이터 생성
df = pd.DataFrame({
    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male'],
    'Purchase': ['Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No']
})

# 교차표 생성
result = pd.crosstab(df['Gender'], df['Purchase'])
print(result)
```
### **출력 결과**
```
Purchase  No  Yes
Gender           
Female     1    2
Male       2    2
```
🔹 `Gender`와 `Purchase`의 관계를 **빈도수**로 나타냄  
🔹 `Male`과 `Female`이 `Yes` 또는 `No`를 선택한 횟수를 집계  

---

## 2️⃣ `crosstab()`으로 **비율(정규화) 계산**
```python
result = pd.crosstab(df['Gender'], df['Purchase'], normalize=True)
print(result)
```
### **출력 결과**
```
Purchase    No   Yes
Gender              
Female    0.142 0.285
Male      0.285 0.285
```
🔹 전체 데이터에서 **비율**을 계산 (`normalize=True`)  
🔹 `0.142`는 **전체 데이터 중 해당 항목의 비율**  

---

## 3️⃣ `crosstab()`으로 **행(row) 기준 비율 계산**
```python
result = pd.crosstab(df['Gender'], df['Purchase'], normalize='index')
print(result)
```
### **출력 결과**
```
Purchase    No  Yes
Gender             
Female    0.33 0.67
Male      0.50 0.50
```
🔹 각 **행(row)에서의 비율**을 계산 (`normalize='index'`)  
🔹 **Female 중 Yes 선택 비율 = 67%**, **Male 중 Yes 선택 비율 = 50%**  

---

## 4️⃣ `crosstab()`에 **합계 추가**
```python
result = pd.crosstab(df['Gender'], df['Purchase'], margins=True)
print(result)
```
### **출력 결과**
```
Purchase  No  Yes  All
Gender                
Female     1    2    3
Male       2    2    4
All        3    4    7
```
🔹 **`margins=True`**: **총합(`All`)을 추가**하여 전체 빈도 표시  

---

## 5️⃣ `crosstab()`을 활용한 **다중 변수 분석**
```python
df['Age_Group'] = ['Young', 'Adult', 'Young', 'Adult', 'Young', 'Adult', 'Young']

result = pd.crosstab([df['Gender'], df['Age_Group']], df['Purchase'])
print(result)
```
### **출력 결과**
```
Purchase            No  Yes
Gender Age_Group           
Female Adult        1    1
       Young       0    1
Male   Adult       1    1
       Young       1    1
```
🔹 **두 개의 기준(Gender, Age_Group)에 대한 분석** 가능  
🔹 `pd.crosstab([df['Gender'], df['Age_Group']], df['Purchase'])` 사용  

---

## 📌 `crosstab()`을 **언제 사용하면 좋을까?**
✅ **두 개 이상의 범주형 변수 간 관계 분석**  
✅ **빈도수(횟수) 계산**  
✅ **비율 계산(normalize=True or normalize='index')**  
✅ **다중 변수(2개 이상) 분석 가능**  
✅ **데이터 집계 및 요약표 생성**  

📌 **결론:**  
✔️ `crosstab()`은 **Pivot Table**처럼 범주형 데이터 분석 시 유용한 도구입니다!

### 📌 `groupby()`를 사용한 그룹핑 및 변형  
아래 코드들은 `groupby()`를 이용해 데이터를 그룹화하고, 이를 변형하는 과정입니다.

---

## **1️⃣ `groupby(['시도명', '상권업종대분류명']).size()`**
```python
merged_df.groupby(['시도명', '상권업종대분류명']).size()
```
✅ **설명:**  
- `groupby(['시도명', '상권업종대분류명'])` → `시도명`과 `상권업종대분류명`을 기준으로 데이터를 그룹화  
- `.size()` → 각 그룹의 개수를 세어서 **빈도수(Count)**를 반환  

**예제 데이터**
| 시도명   | 상권업종대분류명 | 매장명   |
|---------|--------------|--------|
| 서울    | 음식점       | A      |
| 서울    | 음식점       | B      |
| 서울    | 카페         | C      |
| 부산    | 음식점       | D      |
| 부산    | 카페         | E      |
| 부산    | 카페         | F      |

**출력 결과**
```
시도명  상권업종대분류명
서울   음식점       2
      카페        1
부산   음식점       1
      카페        2
dtype: int64
```
---

## **2️⃣ `groupby().size().T` (전치)**
```python
merged_df.groupby(['시도명', '상권업종대분류명']).size().T
```
✅ **설명:**  
- `.T`는 데이터의 **행과 열을 뒤집는(transpose)** 연산  
- 하지만 `.size()`의 결과는 **Series**이므로 `.T`를 해도 변화 없음!  
- 즉, `.T`는 의미 없는 연산

✅ **`.T`를 유의미하게 만들려면?**  
→ `.unstack()`을 이용해야 함!

---

## **3️⃣ `groupby().size().unstack(fill_value=0)`**
```python
merged_df.groupby(['시도명', '상권업종대분류명']).size().unstack(fill_value=0)
```
✅ **설명:**  
- `.unstack()` → **두 번째 그룹핑한 변수(`상권업종대분류명`)를 열(column)로 변환**  
- `fill_value=0` → **결측값을 0으로 채움** (매장이 없는 경우)

**출력 결과 (테이블 형태)**
```
상권업종대분류명  음식점  카페
시도명             
서울        2   1
부산        1   2
```
✔️ **이제 `시도명`을 기준으로 행(row), `상권업종대분류명`을 기준으로 열(column)로 변환됨!**  
✔️ `.T`를 적용하면 **더 깔끔한 표 형태**가 됨.

---

### 📌 **정리**
| 코드 | 설명 |
|------|------|
| `groupby().size()` | 그룹별 개수 계산 (Series 형태) |
| `groupby().size().T` | `.size()` 결과는 Series라 `.T` 적용해도 의미 없음 |
| `groupby().size().unstack(fill_value=0)` | 그룹핑 결과를 표(Table) 형태로 변환 |

✔️ **결론**:  
→ `unstack(fill_value=0)`를 사용하면 `groupby()` 결과를 **더 직관적인 표 형태**로 변환 가능! 🚀

### **(1,3)과 (3,3)의 덧셈이 가능한 이유: NumPy 브로드캐스팅**  

맞아! 행렬 연산을 일반적인 선형대수처럼 생각하면 `(1,3) + (3,3)`은 불가능해 보일 수 있어.  
하지만 **NumPy의 브로드캐스팅(Broadcasting) 규칙** 덕분에 자동으로 연산이 가능해!  

---

## **📌 (1,3)과 (3,3) 더하기 예제**
```python
import numpy as np

A = np.array([[1, 2, 3]])  # (1,3) → 2D 배열
B = np.array([[10, 20, 30], [40, 50, 60], [70, 80, 90]])  # (3,3)

C = A + B  # 브로드캐스팅 적용!
print(C)
```
✅ **출력 결과**
```
[[ 11  22  33]
 [ 41  52  63]
 [ 71  82  93]]
```

---

## **📌 (1,3)과 (3,3)이 연산되는 과정**
NumPy의 브로드캐스팅 규칙에 따라 `(1,3)`을 `(3,3)` 크기로 **자동 확장**해서 더해 줘.

| **A (1×3) → 브로드캐스팅 → (3×3)** | **B (3×3)** | **A + B (결과)** |
|------|------|------|
| `[1, 2, 3]` | `[10, 20, 30]` | `[11, 22, 33]` |
| `[1, 2, 3]` | `[40, 50, 60]` | `[41, 52, 63]` |
| `[1, 2, 3]` | `[70, 80, 90]` | `[71, 82, 93]` |

✔ **브로드캐스팅 과정:**  
1. `(1,3)`을 `(3,3)` 크기로 확장 (행을 3번 복제)  
2. 같은 위치의 요소끼리 더함  
3. 최종적으로 `(3,3)` 결과 반환  

---

## **📌 NumPy 브로드캐스팅 규칙**
두 배열이 연산될 때 **각 축의 크기를 비교**해서  
1. **같으면 그대로 진행**  
2. **한쪽이 1이면 확장**  
3. **다르면 오류 발생**  

✔ **예제 1** `(1,3) + (3,3)`  
   - `(1,3)` → `(3,3)`으로 확장 후 연산 ✅  
   
✔ **예제 2** `(3,1) + (3,3)`  
   - `(3,1)` → `(3,3)`으로 확장 후 연산 ✅  
   
✔ **예제 3** `(3,2) + (3,3)`  
   - 크기가 맞지 않아 **오류 발생** ❌  

---

## **📌 행렬 연산과 다르게 생각해야 하는 이유**
NumPy는 **선형대수(행렬 연산)**가 아니라 **원소별(Element-wise) 연산**이 기본이야.  
행렬 곱셈(`np.dot()`)처럼 규칙이 정해진 연산이 아니라,  
**각 요소를 대응시키고 부족한 부분을 자동으로 채워주는 방식**으로 동작해.

💡 **즉, "행렬 덧셈"이 아니라 "브로드캐스팅된 요소별 덧셈"이 적용되는 것!**  

---

## **✅ 결론**
✔ `(1,3) + (3,3)`은 **(1,3)이 (3,3)으로 확장**되면서 연산이 가능해짐  
✔ NumPy는 **선형대수적인 행렬 연산이 아니라 원소별(Element-wise) 연산을 기본**으로 함  
✔ 브로드캐스팅 덕분에 **반복문 없이 효율적으로 연산 가능**  

이제 브로드캐스팅 개념이 더 명확해졌을 거야! 🚀🔥

### 표준편차란 무엇인가?

**표준편차**(Standard Deviation)는 데이터 값들이 평균값을 중심으로 얼마나 퍼져 있는지를 나타내는 통계적인 척도입니다. 표준편차가 작으면 데이터들이 평균값에 가까운 값들을 가지며, 표준편차가 크면 데이터들이 평균값에서 멀리 분포하고 있음을 의미합니다.

### 표준편차 계산 방법:
1. **평균 계산**  
   모든 데이터 값의 평균을 구합니다.
   
2. **각 데이터에서 평균 빼기**  
   각 데이터 값에서 평균을 빼고 차이를 계산합니다.
   
3. **차이의 제곱 구하기**  
   차이를 제곱합니다. 제곱을 하는 이유는 두 가지입니다:
   - **부호 제거**: 음수와 양수의 차이를 동일하게 취급하기 위해 제곱합니다. 음수와 양수의 차이는 절대 값만 중요하기 때문에, 제곱을 통해 모든 차이가 양수로 바뀌게 됩니다.
   - **차이 강조**: 차이가 클수록 제곱된 값이 더 커져, 큰 차이를 더 뚜렷하게 강조할 수 있습니다.
   
4. **제곱값들의 평균 구하기 (분산 계산)**  
   제곱값들의 평균을 구하여 **분산**을 계산합니다.

5. **표준편차 구하기**  
   분산의 제곱근을 구하여 **표준편차**를 계산합니다.

### 수식:
\[
\text{표준편차} (\sigma) = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2}
\]
- \( N \)은 데이터의 개수
- \( x_i \)는 각 데이터 값
- \( \mu \)는 평균

### 예시: 데이터 [1, 2, 3, 4, 5]
1. **평균**:  
   \[
   \text{평균} = \frac{1 + 2 + 3 + 4 + 5}{5} = 3
   \]
   
2. **각 값에서 평균 빼기**:  
   - \( 1 - 3 = -2 \)
   - \( 2 - 3 = -1 \)
   - \( 3 - 3 = 0 \)
   - \( 4 - 3 = 1 \)
   - \( 5 - 3 = 2 \)
   
3. **차이 제곱**:  
   - \( (-2)^2 = 4 \)
   - \( (-1)^2 = 1 \)
   - \( (0)^2 = 0 \)
   - \( (1)^2 = 1 \)
   - \( (2)^2 = 4 \)
   
4. **분산 계산**:  
   \[
   \text{분산} = \frac{4 + 1 + 0 + 1 + 4}{5} = 2
   \]
   
5. **표준편차 계산**:  
   \[
   \text{표준편차} = \sqrt{2} \approx 1.41
   \]

### 결과:
- **평균** = 3
- **표준편차** ≈ 1.41

### 제곱을 하는 이유:
- **부호 제거**: 음수와 양수의 차이를 동일하게 취급하려면 차이를 제곱해야 합니다.
- **차이 강조**: 제곱을 통해 큰 차이가 더 강조됩니다.
- **수학적 안정성**: 제곱 후에 음수가 될 수 없으므로, 통계적으로 안정적이고 의미 있는 값을 얻게 됩니다.

표준편차는 데이터의 변동성을 평가할 때 유용한 지표로 사용됩니다.

결측치를 확인하고 제거하는 작업은 데이터 전처리에서 매우 중요한 단계입니다. `pandas`를 사용하여 결측치를 확인하고 제거하는 방법을 자세히 설명드리겠습니다.

### 1. **결측치 확인하기**
데이터셋에서 결측치가 있는 컬럼을 확인하려면 `isnull()` 메서드를 사용하고, `sum()`을 사용하여 각 컬럼별 결측치의 개수를 확인할 수 있습니다.

```python
# 결측치 확인
df.isnull().sum()
```

이 코드는 각 컬럼에서 결측치가 얼마나 있는지 출력합니다.

### 2. **결측치 시각적으로 확인하기 (선택 사항)**
`seaborn`을 사용하여 결측치를 시각적으로 확인할 수도 있습니다. `heatmap()`을 사용하여 결측치를 시각화할 수 있습니다.

```python
import seaborn as sns
import matplotlib.pyplot as plt

# 결측치 히트맵 시각화
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.show()
```

이렇게 하면 결측치가 있는 부분이 시각적으로 강조되어 나타납니다.

### 3. **결측치 처리 방법**
결측치를 처리하는 방법에는 여러 가지가 있으며, 주로 사용되는 방법은 다음과 같습니다:

#### (1) **결측치 삭제하기**
결측치가 포함된 행 또는 열을 삭제할 수 있습니다. `dropna()` 메서드를 사용하여 결측치를 삭제합니다.

- **결측치가 포함된 행 삭제**:
  ```python
  df = df.dropna()
  ```

- **결측치가 포함된 열 삭제**:
  ```python
  df = df.dropna(axis=1)
  ```

#### (2) **결측치 대체하기**
결측치를 삭제하는 대신, 적절한 값으로 대체할 수 있습니다. `fillna()` 메서드를 사용하여 결측치를 특정 값으로 대체할 수 있습니다.

- **특정 값으로 대체하기**:
  예를 들어, `Age` 컬럼의 결측치는 **평균** 또는 **중앙값**으로 대체할 수 있습니다.
  ```python
  df['Age'].fillna(df['Age'].mean(), inplace=True)  # 평균으로 대체
  ```

- **최빈값(모드)으로 대체하기**:
  예를 들어, `Embarked` 컬럼의 결측치는 최빈값으로 대체할 수 있습니다.
  ```python
  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)  # 최빈값으로 대체
  ```

- **특정 값으로 대체하기 (사용자가 정의한 값)**:
  ```python
  df['Fare'].fillna(0, inplace=True)  # 0으로 대체
  ```

#### (3) **결측치 처리 후 확인**
결측치를 처리한 후, 다시 한 번 결측치가 제대로 처리되었는지 확인합니다.

```python
df.isnull().sum()
```

이제 `isnull().sum()`을 실행하면 모든 컬럼의 결측치가 처리된 상태여야 합니다.

### 예시 코드:
```python
import pandas as pd

url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'
df = pd.read_csv(url)

# 결측치 확인
print("결측치 개수 확인:")
print(df.isnull().sum())

# 결측치 대체 (Age 컬럼은 평균값으로, Embarked 컬럼은 최빈값으로 대체)
df['Age'].fillna(df['Age'].mean(), inplace=True)
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# 결측치가 제대로 처리되었는지 확인
print("\n결측치 처리 후:")
print(df.isnull().sum())
```

### 결론:
- **`isnull().sum()`**: 결측치가 있는 컬럼과 결측치 개수를 확인합니다.
- **`dropna()`**: 결측치가 있는 행 또는 열을 삭제합니다.
- **`fillna()`**: 결측치를 적절한 값(평균, 중앙값, 최빈값 등)으로 대체합니다.

이 과정들을 통해 데이터셋의 결측치를 처리할 수 있습니다.
