# **PCA(주성분 분석) 및 인공신경망 개요 + 예제**

---

## **1. PCA (Principal Component Analysis, 주성분 분석)**

### 📌 **PCA란?**
- 다차원 데이터를 **차원 축소**하여 주요 패턴을 추출하는 **통계 기법**  
- 데이터의 **분산(Variance)**을 최대화하는 방향으로 새로운 좌표계를 설정함  
- 설정된 좌표계를 **주성분(Principal Component)**이라 하며, 정보 손실을 최소화하면서 차원을 축소하는 것이 목적  

### **PCA 절차 + 예제**
1️⃣ **데이터 표준화**  
   - 특성 간의 스케일 차이를 없애기 위해 **평균을 0, 표준편차를 1**로 변환  
   - 📌 **예제 (Python 코드)**  
     ```python
     from sklearn.preprocessing import StandardScaler
     import numpy as np

     # 예제 데이터 (학생 수학, 과학 성적)
     data = np.array([[85, 90], [88, 85], [75, 80], [95, 95], [80, 75]])

     # 표준화
     scaler = StandardScaler()
     data_std = scaler.fit_transform(data)
     print(data_std)
     ```

2️⃣ **공분산 행렬 계산**  
   - 변수들 간의 관계(상관성, 분산)를 나타내는 **공분산 행렬** 계산  
   - 📌 **예제**
     ```python
     cov_matrix = np.cov(data_std.T)
     print(cov_matrix)
     ```

3️⃣ **고유값(Eigenvalue)과 고유벡터(Eigenvector) 계산**  
   - 공분산 행렬을 **고유값 분해**하여 주성분의 방향과 크기 결정  
   - 📌 **예제**
     ```python
     from numpy.linalg import eig
     eigenvalues, eigenvectors = eig(cov_matrix)
     print("고유값:", eigenvalues)
     print("고유벡터:", eigenvectors)
     ```

4️⃣ **주성분 선택**  
   - 고유값이 큰 순서대로 주성분을 선택  
   - 보통 **전체 분산의 80~90% 이상**을 설명하는 주성분 개수를 선택  

5️⃣ **데이터 변환**  
   - **고유벡터 행렬**을 사용해 원래 데이터를 새로운 좌표계(주성분 공간)로 변환  
   - 📌 **예제**
     ```python
     pca_data = np.dot(data_std, eigenvectors)
     print("PCA 변환 데이터:", pca_data)
     ```

---

### **📊 PCA를 실제 데이터에 적용해보기**
```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris

# 데이터 불러오기
iris = load_iris()
X = iris.data  # 특성 데이터

# PCA 적용 (2차원으로 축소)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# 시각화
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=iris.target, palette="Set1")
plt.xlabel("주성분 1")
plt.ylabel("주성분 2")
plt.title("Iris 데이터 PCA 변환")
plt.show()
```
✅ **결과**: 4차원 데이터를 2차원으로 축소하여 시각화 가능

---

## **2. 인공신경망(Artificial Neural Network, ANN)**

### 📌 **신경망이란?**
- **생물학적 신경망(Neuron)의 구조**를 본떠 만든 모델  
- **입력층(Input Layer), 은닉층(Hidden Layer), 출력층(Output Layer)**으로 구성됨  
- **활성화 함수(Activation Function)**를 사용하여 일정 임계치를 넘으면 다음 층으로 신호 전달  

---

### **1️⃣ 로지스틱 회귀(Logistic Regression)**
- **이진 분류(Binary Classification)** 모델  
- 📌 **예제 (MNIST 숫자 이미지 분류 - 로지스틱 회귀)**
  ```python
  from sklearn.linear_model import LogisticRegression
  from sklearn.datasets import load_digits
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import accuracy_score

  # 데이터 로드
  digits = load_digits()
  X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)

  # 모델 학습
  model = LogisticRegression(max_iter=10000)
  model.fit(X_train, y_train)

  # 예측 및 정확도 평가
  y_pred = model.predict(X_test)
  print("로지스틱 회귀 정확도:", accuracy_score(y_test, y_pred))
  ```
✅ **결과**: 숫자 인식 성능 확인 가능  

---

### **2️⃣ 인공신경망(ANN) 모델 구현**
- 📌 **예제 (Fashion MNIST 데이터 - 신경망 적용)**
  ```python
  import tensorflow as tf
  from tensorflow.keras import layers, models
  from tensorflow.keras.datasets import fashion_mnist
  import matplotlib.pyplot as plt

  # 데이터 로드 및 전처리
  (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
  X_train, X_test = X_train / 255.0, X_test / 255.0  # 정규화

  # 신경망 모델 정의
  model = models.Sequential([
      layers.Flatten(input_shape=(28, 28)),  # 2D 이미지를 1D로 변환
      layers.Dense(128, activation='relu'),  # 은닉층
      layers.Dense(10, activation='softmax') # 출력층 (10개 클래스)
  ])

  # 모델 컴파일
  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

  # 모델 학습
  model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

  # 모델 평가
  test_loss, test_acc = model.evaluate(X_test, y_test)
  print("신경망 테스트 정확도:", test_acc)
  ```
✅ **결과**: 패션 MNIST 분류 성능 확인 가능  

---

## **3. PCA와 신경망 결합하기**
1️⃣ PCA를 사용하여 차원 축소  
2️⃣ 축소된 데이터를 신경망(ANN)에 입력  
3️⃣ 연산량을 줄이고 모델 성능을 비교  

📌 **PCA + ANN 예제**
```python
# PCA 적용
pca = PCA(n_components=50)  # 784차원 → 50차원 축소
X_train_pca = pca.fit_transform(X_train.reshape(-1, 28*28))
X_test_pca = pca.transform(X_test.reshape(-1, 28*28))

# 신경망 모델 정의
model = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(50,)),  # 입력 차원 변경
    layers.Dense(10, activation='softmax')
])

# 모델 학습
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train_pca, y_train, epochs=10, validation_data=(X_test_pca, y_test))
```
✅ **결과**: PCA로 축소 후 ANN 적용하여 성능 평가 가능  

---

## **4. 요약**
✅ **PCA(주성분 분석)**:  
- 데이터의 차원을 축소하여 주요 정보를 보존하는 기법  
- **설명된 분산 비율**을 통해 주성분 개수 결정  
- **분류기와 함께 사용 가능하나 성능이 낮아질 수도 있음**  

✅ **인공신경망(ANN)**:  
- 다층 퍼셉트론(MLP) 구조를 사용하여 비선형 문제 해결  
- 딥러닝(Deep Learning)의 기초  

✅ **PCA + 신경망 조합 가능**  
- **PCA로 차원을 줄이고 ANN을 적용하여 성능 비교 가능**  
- 계산량을 줄일 수 있지만 성능이 떨어질 수도 있음  

이제 PCA와 신경망을 실제 데이터에 활용해서 실험해보면 좋을 것임! 🚀

## **📌 머신러닝 분류 모델에 적용 - 랜덤포레스트 (RandomForestClassifier)**

### **1️⃣ 랜덤포레스트 개요**
- 랜덤포레스트(RandomForest)는 여러 개의 **의사결정나무(Decision Tree)**를 조합하여 예측을 수행하는 **앙상블 학습(Ensemble Learning)** 방법임.
- 개별 트리가 과적합되는 것을 방지하고 **더 높은 예측 정확도를 제공**하는 장점이 있음.
- 특히 **범주형 데이터 처리에 강함** → 수치형 데이터를 범주형으로 변환하면 성능이 향상될 수 있음.

---

### **2️⃣ 랜덤포레스트 적용 과정**
#### **① 데이터 준비**
👉 `UCI Adult Income Dataset` 사용 (수입이 50K 이상인지 예측)

```python
# 필요한 라이브러리 불러오기
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# UCI 데이터 로드
from ucimlrepo import fetch_ucirepo
adult = fetch_ucirepo(id=2)

# 특성과 타겟 분리
X = adult.data.features
y = adult.data.targets
```

---

#### **② 데이터 전처리**
✅ **수치형 데이터 표준화(Standardization)**
✅ **범주형 데이터 인코딩(Label Encoding)**

```python
# 범주형 데이터 라벨 인코딩
label_encoders = {}
for col in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# 데이터 분할 (80% 학습, 20% 테스트)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

#### **③ 랜덤포레스트 모델 학습**
```python
# 랜덤포레스트 모델 학습
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# 예측 및 정확도 평가
y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"랜덤포레스트 정확도: {accuracy:.4f}")
```

✅ **결과:**  
랜덤포레스트 모델이 수입을 예측하는 정확도를 계산할 수 있음.

---

### **3️⃣ 파이프라인(Pipeline) 자동화**
- 전처리 + 모델 학습 + 예측을 하나의 흐름으로 자동화  
- **장점:** 여러 단계의 처리를 한 번에 실행 가능

```python
# 랜덤포레스트 파이프라인 구축
pipeline = Pipeline([
    ('scaler', StandardScaler()),       # 스케일링
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))  # 모델
])

# 파이프라인 학습
pipeline.fit(X_train, y_train)

# 파이프라인 예측
y_pred_pipeline = pipeline.predict(X_test)
accuracy_pipeline = accuracy_score(y_test, y_pred_pipeline)

print(f"파이프라인 적용 후 정확도: {accuracy_pipeline:.4f}")
```

✅ **결과:**  
- **파이프라인을 사용하면 데이터 전처리부터 모델 학습까지 자동으로 진행됨.**
- **코드 유지보수 및 확장성이 증가함.**

---

## **📌 인공신경망 (Artificial Neural Network, ANN)**
### **1️⃣ 개요**
- 인간의 **뉴런(Neuron) 구조**를 모방하여 설계된 신경망 모델
- 입력층(Input Layer) → 은닉층(Hidden Layer) → 출력층(Output Layer) 구조
- **활성화 함수(Activation Function)**를 사용하여 입력을 비선형 변환하여 학습
- **이미지, 텍스트, 음성 데이터 분석 등에 사용됨**

---

### **2️⃣ ANN 적용 예제 (Fashion MNIST 데이터셋)**
✅ **Fashion MNIST 데이터셋**:  
- 28x28 픽셀의 흑백 이미지로 이루어진 **10개 의류 카테고리 분류**
- 신경망을 사용해 옷의 종류를 분류하는 문제

#### **① 데이터 로드 및 전처리**
```python
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# 데이터 로드
fashion_mnist = tf.keras.datasets.fashion_mnist
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# 데이터 정규화 (0~255 → 0~1)
X_train, X_test = X_train / 255.0, X_test / 255.0
```

---

#### **② 신경망 모델 설계**
✅ **Dense 레이어**를 쌓아 Fully Connected 신경망 생성  
✅ **ReLU 활성화 함수** 사용  
✅ **Softmax 함수**로 10개 클래스 분류  

```python
# 신경망 모델 정의
model = models.Sequential([
    layers.Flatten(input_shape=(28, 28)),       # 2D → 1D 변환
    layers.Dense(128, activation='relu'),       # 은닉층 1 (128개 뉴런, ReLU)
    layers.Dense(64, activation='relu'),        # 은닉층 2 (64개 뉴런, ReLU)
    layers.Dense(10, activation='softmax')      # 출력층 (10개 클래스, Softmax)
])

# 모델 컴파일
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 모델 요약 출력
model.summary()
```

---

#### **③ 모델 학습 및 평가**
```python
# 모델 학습
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

# 모델 평가
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
print(f"테스트 정확도: {test_acc:.4f}")
```

✅ **결과:**  
Fashion MNIST 데이터셋을 학습한 후, **의류를 10개 카테고리로 분류하는 신경망 모델**을 구축할 수 있음.

---

## **📌 결론**
| 모델 | 특징 | 예제 |
|------|------|------|
| **랜덤포레스트** | 다수의 의사결정나무를 조합해 성능 향상 | 성인 소득 예측 (50K 이상 여부) |
| **파이프라인** | 데이터 전처리 + 모델 학습을 자동화 | `Pipeline`을 사용한 학습 |
| **인공신경망** | 뉴런 구조를 모방, 딥러닝의 기본 구조 | Fashion MNIST 의류 분류 |

🚀 **정리:**  
- 랜덤포레스트는 **분류 문제에서 성능이 뛰어나며, 파이프라인을 사용하면 자동화 가능**  
- 인공신경망(ANN)은 **비선형 데이터를 처리할 수 있으며, 이미지 및 복잡한 데이터 분석에 적합**  
- 각 방법을 **데이터 특성에 맞게 선택하여 적용해야 함.**

### **1. 검증 손실 (Validation Loss)**
#### **📌 개념**
- 모델이 훈련 데이터(training set)에서는 잘 동작하지만, 검증 데이터(validation set)에서는 성능이 떨어질 수 있음.  
- 훈련 손실(training loss)과 검증 손실(validation loss)을 비교하면 **과적합(overfitting)** 여부를 판단 가능.  
- 과적합이 발생하면 검증 손실이 **증가**하지만 훈련 손실은 계속 감소함.  

#### **📌 예제: 검증 손실 확인**
```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten
import matplotlib.pyplot as plt

# MNIST 데이터 로드
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# 데이터 정규화 (0~255 → 0~1)
x_train, x_test = x_train / 255.0, x_test / 255.0

# 훈련 데이터와 검증 데이터로 분리
x_train, x_val = x_train[:50000], x_train[50000:]  # 50000개 훈련, 10000개 검증
y_train, y_val = y_train[:50000], y_train[50000:]

# 모델 생성
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# 모델 컴파일
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 모델 훈련 (검증 데이터 포함)
history = model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val))

# 훈련 손실 및 검증 손실 시각화
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training vs. Validation Loss')
plt.show()
```
🔹 `validation_data=(x_val, y_val)`: 검증 데이터 제공  
🔹 `history.history['loss']`: 훈련 데이터의 손실  
🔹 `history.history['val_loss']`: 검증 데이터의 손실  

**✅ 결과 해석**
- 두 그래프가 같이 감소하면 **훈련이 잘 진행**됨.  
- 검증 손실이 증가하는 시점에서 **과적합** 발생.  

---

### **2. 드롭아웃 (Dropout)**
#### **📌 개념**
- 신경망이 특정 뉴런에 과하게 의존하는 것을 방지하기 위해 **일부 뉴런을 랜덤하게 제거(drop)** 하는 기법.  
- 과적합을 줄이는 **정규화(regularization) 방법** 중 하나.  
- 훈련 중에는 뉴런을 랜덤으로 꺼버리지만, 테스트 시에는 모든 뉴런을 활성화하여 예측 수행.  

#### **📌 예제: 드롭아웃 적용**
```python
from tensorflow.keras.layers import Dropout

model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dropout(0.5),  # 50% 뉴런을 랜덤으로 비활성화
    Dense(64, activation='relu'),
    Dropout(0.3),  # 30% 뉴런 비활성화
    Dense(10, activation='softmax')
])

# 모델 컴파일 및 훈련
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val))
```
🔹 `Dropout(0.5)`: 50% 확률로 뉴런 비활성화  
🔹 `Dropout(0.3)`: 30% 확률로 뉴런 비활성화  

**✅ 결과**
- 드롭아웃을 사용하면 훈련 손실이 더디게 줄어들지만 **검증 손실이 낮아지는 효과**가 있음.  
- 과적합 방지에 도움.  

---

### **3. 모델 저장과 복원**
#### **📌 개념**
- 모델 훈련 후 저장하고, 나중에 복원하여 재사용 가능.  
- 전체 모델 저장 또는 가중치만 저장 가능.  

#### **📌 예제: 전체 모델 저장 및 로드**
```python
# 모델 저장
model.save('my_model.h5')

# 모델 로드
from tensorflow.keras.models import load_model
new_model = load_model('my_model.h5')

# 저장된 모델로 예측 수행
predictions = new_model.predict(x_test)
```
🔹 `model.save('파일명.h5')`: 전체 모델(구조 + 가중치) 저장  
🔹 `load_model('파일명.h5')`: 저장된 모델 로드  

#### **📌 예제: 가중치만 저장 및 로드**
```python
# 가중치 저장
model.save_weights('weights.h5')

# 가중치 로드
model.load_weights('weights.h5')
```
🔹 `save_weights()`: 가중치만 저장  
🔹 `load_weights()`: 가중치만 불러오기  

---

### **4. 콜백 (Callback)**
#### **📌 개념**
- 훈련 도중 특정 이벤트가 발생하면 자동으로 실행되는 함수.  
- 대표적인 콜백:
  - `ModelCheckpoint`: 특정 조건에서 모델 저장  
  - `EarlyStopping`: 검증 손실이 증가하면 훈련 조기 종료  

#### **📌 예제: ModelCheckpoint 사용**
```python
from tensorflow.keras.callbacks import ModelCheckpoint

# 체크포인트 콜백 설정 (가장 좋은 성능의 모델 저장)
checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')

# 모델 훈련
model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), callbacks=[checkpoint])
```
🔹 `save_best_only=True`: 가장 좋은 검증 손실을 가진 모델만 저장  
🔹 `monitor='val_loss'`: 검증 손실 기준으로 저장  

---

### **5. 조기 종료 (Early Stopping)**
#### **📌 개념**
- 훈련을 계속하면 과적합이 발생하므로, **더 이상 검증 성능이 개선되지 않으면 훈련을 자동 중단**하는 기법.  
- 너무 많은 epoch 동안 검증 성능이 개선되지 않으면 조기 종료.  

#### **📌 예제: 조기 종료 적용**
```python
from tensorflow.keras.callbacks import EarlyStopping

# 조기 종료 콜백 설정 (5 epoch 동안 개선이 없으면 종료)
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# 모델 훈련
model.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val), callbacks=[early_stopping])
```
🔹 `monitor='val_loss'`: 검증 손실을 기준으로 판단  
🔹 `patience=5`: 5 epoch 동안 개선이 없으면 종료  
🔹 `restore_best_weights=True`: 가장 좋은 모델 가중치로 복원  

---

### **📌 최종 정리**
✅ **검증 손실 (Validation Loss)** → 훈련 데이터와 비교하여 과적합 여부 판단  
✅ **드롭아웃 (Dropout)** → 과적합 방지를 위해 일부 뉴런 랜덤 비활성화  
✅ **모델 저장과 복원** → `save()`로 저장하고 `load_model()`로 복원  
✅ **콜백 (Callback)** → 훈련 중 특정 조건에서 자동 실행 (`ModelCheckpoint`, `EarlyStopping`)  
✅ **조기 종료 (Early Stopping)** → 검증 성능이 더 이상 개선되지 않으면 자동 종료  

✔️ **검증 손실이 증가하면? → 드롭아웃 사용 + 조기 종료 적용**  
✔️ **훈련이 너무 오래 걸리면? → 조기 종료 + 체크포인트 사용하여 저장**  

### **1. 인공신경망 (Artificial Neural Network, ANN)**  
- 인간의 신경망을 모방하여 만든 수학적 모델  
- 입력 → 은닉층(Hidden Layer) → 출력층(Output Layer) 구조  

#### **📌 예제: ANN 기본 구조**
```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# ANN 모델 생성
model = Sequential([
    Dense(32, activation='relu', input_shape=(10,)),  # 입력층 + 첫 번째 은닉층
    Dense(16, activation='relu'),  # 두 번째 은닉층
    Dense(1, activation='sigmoid')  # 출력층 (이진 분류)
])

# 모델 요약
model.summary()
```
🔹 `Dense(32, activation='relu')`: 32개의 뉴런, 활성화 함수 ReLU  
🔹 `Dense(1, activation='sigmoid')`: 출력층 (이진 분류)  

---

### **2. 단일 신경망 vs 다중 신경망**
#### ✅ **단일 신경망 (Single-Layer ANN)**
- 하나의 `Dense` 층만 존재하는 신경망  

```python
model = Sequential([
    Dense(1, activation='sigmoid', input_shape=(10,))
])
```

#### ✅ **다중 신경망 (Multi-Layer ANN)**
- 여러 개의 `Dense` 층이 있는 신경망  

```python
model = Sequential([
    Dense(64, activation='relu', input_shape=(10,)),  
    Dense(32, activation='relu'),  
    Dense(16, activation='relu'),  
    Dense(1, activation='sigmoid')
])
```
🔹 은닉층이 많을수록 복잡한 문제 해결 가능  

---

### **3. 히든 레이어 (Hidden Layer)**
- 입력층과 출력층 사이에 위치하는 레이어  
- 은닉층이 많을수록 신경망이 더 깊어짐 (**딥러닝**)  

```python
model = Sequential([
    Dense(128, activation='relu', input_shape=(10,)),  # 첫 번째 히든 레이어
    Dense(64, activation='relu'),  # 두 번째 히든 레이어
    Dense(32, activation='relu'),  # 세 번째 히든 레이어
    Dense(1, activation='sigmoid')  # 출력층
])
```

---

### **4. 활성화 함수 (Activation Function)**
- 뉴런의 출력 값을 조정하는 역할  

#### ✅ **시그모이드 계열 (Sigmoid, Tanh)**
- `sigmoid`: 0~1 범위로 조정 (이진 분류에 적합)
- `tanh`: -1~1 범위로 조정  

```python
Dense(1, activation='sigmoid')  # 출력층에서 사용
Dense(64, activation='tanh')  # 은닉층에서 사용 가능
```

#### ✅ **ReLU (Rectified Linear Unit)**
- `ReLU(x) = max(0, x)`
- 기울기 소실 문제 해결  

```python
Dense(64, activation='relu')
```

#### ✅ **Leaky ReLU**
- ReLU의 단점 보완 (음수 영역에서도 작은 값 유지)  

```python
from tensorflow.keras.layers import LeakyReLU

Dense(64, activation=LeakyReLU(alpha=0.1))  
```

---

### **5. 과적합 방지 (Dropout)**
- 일부 뉴런을 랜덤하게 제거하여 **과적합(overfitting) 방지**  

```python
from tensorflow.keras.layers import Dropout

model = Sequential([
    Dense(128, activation='relu', input_shape=(10,)),  
    Dropout(0.5),  # 50% 뉴런 비활성화
    Dense(64, activation='relu'),  
    Dropout(0.3),  # 30% 뉴런 비활성화
    Dense(1, activation='sigmoid')
])
```

---

### **6. 모델 생성 (Sequential)**
- `Sequential()`을 사용하여 신경망을 조립  

```python
model = Sequential([
    Dense(64, activation='relu', input_shape=(10,)),  
    Dense(32, activation='relu'),  
    Dense(1, activation='sigmoid')
])
```

---

### **7. 모델 컴파일 (Compile)**
- 손실 함수, 옵티마이저, 평가 방법 설정  

```python
model.compile(
    loss='binary_crossentropy',  
    optimizer='adam',  
    metrics=['accuracy']
)
```
🔹 `loss`: 손실 함수 지정  
🔹 `optimizer`: Adam 사용  
🔹 `metrics`: 정확도(accuracy) 측정  

---

### **8. 손실 함수 (Loss Function)**
| 문제 유형 | 손실 함수 |
|------------|------------------------------|
| 이진 분류 | `binary_crossentropy` |
| 다중 분류 | `categorical_crossentropy` |
| 회귀(예측) | `mse` (Mean Squared Error), `mae` |

```python
# 이진 분류
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# 다중 분류
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# 회귀 (MSE 사용)
model.compile(loss='mse', optimizer='adam', metrics=['mse'])
```

---

### **9. 모델 평가 방법**
- **분류 문제**: `accuracy` 사용  
- **예측(회귀) 문제**: `mse`, `mae` 사용  

```python
model.compile(loss='mse', optimizer='adam', metrics=['mse'])  # 회귀
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # 분류
```

---

### **10. 모델 학습 (Training)**
- `validation_split` 또는 `validation_data` 사용  
- `callbacks` 사용 가능  

```python
history = model.fit(
    x_train, y_train,  
    epochs=20,  
    validation_split=0.2,  # 훈련 데이터의 20%를 검증용으로 사용
    callbacks=[checkpoint, early_stopping]
)
```

---

### **11. 콜백 (Callback)**
#### ✅ **1) 체크포인트 (ModelCheckpoint)**
- 모델을 저장하는 콜백  

```python
from tensorflow.keras.callbacks import ModelCheckpoint

checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')

model.fit(x_train, y_train, epochs=20, validation_split=0.2, callbacks=[checkpoint])
```

#### ✅ **2) 조기 종료 (Early Stopping)**
- 검증 손실이 개선되지 않으면 학습 중단  

```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit(x_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping])
```

#### ✅ **3) TensorBoard**
- 훈련 과정 시각화  

```python
from tensorflow.keras.callbacks import TensorBoard

tensorboard = TensorBoard(log_dir='./logs')

model.fit(x_train, y_train, epochs=20, validation_split=0.2, callbacks=[tensorboard])
```

---

### **📌 최종 정리**
✅ **ANN** → 단일 신경망(`Dense`) vs 다중 신경망(은닉층 多)  
✅ **히든 레이어** → 입력과 출력 사이의 레이어  
✅ **활성화 함수** → `sigmoid`, `ReLU`, `LeakyReLU` 등  
✅ **과적합 방지** → `Dropout()` 사용  
✅ **모델 생성** → `Sequential()` 사용  
✅ **손실 함수** → 분류(`binary_crossentropy`), 회귀(`mse`)  
✅ **모델 평가** → 분류(`accuracy`), 회귀(`mse`)  
✅ **콜백** → `ModelCheckpoint`, `EarlyStopping`, `TensorBoard`  

✔️ **과적합 해결법** → `Dropout + EarlyStopping`  
✔️ **가장 좋은 모델 저장** → `ModelCheckpoint(save_best_only=True)`
