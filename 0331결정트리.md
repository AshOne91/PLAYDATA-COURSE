# **PCA(ì£¼ì„±ë¶„ ë¶„ì„) ë° ì¸ê³µì‹ ê²½ë§ ê°œìš” + ì˜ˆì œ**

---

## **1. PCA (Principal Component Analysis, ì£¼ì„±ë¶„ ë¶„ì„)**

### ğŸ“Œ **PCAë€?**
- ë‹¤ì°¨ì› ë°ì´í„°ë¥¼ **ì°¨ì› ì¶•ì†Œ**í•˜ì—¬ ì£¼ìš” íŒ¨í„´ì„ ì¶”ì¶œí•˜ëŠ” **í†µê³„ ê¸°ë²•**  
- ë°ì´í„°ì˜ **ë¶„ì‚°(Variance)**ì„ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ìƒˆë¡œìš´ ì¢Œí‘œê³„ë¥¼ ì„¤ì •í•¨  
- ì„¤ì •ëœ ì¢Œí‘œê³„ë¥¼ **ì£¼ì„±ë¶„(Principal Component)**ì´ë¼ í•˜ë©°, ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë©´ì„œ ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ê²ƒì´ ëª©ì   

### **PCA ì ˆì°¨ + ì˜ˆì œ**
1ï¸âƒ£ **ë°ì´í„° í‘œì¤€í™”**  
   - íŠ¹ì„± ê°„ì˜ ìŠ¤ì¼€ì¼ ì°¨ì´ë¥¼ ì—†ì• ê¸° ìœ„í•´ **í‰ê· ì„ 0, í‘œì¤€í¸ì°¨ë¥¼ 1**ë¡œ ë³€í™˜  
   - ğŸ“Œ **ì˜ˆì œ (Python ì½”ë“œ)**  
     ```python
     from sklearn.preprocessing import StandardScaler
     import numpy as np

     # ì˜ˆì œ ë°ì´í„° (í•™ìƒ ìˆ˜í•™, ê³¼í•™ ì„±ì )
     data = np.array([[85, 90], [88, 85], [75, 80], [95, 95], [80, 75]])

     # í‘œì¤€í™”
     scaler = StandardScaler()
     data_std = scaler.fit_transform(data)
     print(data_std)
     ```

2ï¸âƒ£ **ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚°**  
   - ë³€ìˆ˜ë“¤ ê°„ì˜ ê´€ê³„(ìƒê´€ì„±, ë¶„ì‚°)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” **ê³µë¶„ì‚° í–‰ë ¬** ê³„ì‚°  
   - ğŸ“Œ **ì˜ˆì œ**
     ```python
     cov_matrix = np.cov(data_std.T)
     print(cov_matrix)
     ```

3ï¸âƒ£ **ê³ ìœ ê°’(Eigenvalue)ê³¼ ê³ ìœ ë²¡í„°(Eigenvector) ê³„ì‚°**  
   - ê³µë¶„ì‚° í–‰ë ¬ì„ **ê³ ìœ ê°’ ë¶„í•´**í•˜ì—¬ ì£¼ì„±ë¶„ì˜ ë°©í–¥ê³¼ í¬ê¸° ê²°ì •  
   - ğŸ“Œ **ì˜ˆì œ**
     ```python
     from numpy.linalg import eig
     eigenvalues, eigenvectors = eig(cov_matrix)
     print("ê³ ìœ ê°’:", eigenvalues)
     print("ê³ ìœ ë²¡í„°:", eigenvectors)
     ```

4ï¸âƒ£ **ì£¼ì„±ë¶„ ì„ íƒ**  
   - ê³ ìœ ê°’ì´ í° ìˆœì„œëŒ€ë¡œ ì£¼ì„±ë¶„ì„ ì„ íƒ  
   - ë³´í†µ **ì „ì²´ ë¶„ì‚°ì˜ 80~90% ì´ìƒ**ì„ ì„¤ëª…í•˜ëŠ” ì£¼ì„±ë¶„ ê°œìˆ˜ë¥¼ ì„ íƒ  

5ï¸âƒ£ **ë°ì´í„° ë³€í™˜**  
   - **ê³ ìœ ë²¡í„° í–‰ë ¬**ì„ ì‚¬ìš©í•´ ì›ë˜ ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ ì¢Œí‘œê³„(ì£¼ì„±ë¶„ ê³µê°„)ë¡œ ë³€í™˜  
   - ğŸ“Œ **ì˜ˆì œ**
     ```python
     pca_data = np.dot(data_std, eigenvectors)
     print("PCA ë³€í™˜ ë°ì´í„°:", pca_data)
     ```

---

### **ğŸ“Š PCAë¥¼ ì‹¤ì œ ë°ì´í„°ì— ì ìš©í•´ë³´ê¸°**
```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
iris = load_iris()
X = iris.data  # íŠ¹ì„± ë°ì´í„°

# PCA ì ìš© (2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# ì‹œê°í™”
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=iris.target, palette="Set1")
plt.xlabel("ì£¼ì„±ë¶„ 1")
plt.ylabel("ì£¼ì„±ë¶„ 2")
plt.title("Iris ë°ì´í„° PCA ë³€í™˜")
plt.show()
```
âœ… **ê²°ê³¼**: 4ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ì—¬ ì‹œê°í™” ê°€ëŠ¥

---

## **2. ì¸ê³µì‹ ê²½ë§(Artificial Neural Network, ANN)**

### ğŸ“Œ **ì‹ ê²½ë§ì´ë€?**
- **ìƒë¬¼í•™ì  ì‹ ê²½ë§(Neuron)ì˜ êµ¬ì¡°**ë¥¼ ë³¸ë–  ë§Œë“  ëª¨ë¸  
- **ì…ë ¥ì¸µ(Input Layer), ì€ë‹‰ì¸µ(Hidden Layer), ì¶œë ¥ì¸µ(Output Layer)**ìœ¼ë¡œ êµ¬ì„±ë¨  
- **í™œì„±í™” í•¨ìˆ˜(Activation Function)**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ì • ì„ê³„ì¹˜ë¥¼ ë„˜ìœ¼ë©´ ë‹¤ìŒ ì¸µìœ¼ë¡œ ì‹ í˜¸ ì „ë‹¬  

---

### **1ï¸âƒ£ ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)**
- **ì´ì§„ ë¶„ë¥˜(Binary Classification)** ëª¨ë¸  
- ğŸ“Œ **ì˜ˆì œ (MNIST ìˆ«ì ì´ë¯¸ì§€ ë¶„ë¥˜ - ë¡œì§€ìŠ¤í‹± íšŒê·€)**
  ```python
  from sklearn.linear_model import LogisticRegression
  from sklearn.datasets import load_digits
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import accuracy_score

  # ë°ì´í„° ë¡œë“œ
  digits = load_digits()
  X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)

  # ëª¨ë¸ í•™ìŠµ
  model = LogisticRegression(max_iter=10000)
  model.fit(X_train, y_train)

  # ì˜ˆì¸¡ ë° ì •í™•ë„ í‰ê°€
  y_pred = model.predict(X_test)
  print("ë¡œì§€ìŠ¤í‹± íšŒê·€ ì •í™•ë„:", accuracy_score(y_test, y_pred))
  ```
âœ… **ê²°ê³¼**: ìˆ«ì ì¸ì‹ ì„±ëŠ¥ í™•ì¸ ê°€ëŠ¥  

---

### **2ï¸âƒ£ ì¸ê³µì‹ ê²½ë§(ANN) ëª¨ë¸ êµ¬í˜„**
- ğŸ“Œ **ì˜ˆì œ (Fashion MNIST ë°ì´í„° - ì‹ ê²½ë§ ì ìš©)**
  ```python
  import tensorflow as tf
  from tensorflow.keras import layers, models
  from tensorflow.keras.datasets import fashion_mnist
  import matplotlib.pyplot as plt

  # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
  (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
  X_train, X_test = X_train / 255.0, X_test / 255.0  # ì •ê·œí™”

  # ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜
  model = models.Sequential([
      layers.Flatten(input_shape=(28, 28)),  # 2D ì´ë¯¸ì§€ë¥¼ 1Dë¡œ ë³€í™˜
      layers.Dense(128, activation='relu'),  # ì€ë‹‰ì¸µ
      layers.Dense(10, activation='softmax') # ì¶œë ¥ì¸µ (10ê°œ í´ë˜ìŠ¤)
  ])

  # ëª¨ë¸ ì»´íŒŒì¼
  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

  # ëª¨ë¸ í•™ìŠµ
  model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

  # ëª¨ë¸ í‰ê°€
  test_loss, test_acc = model.evaluate(X_test, y_test)
  print("ì‹ ê²½ë§ í…ŒìŠ¤íŠ¸ ì •í™•ë„:", test_acc)
  ```
âœ… **ê²°ê³¼**: íŒ¨ì…˜ MNIST ë¶„ë¥˜ ì„±ëŠ¥ í™•ì¸ ê°€ëŠ¥  

---

## **3. PCAì™€ ì‹ ê²½ë§ ê²°í•©í•˜ê¸°**
1ï¸âƒ£ PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì°¨ì› ì¶•ì†Œ  
2ï¸âƒ£ ì¶•ì†Œëœ ë°ì´í„°ë¥¼ ì‹ ê²½ë§(ANN)ì— ì…ë ¥  
3ï¸âƒ£ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê³  ëª¨ë¸ ì„±ëŠ¥ì„ ë¹„êµ  

ğŸ“Œ **PCA + ANN ì˜ˆì œ**
```python
# PCA ì ìš©
pca = PCA(n_components=50)  # 784ì°¨ì› â†’ 50ì°¨ì› ì¶•ì†Œ
X_train_pca = pca.fit_transform(X_train.reshape(-1, 28*28))
X_test_pca = pca.transform(X_test.reshape(-1, 28*28))

# ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜
model = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(50,)),  # ì…ë ¥ ì°¨ì› ë³€ê²½
    layers.Dense(10, activation='softmax')
])

# ëª¨ë¸ í•™ìŠµ
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train_pca, y_train, epochs=10, validation_data=(X_test_pca, y_test))
```
âœ… **ê²°ê³¼**: PCAë¡œ ì¶•ì†Œ í›„ ANN ì ìš©í•˜ì—¬ ì„±ëŠ¥ í‰ê°€ ê°€ëŠ¥  

---

## **4. ìš”ì•½**
âœ… **PCA(ì£¼ì„±ë¶„ ë¶„ì„)**:  
- ë°ì´í„°ì˜ ì°¨ì›ì„ ì¶•ì†Œí•˜ì—¬ ì£¼ìš” ì •ë³´ë¥¼ ë³´ì¡´í•˜ëŠ” ê¸°ë²•  
- **ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨**ì„ í†µí•´ ì£¼ì„±ë¶„ ê°œìˆ˜ ê²°ì •  
- **ë¶„ë¥˜ê¸°ì™€ í•¨ê»˜ ì‚¬ìš© ê°€ëŠ¥í•˜ë‚˜ ì„±ëŠ¥ì´ ë‚®ì•„ì§ˆ ìˆ˜ë„ ìˆìŒ**  

âœ… **ì¸ê³µì‹ ê²½ë§(ANN)**:  
- ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP) êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ì„ í˜• ë¬¸ì œ í•´ê²°  
- ë”¥ëŸ¬ë‹(Deep Learning)ì˜ ê¸°ì´ˆ  

âœ… **PCA + ì‹ ê²½ë§ ì¡°í•© ê°€ëŠ¥**  
- **PCAë¡œ ì°¨ì›ì„ ì¤„ì´ê³  ANNì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ ë¹„êµ ê°€ëŠ¥**  
- ê³„ì‚°ëŸ‰ì„ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ë„ ìˆìŒ  

ì´ì œ PCAì™€ ì‹ ê²½ë§ì„ ì‹¤ì œ ë°ì´í„°ì— í™œìš©í•´ì„œ ì‹¤í—˜í•´ë³´ë©´ ì¢‹ì„ ê²ƒì„! ğŸš€

## **ğŸ“Œ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ ëª¨ë¸ì— ì ìš© - ëœë¤í¬ë ˆìŠ¤íŠ¸ (RandomForestClassifier)**

### **1ï¸âƒ£ ëœë¤í¬ë ˆìŠ¤íŠ¸ ê°œìš”**
- ëœë¤í¬ë ˆìŠ¤íŠ¸(RandomForest)ëŠ” ì—¬ëŸ¬ ê°œì˜ **ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree)**ë¥¼ ì¡°í•©í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” **ì•™ìƒë¸” í•™ìŠµ(Ensemble Learning)** ë°©ë²•ì„.
- ê°œë³„ íŠ¸ë¦¬ê°€ ê³¼ì í•©ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³  **ë” ë†’ì€ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ì œê³µ**í•˜ëŠ” ì¥ì ì´ ìˆìŒ.
- íŠ¹íˆ **ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬ì— ê°•í•¨** â†’ ìˆ˜ì¹˜í˜• ë°ì´í„°ë¥¼ ë²”ì£¼í˜•ìœ¼ë¡œ ë³€í™˜í•˜ë©´ ì„±ëŠ¥ì´ í–¥ìƒë  ìˆ˜ ìˆìŒ.

---

### **2ï¸âƒ£ ëœë¤í¬ë ˆìŠ¤íŠ¸ ì ìš© ê³¼ì •**
#### **â‘  ë°ì´í„° ì¤€ë¹„**
ğŸ‘‰ `UCI Adult Income Dataset` ì‚¬ìš© (ìˆ˜ì…ì´ 50K ì´ìƒì¸ì§€ ì˜ˆì¸¡)

```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# UCI ë°ì´í„° ë¡œë“œ
from ucimlrepo import fetch_ucirepo
adult = fetch_ucirepo(id=2)

# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = adult.data.features
y = adult.data.targets
```

---

#### **â‘¡ ë°ì´í„° ì „ì²˜ë¦¬**
âœ… **ìˆ˜ì¹˜í˜• ë°ì´í„° í‘œì¤€í™”(Standardization)**
âœ… **ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”©(Label Encoding)**

```python
# ë²”ì£¼í˜• ë°ì´í„° ë¼ë²¨ ì¸ì½”ë”©
label_encoders = {}
for col in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# ë°ì´í„° ë¶„í•  (80% í•™ìŠµ, 20% í…ŒìŠ¤íŠ¸)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

#### **â‘¢ ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ**
```python
# ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° ì •í™•ë„ í‰ê°€
y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"ëœë¤í¬ë ˆìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.4f}")
```

âœ… **ê²°ê³¼:**  
ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì´ ìˆ˜ì…ì„ ì˜ˆì¸¡í•˜ëŠ” ì •í™•ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŒ.

---

### **3ï¸âƒ£ íŒŒì´í”„ë¼ì¸(Pipeline) ìë™í™”**
- ì „ì²˜ë¦¬ + ëª¨ë¸ í•™ìŠµ + ì˜ˆì¸¡ì„ í•˜ë‚˜ì˜ íë¦„ìœ¼ë¡œ ìë™í™”  
- **ì¥ì :** ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì²˜ë¦¬ë¥¼ í•œ ë²ˆì— ì‹¤í–‰ ê°€ëŠ¥

```python
# ëœë¤í¬ë ˆìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
pipeline = Pipeline([
    ('scaler', StandardScaler()),       # ìŠ¤ì¼€ì¼ë§
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))  # ëª¨ë¸
])

# íŒŒì´í”„ë¼ì¸ í•™ìŠµ
pipeline.fit(X_train, y_train)

# íŒŒì´í”„ë¼ì¸ ì˜ˆì¸¡
y_pred_pipeline = pipeline.predict(X_test)
accuracy_pipeline = accuracy_score(y_test, y_pred_pipeline)

print(f"íŒŒì´í”„ë¼ì¸ ì ìš© í›„ ì •í™•ë„: {accuracy_pipeline:.4f}")
```

âœ… **ê²°ê³¼:**  
- **íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ë©´ ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ í•™ìŠµê¹Œì§€ ìë™ìœ¼ë¡œ ì§„í–‰ë¨.**
- **ì½”ë“œ ìœ ì§€ë³´ìˆ˜ ë° í™•ì¥ì„±ì´ ì¦ê°€í•¨.**

---

## **ğŸ“Œ ì¸ê³µì‹ ê²½ë§ (Artificial Neural Network, ANN)**
### **1ï¸âƒ£ ê°œìš”**
- ì¸ê°„ì˜ **ë‰´ëŸ°(Neuron) êµ¬ì¡°**ë¥¼ ëª¨ë°©í•˜ì—¬ ì„¤ê³„ëœ ì‹ ê²½ë§ ëª¨ë¸
- ì…ë ¥ì¸µ(Input Layer) â†’ ì€ë‹‰ì¸µ(Hidden Layer) â†’ ì¶œë ¥ì¸µ(Output Layer) êµ¬ì¡°
- **í™œì„±í™” í•¨ìˆ˜(Activation Function)**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ì„ ë¹„ì„ í˜• ë³€í™˜í•˜ì—¬ í•™ìŠµ
- **ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ìŒì„± ë°ì´í„° ë¶„ì„ ë“±ì— ì‚¬ìš©ë¨**

---

### **2ï¸âƒ£ ANN ì ìš© ì˜ˆì œ (Fashion MNIST ë°ì´í„°ì…‹)**
âœ… **Fashion MNIST ë°ì´í„°ì…‹**:  
- 28x28 í”½ì…€ì˜ í‘ë°± ì´ë¯¸ì§€ë¡œ ì´ë£¨ì–´ì§„ **10ê°œ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜**
- ì‹ ê²½ë§ì„ ì‚¬ìš©í•´ ì˜·ì˜ ì¢…ë¥˜ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ

#### **â‘  ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬**
```python
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# ë°ì´í„° ë¡œë“œ
fashion_mnist = tf.keras.datasets.fashion_mnist
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# ë°ì´í„° ì •ê·œí™” (0~255 â†’ 0~1)
X_train, X_test = X_train / 255.0, X_test / 255.0
```

---

#### **â‘¡ ì‹ ê²½ë§ ëª¨ë¸ ì„¤ê³„**
âœ… **Dense ë ˆì´ì–´**ë¥¼ ìŒ“ì•„ Fully Connected ì‹ ê²½ë§ ìƒì„±  
âœ… **ReLU í™œì„±í™” í•¨ìˆ˜** ì‚¬ìš©  
âœ… **Softmax í•¨ìˆ˜**ë¡œ 10ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜  

```python
# ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜
model = models.Sequential([
    layers.Flatten(input_shape=(28, 28)),       # 2D â†’ 1D ë³€í™˜
    layers.Dense(128, activation='relu'),       # ì€ë‹‰ì¸µ 1 (128ê°œ ë‰´ëŸ°, ReLU)
    layers.Dense(64, activation='relu'),        # ì€ë‹‰ì¸µ 2 (64ê°œ ë‰´ëŸ°, ReLU)
    layers.Dense(10, activation='softmax')      # ì¶œë ¥ì¸µ (10ê°œ í´ë˜ìŠ¤, Softmax)
])

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# ëª¨ë¸ ìš”ì•½ ì¶œë ¥
model.summary()
```

---

#### **â‘¢ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€**
```python
# ëª¨ë¸ í•™ìŠµ
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

# ëª¨ë¸ í‰ê°€
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}")
```

âœ… **ê²°ê³¼:**  
Fashion MNIST ë°ì´í„°ì…‹ì„ í•™ìŠµí•œ í›„, **ì˜ë¥˜ë¥¼ 10ê°œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ëŠ” ì‹ ê²½ë§ ëª¨ë¸**ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŒ.

---

## **ğŸ“Œ ê²°ë¡ **
| ëª¨ë¸ | íŠ¹ì§• | ì˜ˆì œ |
|------|------|------|
| **ëœë¤í¬ë ˆìŠ¤íŠ¸** | ë‹¤ìˆ˜ì˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¥¼ ì¡°í•©í•´ ì„±ëŠ¥ í–¥ìƒ | ì„±ì¸ ì†Œë“ ì˜ˆì¸¡ (50K ì´ìƒ ì—¬ë¶€) |
| **íŒŒì´í”„ë¼ì¸** | ë°ì´í„° ì „ì²˜ë¦¬ + ëª¨ë¸ í•™ìŠµì„ ìë™í™” | `Pipeline`ì„ ì‚¬ìš©í•œ í•™ìŠµ |
| **ì¸ê³µì‹ ê²½ë§** | ë‰´ëŸ° êµ¬ì¡°ë¥¼ ëª¨ë°©, ë”¥ëŸ¬ë‹ì˜ ê¸°ë³¸ êµ¬ì¡° | Fashion MNIST ì˜ë¥˜ ë¶„ë¥˜ |

ğŸš€ **ì •ë¦¬:**  
- ëœë¤í¬ë ˆìŠ¤íŠ¸ëŠ” **ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë©°, íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ë©´ ìë™í™” ê°€ëŠ¥**  
- ì¸ê³µì‹ ê²½ë§(ANN)ì€ **ë¹„ì„ í˜• ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¯¸ì§€ ë° ë³µì¡í•œ ë°ì´í„° ë¶„ì„ì— ì í•©**  
- ê° ë°©ë²•ì„ **ë°ì´í„° íŠ¹ì„±ì— ë§ê²Œ ì„ íƒí•˜ì—¬ ì ìš©í•´ì•¼ í•¨.**

### **1. ê²€ì¦ ì†ì‹¤ (Validation Loss)**
#### **ğŸ“Œ ê°œë…**
- ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°(training set)ì—ì„œëŠ” ì˜ ë™ì‘í•˜ì§€ë§Œ, ê²€ì¦ ë°ì´í„°(validation set)ì—ì„œëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ.  
- í›ˆë ¨ ì†ì‹¤(training loss)ê³¼ ê²€ì¦ ì†ì‹¤(validation loss)ì„ ë¹„êµí•˜ë©´ **ê³¼ì í•©(overfitting)** ì—¬ë¶€ë¥¼ íŒë‹¨ ê°€ëŠ¥.  
- ê³¼ì í•©ì´ ë°œìƒí•˜ë©´ ê²€ì¦ ì†ì‹¤ì´ **ì¦ê°€**í•˜ì§€ë§Œ í›ˆë ¨ ì†ì‹¤ì€ ê³„ì† ê°ì†Œí•¨.  

#### **ğŸ“Œ ì˜ˆì œ: ê²€ì¦ ì†ì‹¤ í™•ì¸**
```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten
import matplotlib.pyplot as plt

# MNIST ë°ì´í„° ë¡œë“œ
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# ë°ì´í„° ì •ê·œí™” (0~255 â†’ 0~1)
x_train, x_test = x_train / 255.0, x_test / 255.0

# í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¡œ ë¶„ë¦¬
x_train, x_val = x_train[:50000], x_train[50000:]  # 50000ê°œ í›ˆë ¨, 10000ê°œ ê²€ì¦
y_train, y_val = y_train[:50000], y_train[50000:]

# ëª¨ë¸ ìƒì„±
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# ëª¨ë¸ í›ˆë ¨ (ê²€ì¦ ë°ì´í„° í¬í•¨)
history = model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val))

# í›ˆë ¨ ì†ì‹¤ ë° ê²€ì¦ ì†ì‹¤ ì‹œê°í™”
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training vs. Validation Loss')
plt.show()
```
ğŸ”¹ `validation_data=(x_val, y_val)`: ê²€ì¦ ë°ì´í„° ì œê³µ  
ğŸ”¹ `history.history['loss']`: í›ˆë ¨ ë°ì´í„°ì˜ ì†ì‹¤  
ğŸ”¹ `history.history['val_loss']`: ê²€ì¦ ë°ì´í„°ì˜ ì†ì‹¤  

**âœ… ê²°ê³¼ í•´ì„**
- ë‘ ê·¸ë˜í”„ê°€ ê°™ì´ ê°ì†Œí•˜ë©´ **í›ˆë ¨ì´ ì˜ ì§„í–‰**ë¨.  
- ê²€ì¦ ì†ì‹¤ì´ ì¦ê°€í•˜ëŠ” ì‹œì ì—ì„œ **ê³¼ì í•©** ë°œìƒ.  

---

### **2. ë“œë¡­ì•„ì›ƒ (Dropout)**
#### **ğŸ“Œ ê°œë…**
- ì‹ ê²½ë§ì´ íŠ¹ì • ë‰´ëŸ°ì— ê³¼í•˜ê²Œ ì˜ì¡´í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ **ì¼ë¶€ ë‰´ëŸ°ì„ ëœë¤í•˜ê²Œ ì œê±°(drop)** í•˜ëŠ” ê¸°ë²•.  
- ê³¼ì í•©ì„ ì¤„ì´ëŠ” **ì •ê·œí™”(regularization) ë°©ë²•** ì¤‘ í•˜ë‚˜.  
- í›ˆë ¨ ì¤‘ì—ëŠ” ë‰´ëŸ°ì„ ëœë¤ìœ¼ë¡œ êº¼ë²„ë¦¬ì§€ë§Œ, í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ëª¨ë“  ë‰´ëŸ°ì„ í™œì„±í™”í•˜ì—¬ ì˜ˆì¸¡ ìˆ˜í–‰.  

#### **ğŸ“Œ ì˜ˆì œ: ë“œë¡­ì•„ì›ƒ ì ìš©**
```python
from tensorflow.keras.layers import Dropout

model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dropout(0.5),  # 50% ë‰´ëŸ°ì„ ëœë¤ìœ¼ë¡œ ë¹„í™œì„±í™”
    Dense(64, activation='relu'),
    Dropout(0.3),  # 30% ë‰´ëŸ° ë¹„í™œì„±í™”
    Dense(10, activation='softmax')
])

# ëª¨ë¸ ì»´íŒŒì¼ ë° í›ˆë ¨
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val))
```
ğŸ”¹ `Dropout(0.5)`: 50% í™•ë¥ ë¡œ ë‰´ëŸ° ë¹„í™œì„±í™”  
ğŸ”¹ `Dropout(0.3)`: 30% í™•ë¥ ë¡œ ë‰´ëŸ° ë¹„í™œì„±í™”  

**âœ… ê²°ê³¼**
- ë“œë¡­ì•„ì›ƒì„ ì‚¬ìš©í•˜ë©´ í›ˆë ¨ ì†ì‹¤ì´ ë”ë””ê²Œ ì¤„ì–´ë“¤ì§€ë§Œ **ê²€ì¦ ì†ì‹¤ì´ ë‚®ì•„ì§€ëŠ” íš¨ê³¼**ê°€ ìˆìŒ.  
- ê³¼ì í•© ë°©ì§€ì— ë„ì›€.  

---

### **3. ëª¨ë¸ ì €ì¥ê³¼ ë³µì›**
#### **ğŸ“Œ ê°œë…**
- ëª¨ë¸ í›ˆë ¨ í›„ ì €ì¥í•˜ê³ , ë‚˜ì¤‘ì— ë³µì›í•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥.  
- ì „ì²´ ëª¨ë¸ ì €ì¥ ë˜ëŠ” ê°€ì¤‘ì¹˜ë§Œ ì €ì¥ ê°€ëŠ¥.  

#### **ğŸ“Œ ì˜ˆì œ: ì „ì²´ ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ**
```python
# ëª¨ë¸ ì €ì¥
model.save('my_model.h5')

# ëª¨ë¸ ë¡œë“œ
from tensorflow.keras.models import load_model
new_model = load_model('my_model.h5')

# ì €ì¥ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
predictions = new_model.predict(x_test)
```
ğŸ”¹ `model.save('íŒŒì¼ëª….h5')`: ì „ì²´ ëª¨ë¸(êµ¬ì¡° + ê°€ì¤‘ì¹˜) ì €ì¥  
ğŸ”¹ `load_model('íŒŒì¼ëª….h5')`: ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ  

#### **ğŸ“Œ ì˜ˆì œ: ê°€ì¤‘ì¹˜ë§Œ ì €ì¥ ë° ë¡œë“œ**
```python
# ê°€ì¤‘ì¹˜ ì €ì¥
model.save_weights('weights.h5')

# ê°€ì¤‘ì¹˜ ë¡œë“œ
model.load_weights('weights.h5')
```
ğŸ”¹ `save_weights()`: ê°€ì¤‘ì¹˜ë§Œ ì €ì¥  
ğŸ”¹ `load_weights()`: ê°€ì¤‘ì¹˜ë§Œ ë¶ˆëŸ¬ì˜¤ê¸°  

---

### **4. ì½œë°± (Callback)**
#### **ğŸ“Œ ê°œë…**
- í›ˆë ¨ ë„ì¤‘ íŠ¹ì • ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ë©´ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜.  
- ëŒ€í‘œì ì¸ ì½œë°±:
  - `ModelCheckpoint`: íŠ¹ì • ì¡°ê±´ì—ì„œ ëª¨ë¸ ì €ì¥  
  - `EarlyStopping`: ê²€ì¦ ì†ì‹¤ì´ ì¦ê°€í•˜ë©´ í›ˆë ¨ ì¡°ê¸° ì¢…ë£Œ  

#### **ğŸ“Œ ì˜ˆì œ: ModelCheckpoint ì‚¬ìš©**
```python
from tensorflow.keras.callbacks import ModelCheckpoint

# ì²´í¬í¬ì¸íŠ¸ ì½œë°± ì„¤ì • (ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ ì €ì¥)
checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')

# ëª¨ë¸ í›ˆë ¨
model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), callbacks=[checkpoint])
```
ğŸ”¹ `save_best_only=True`: ê°€ì¥ ì¢‹ì€ ê²€ì¦ ì†ì‹¤ì„ ê°€ì§„ ëª¨ë¸ë§Œ ì €ì¥  
ğŸ”¹ `monitor='val_loss'`: ê²€ì¦ ì†ì‹¤ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥  

---

### **5. ì¡°ê¸° ì¢…ë£Œ (Early Stopping)**
#### **ğŸ“Œ ê°œë…**
- í›ˆë ¨ì„ ê³„ì†í•˜ë©´ ê³¼ì í•©ì´ ë°œìƒí•˜ë¯€ë¡œ, **ë” ì´ìƒ ê²€ì¦ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í›ˆë ¨ì„ ìë™ ì¤‘ë‹¨**í•˜ëŠ” ê¸°ë²•.  
- ë„ˆë¬´ ë§ì€ epoch ë™ì•ˆ ê²€ì¦ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ.  

#### **ğŸ“Œ ì˜ˆì œ: ì¡°ê¸° ì¢…ë£Œ ì ìš©**
```python
from tensorflow.keras.callbacks import EarlyStopping

# ì¡°ê¸° ì¢…ë£Œ ì½œë°± ì„¤ì • (5 epoch ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ)
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# ëª¨ë¸ í›ˆë ¨
model.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val), callbacks=[early_stopping])
```
ğŸ”¹ `monitor='val_loss'`: ê²€ì¦ ì†ì‹¤ì„ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨  
ğŸ”¹ `patience=5`: 5 epoch ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ  
ğŸ”¹ `restore_best_weights=True`: ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¡œ ë³µì›  

---

### **ğŸ“Œ ìµœì¢… ì •ë¦¬**
âœ… **ê²€ì¦ ì†ì‹¤ (Validation Loss)** â†’ í›ˆë ¨ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ê³¼ì í•© ì—¬ë¶€ íŒë‹¨  
âœ… **ë“œë¡­ì•„ì›ƒ (Dropout)** â†’ ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ì¼ë¶€ ë‰´ëŸ° ëœë¤ ë¹„í™œì„±í™”  
âœ… **ëª¨ë¸ ì €ì¥ê³¼ ë³µì›** â†’ `save()`ë¡œ ì €ì¥í•˜ê³  `load_model()`ë¡œ ë³µì›  
âœ… **ì½œë°± (Callback)** â†’ í›ˆë ¨ ì¤‘ íŠ¹ì • ì¡°ê±´ì—ì„œ ìë™ ì‹¤í–‰ (`ModelCheckpoint`, `EarlyStopping`)  
âœ… **ì¡°ê¸° ì¢…ë£Œ (Early Stopping)** â†’ ê²€ì¦ ì„±ëŠ¥ì´ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ìë™ ì¢…ë£Œ  

âœ”ï¸ **ê²€ì¦ ì†ì‹¤ì´ ì¦ê°€í•˜ë©´? â†’ ë“œë¡­ì•„ì›ƒ ì‚¬ìš© + ì¡°ê¸° ì¢…ë£Œ ì ìš©**  
âœ”ï¸ **í›ˆë ¨ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ë©´? â†’ ì¡°ê¸° ì¢…ë£Œ + ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©í•˜ì—¬ ì €ì¥**  

### **1. ì¸ê³µì‹ ê²½ë§ (Artificial Neural Network, ANN)**  
- ì¸ê°„ì˜ ì‹ ê²½ë§ì„ ëª¨ë°©í•˜ì—¬ ë§Œë“  ìˆ˜í•™ì  ëª¨ë¸  
- ì…ë ¥ â†’ ì€ë‹‰ì¸µ(Hidden Layer) â†’ ì¶œë ¥ì¸µ(Output Layer) êµ¬ì¡°  

#### **ğŸ“Œ ì˜ˆì œ: ANN ê¸°ë³¸ êµ¬ì¡°**
```python
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# ANN ëª¨ë¸ ìƒì„±
model = Sequential([
    Dense(32, activation='relu', input_shape=(10,)),  # ì…ë ¥ì¸µ + ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ
    Dense(16, activation='relu'),  # ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ
    Dense(1, activation='sigmoid')  # ì¶œë ¥ì¸µ (ì´ì§„ ë¶„ë¥˜)
])

# ëª¨ë¸ ìš”ì•½
model.summary()
```
ğŸ”¹ `Dense(32, activation='relu')`: 32ê°œì˜ ë‰´ëŸ°, í™œì„±í™” í•¨ìˆ˜ ReLU  
ğŸ”¹ `Dense(1, activation='sigmoid')`: ì¶œë ¥ì¸µ (ì´ì§„ ë¶„ë¥˜)  

---

### **2. ë‹¨ì¼ ì‹ ê²½ë§ vs ë‹¤ì¤‘ ì‹ ê²½ë§**
#### âœ… **ë‹¨ì¼ ì‹ ê²½ë§ (Single-Layer ANN)**
- í•˜ë‚˜ì˜ `Dense` ì¸µë§Œ ì¡´ì¬í•˜ëŠ” ì‹ ê²½ë§  

```python
model = Sequential([
    Dense(1, activation='sigmoid', input_shape=(10,))
])
```

#### âœ… **ë‹¤ì¤‘ ì‹ ê²½ë§ (Multi-Layer ANN)**
- ì—¬ëŸ¬ ê°œì˜ `Dense` ì¸µì´ ìˆëŠ” ì‹ ê²½ë§  

```python
model = Sequential([
    Dense(64, activation='relu', input_shape=(10,)),  
    Dense(32, activation='relu'),  
    Dense(16, activation='relu'),  
    Dense(1, activation='sigmoid')
])
```
ğŸ”¹ ì€ë‹‰ì¸µì´ ë§ì„ìˆ˜ë¡ ë³µì¡í•œ ë¬¸ì œ í•´ê²° ê°€ëŠ¥  

---

### **3. íˆë“  ë ˆì´ì–´ (Hidden Layer)**
- ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µ ì‚¬ì´ì— ìœ„ì¹˜í•˜ëŠ” ë ˆì´ì–´  
- ì€ë‹‰ì¸µì´ ë§ì„ìˆ˜ë¡ ì‹ ê²½ë§ì´ ë” ê¹Šì–´ì§ (**ë”¥ëŸ¬ë‹**)  

```python
model = Sequential([
    Dense(128, activation='relu', input_shape=(10,)),  # ì²« ë²ˆì§¸ íˆë“  ë ˆì´ì–´
    Dense(64, activation='relu'),  # ë‘ ë²ˆì§¸ íˆë“  ë ˆì´ì–´
    Dense(32, activation='relu'),  # ì„¸ ë²ˆì§¸ íˆë“  ë ˆì´ì–´
    Dense(1, activation='sigmoid')  # ì¶œë ¥ì¸µ
])
```

---

### **4. í™œì„±í™” í•¨ìˆ˜ (Activation Function)**
- ë‰´ëŸ°ì˜ ì¶œë ¥ ê°’ì„ ì¡°ì •í•˜ëŠ” ì—­í•   

#### âœ… **ì‹œê·¸ëª¨ì´ë“œ ê³„ì—´ (Sigmoid, Tanh)**
- `sigmoid`: 0~1 ë²”ìœ„ë¡œ ì¡°ì • (ì´ì§„ ë¶„ë¥˜ì— ì í•©)
- `tanh`: -1~1 ë²”ìœ„ë¡œ ì¡°ì •  

```python
Dense(1, activation='sigmoid')  # ì¶œë ¥ì¸µì—ì„œ ì‚¬ìš©
Dense(64, activation='tanh')  # ì€ë‹‰ì¸µì—ì„œ ì‚¬ìš© ê°€ëŠ¥
```

#### âœ… **ReLU (Rectified Linear Unit)**
- `ReLU(x) = max(0, x)`
- ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²°  

```python
Dense(64, activation='relu')
```

#### âœ… **Leaky ReLU**
- ReLUì˜ ë‹¨ì  ë³´ì™„ (ìŒìˆ˜ ì˜ì—­ì—ì„œë„ ì‘ì€ ê°’ ìœ ì§€)  

```python
from tensorflow.keras.layers import LeakyReLU

Dense(64, activation=LeakyReLU(alpha=0.1))  
```

---

### **5. ê³¼ì í•© ë°©ì§€ (Dropout)**
- ì¼ë¶€ ë‰´ëŸ°ì„ ëœë¤í•˜ê²Œ ì œê±°í•˜ì—¬ **ê³¼ì í•©(overfitting) ë°©ì§€**  

```python
from tensorflow.keras.layers import Dropout

model = Sequential([
    Dense(128, activation='relu', input_shape=(10,)),  
    Dropout(0.5),  # 50% ë‰´ëŸ° ë¹„í™œì„±í™”
    Dense(64, activation='relu'),  
    Dropout(0.3),  # 30% ë‰´ëŸ° ë¹„í™œì„±í™”
    Dense(1, activation='sigmoid')
])
```

---

### **6. ëª¨ë¸ ìƒì„± (Sequential)**
- `Sequential()`ì„ ì‚¬ìš©í•˜ì—¬ ì‹ ê²½ë§ì„ ì¡°ë¦½  

```python
model = Sequential([
    Dense(64, activation='relu', input_shape=(10,)),  
    Dense(32, activation='relu'),  
    Dense(1, activation='sigmoid')
])
```

---

### **7. ëª¨ë¸ ì»´íŒŒì¼ (Compile)**
- ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, í‰ê°€ ë°©ë²• ì„¤ì •  

```python
model.compile(
    loss='binary_crossentropy',  
    optimizer='adam',  
    metrics=['accuracy']
)
```
ğŸ”¹ `loss`: ì†ì‹¤ í•¨ìˆ˜ ì§€ì •  
ğŸ”¹ `optimizer`: Adam ì‚¬ìš©  
ğŸ”¹ `metrics`: ì •í™•ë„(accuracy) ì¸¡ì •  

---

### **8. ì†ì‹¤ í•¨ìˆ˜ (Loss Function)**
| ë¬¸ì œ ìœ í˜• | ì†ì‹¤ í•¨ìˆ˜ |
|------------|------------------------------|
| ì´ì§„ ë¶„ë¥˜ | `binary_crossentropy` |
| ë‹¤ì¤‘ ë¶„ë¥˜ | `categorical_crossentropy` |
| íšŒê·€(ì˜ˆì¸¡) | `mse` (Mean Squared Error), `mae` |

```python
# ì´ì§„ ë¶„ë¥˜
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# ë‹¤ì¤‘ ë¶„ë¥˜
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# íšŒê·€ (MSE ì‚¬ìš©)
model.compile(loss='mse', optimizer='adam', metrics=['mse'])
```

---

### **9. ëª¨ë¸ í‰ê°€ ë°©ë²•**
- **ë¶„ë¥˜ ë¬¸ì œ**: `accuracy` ì‚¬ìš©  
- **ì˜ˆì¸¡(íšŒê·€) ë¬¸ì œ**: `mse`, `mae` ì‚¬ìš©  

```python
model.compile(loss='mse', optimizer='adam', metrics=['mse'])  # íšŒê·€
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # ë¶„ë¥˜
```

---

### **10. ëª¨ë¸ í•™ìŠµ (Training)**
- `validation_split` ë˜ëŠ” `validation_data` ì‚¬ìš©  
- `callbacks` ì‚¬ìš© ê°€ëŠ¥  

```python
history = model.fit(
    x_train, y_train,  
    epochs=20,  
    validation_split=0.2,  # í›ˆë ¨ ë°ì´í„°ì˜ 20%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ì‚¬ìš©
    callbacks=[checkpoint, early_stopping]
)
```

---

### **11. ì½œë°± (Callback)**
#### âœ… **1) ì²´í¬í¬ì¸íŠ¸ (ModelCheckpoint)**
- ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ì½œë°±  

```python
from tensorflow.keras.callbacks import ModelCheckpoint

checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')

model.fit(x_train, y_train, epochs=20, validation_split=0.2, callbacks=[checkpoint])
```

#### âœ… **2) ì¡°ê¸° ì¢…ë£Œ (Early Stopping)**
- ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨  

```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit(x_train, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping])
```

#### âœ… **3) TensorBoard**
- í›ˆë ¨ ê³¼ì • ì‹œê°í™”  

```python
from tensorflow.keras.callbacks import TensorBoard

tensorboard = TensorBoard(log_dir='./logs')

model.fit(x_train, y_train, epochs=20, validation_split=0.2, callbacks=[tensorboard])
```

---

### **ğŸ“Œ ìµœì¢… ì •ë¦¬**
âœ… **ANN** â†’ ë‹¨ì¼ ì‹ ê²½ë§(`Dense`) vs ë‹¤ì¤‘ ì‹ ê²½ë§(ì€ë‹‰ì¸µ å¤š)  
âœ… **íˆë“  ë ˆì´ì–´** â†’ ì…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ ë ˆì´ì–´  
âœ… **í™œì„±í™” í•¨ìˆ˜** â†’ `sigmoid`, `ReLU`, `LeakyReLU` ë“±  
âœ… **ê³¼ì í•© ë°©ì§€** â†’ `Dropout()` ì‚¬ìš©  
âœ… **ëª¨ë¸ ìƒì„±** â†’ `Sequential()` ì‚¬ìš©  
âœ… **ì†ì‹¤ í•¨ìˆ˜** â†’ ë¶„ë¥˜(`binary_crossentropy`), íšŒê·€(`mse`)  
âœ… **ëª¨ë¸ í‰ê°€** â†’ ë¶„ë¥˜(`accuracy`), íšŒê·€(`mse`)  
âœ… **ì½œë°±** â†’ `ModelCheckpoint`, `EarlyStopping`, `TensorBoard`  

âœ”ï¸ **ê³¼ì í•© í•´ê²°ë²•** â†’ `Dropout + EarlyStopping`  
âœ”ï¸ **ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì €ì¥** â†’ `ModelCheckpoint(save_best_only=True)`
