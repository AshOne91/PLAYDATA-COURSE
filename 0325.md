전통적인 프로그램
```
if fish_size >= 30:
  print('도미')
```

도미 vs 빙어
- 2개의 클래스(class)
- 분류(classification)
- 이진 분류(binary classification)

도미 데이터
bream_length = [25.4 ...
bream_weight = [242, ....
도미 무게와 길이 회귀 분석
도미의 무게(weight)와 길이(length) 관계를 분석하는 문제는 회귀(Regression) 문제임.


도미 vs 빙어 분류 (Binary Classification)
문제 정의:
입력(Input): 물고기의 길이, 무게 등의 특징(Feature)
출력(Output): 도미(1) 또는 빙어(0) (이진 분류)
목표: 주어진 물고기의 특징을 바탕으로 도미인지 빙어인지 분류

플랏(Plot)
📌 정의:
데이터를 시각화하는 그래프 전반을 의미
선 그래프(Line Plot), 바 그래프(Bar Plot), 산점도(Scatter Plot) 등 모든 그래프 포함
즉, 산점도도 플랏의 한 종류

산점도(Scatter Plot)
📌 정의:
두 개의 연속형 변수 간 관계를 점으로 나타내는 그래프
점들이 어떤 패턴을 형성하는지 확인하는 데 유용
상관 관계(Correlation) 분석에 많이 사용

도미와 빙어를 구분하려면?
도미와 빙어를 이진 분류(Binary Classification) 하려면 두 클래스(도미, 빙어)를 함께 학습해야 함.
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1. 도미 데이터 (길이, 무게)
domi_data = np.array([[25, 150], [30, 200], [35, 250]])

# 2. 빙어 데이터 (길이, 무게)
bingo_data = np.array([[8, 7], [9, 9], [10, 12]])

# 3. 클래스 레이블 (도미=1, 빙어=0)
domi_labels = np.ones(domi_data.shape[0])  # 도미는 1
bingo_labels = np.zeros(bingo_data.shape[0])  # 빙어는 0

# 4. 도미와 빙어 데이터 합치기
X = np.vstack((domi_data, bingo_data))  # 길이와 무게 데이터 합치기
y = np.concatenate((domi_labels, bingo_labels))  # 클래스 레이블 합치기

# 5. 훈련 데이터와 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 6. 데이터 표준화 (스케일링)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 7. 로지스틱 회귀 모델 학습
model = LogisticRegression()
model.fit(X_train, y_train)

# 8. 예측
y_pred = model.predict(X_test)

# 9. 정확도 평가
accuracy = accuracy_score(y_test, y_pred)
print("모델 정확도:", accuracy)

# 10. 데이터 시각화
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', label="Fish Data")
plt.xlabel("Length (cm)")
plt.ylabel("Weight (g)")
plt.title("도미 vs 빙어 산점도")
plt.legend()
plt.show()
```

`n_neighbors=len(X)`로 설정하면, **전체 데이터**를 이웃으로 간주하여 **다수결** 방식으로 예측을 진행합니다. 이 방식은 **샘플링 편향**이나 **과적합** 문제를 일으킬 수 있으며, **소수 클래스**의 예측 정확도가 떨어질 수 있습니다. 일반적으로 **적절한 K값**을 선택하는 것이 중요하며, `n_neighbors`를 너무 크게 설정하지 않는 것이 좋습니다.

**해결책**:
- K값을 **작게** 설정 (예: 5)
- **교차 검증**을 통해 최적의 K값 찾기
- **데이터 균형** 맞추기


K-최근접 알고리즘은 지도학습

- **훈련 세트 (Training Set)**: 모델을 학습시키는 데 사용되는 데이터.
- **테스트 세트 (Test Set)**: 모델의 성능을 평가하는 데 사용되는 데이터, 훈련에는 사용되지 않음.

훈련 세트로 모델을 학습한 후, 테스트 세트로 모델을 평가하여 성능을 확인합니다. 일반적으로 데이터의 70~80%는 훈련 세트, 나머지 20~30%는 테스트 세트로 나눕니다.

from imblearn.over_sampling import SMOTE
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# 데이터 로드
wine = load_wine()
X = wine.data
y = wine.target

# 훈련/테스트 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SMOTE 적용
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# 모델 훈련
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_res, y_train_res)

# 예측 및 평가
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))









