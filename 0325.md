ì „í†µì ì¸ í”„ë¡œê·¸ë¨
```
if fish_size >= 30:
  print('ë„ë¯¸')
```

ë„ë¯¸ vs ë¹™ì–´
- 2ê°œì˜ í´ë˜ìŠ¤(class)
- ë¶„ë¥˜(classification)
- ì´ì§„ ë¶„ë¥˜(binary classification)

ë„ë¯¸ ë°ì´í„°
bream_length = [25.4 ...
bream_weight = [242, ....
ë„ë¯¸ ë¬´ê²Œì™€ ê¸¸ì´ íšŒê·€ ë¶„ì„
ë„ë¯¸ì˜ ë¬´ê²Œ(weight)ì™€ ê¸¸ì´(length) ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” ë¬¸ì œëŠ” íšŒê·€(Regression) ë¬¸ì œì„.


ë„ë¯¸ vs ë¹™ì–´ ë¶„ë¥˜ (Binary Classification)
ë¬¸ì œ ì •ì˜:
ì…ë ¥(Input): ë¬¼ê³ ê¸°ì˜ ê¸¸ì´, ë¬´ê²Œ ë“±ì˜ íŠ¹ì§•(Feature)
ì¶œë ¥(Output): ë„ë¯¸(1) ë˜ëŠ” ë¹™ì–´(0) (ì´ì§„ ë¶„ë¥˜)
ëª©í‘œ: ì£¼ì–´ì§„ ë¬¼ê³ ê¸°ì˜ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ë„ë¯¸ì¸ì§€ ë¹™ì–´ì¸ì§€ ë¶„ë¥˜

í”Œë(Plot)
ğŸ“Œ ì •ì˜:
ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ëŠ” ê·¸ë˜í”„ ì „ë°˜ì„ ì˜ë¯¸
ì„  ê·¸ë˜í”„(Line Plot), ë°” ê·¸ë˜í”„(Bar Plot), ì‚°ì ë„(Scatter Plot) ë“± ëª¨ë“  ê·¸ë˜í”„ í¬í•¨
ì¦‰, ì‚°ì ë„ë„ í”Œëì˜ í•œ ì¢…ë¥˜

ì‚°ì ë„(Scatter Plot)
ğŸ“Œ ì •ì˜:
ë‘ ê°œì˜ ì—°ì†í˜• ë³€ìˆ˜ ê°„ ê´€ê³„ë¥¼ ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê·¸ë˜í”„
ì ë“¤ì´ ì–´ë–¤ íŒ¨í„´ì„ í˜•ì„±í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë° ìœ ìš©
ìƒê´€ ê´€ê³„(Correlation) ë¶„ì„ì— ë§ì´ ì‚¬ìš©

ë„ë¯¸ì™€ ë¹™ì–´ë¥¼ êµ¬ë¶„í•˜ë ¤ë©´?
ë„ë¯¸ì™€ ë¹™ì–´ë¥¼ ì´ì§„ ë¶„ë¥˜(Binary Classification) í•˜ë ¤ë©´ ë‘ í´ë˜ìŠ¤(ë„ë¯¸, ë¹™ì–´)ë¥¼ í•¨ê»˜ í•™ìŠµí•´ì•¼ í•¨.
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1. ë„ë¯¸ ë°ì´í„° (ê¸¸ì´, ë¬´ê²Œ)
domi_data = np.array([[25, 150], [30, 200], [35, 250]])

# 2. ë¹™ì–´ ë°ì´í„° (ê¸¸ì´, ë¬´ê²Œ)
bingo_data = np.array([[8, 7], [9, 9], [10, 12]])

# 3. í´ë˜ìŠ¤ ë ˆì´ë¸” (ë„ë¯¸=1, ë¹™ì–´=0)
domi_labels = np.ones(domi_data.shape[0])  # ë„ë¯¸ëŠ” 1
bingo_labels = np.zeros(bingo_data.shape[0])  # ë¹™ì–´ëŠ” 0

# 4. ë„ë¯¸ì™€ ë¹™ì–´ ë°ì´í„° í•©ì¹˜ê¸°
X = np.vstack((domi_data, bingo_data))  # ê¸¸ì´ì™€ ë¬´ê²Œ ë°ì´í„° í•©ì¹˜ê¸°
y = np.concatenate((domi_labels, bingo_labels))  # í´ë˜ìŠ¤ ë ˆì´ë¸” í•©ì¹˜ê¸°

# 5. í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 6. ë°ì´í„° í‘œì¤€í™” (ìŠ¤ì¼€ì¼ë§)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 7. ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ
model = LogisticRegression()
model.fit(X_train, y_train)

# 8. ì˜ˆì¸¡
y_pred = model.predict(X_test)

# 9. ì •í™•ë„ í‰ê°€
accuracy = accuracy_score(y_test, y_pred)
print("ëª¨ë¸ ì •í™•ë„:", accuracy)

# 10. ë°ì´í„° ì‹œê°í™”
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', label="Fish Data")
plt.xlabel("Length (cm)")
plt.ylabel("Weight (g)")
plt.title("ë„ë¯¸ vs ë¹™ì–´ ì‚°ì ë„")
plt.legend()
plt.show()
```

`n_neighbors=len(X)`ë¡œ ì„¤ì •í•˜ë©´, **ì „ì²´ ë°ì´í„°**ë¥¼ ì´ì›ƒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ **ë‹¤ìˆ˜ê²°** ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡ì„ ì§„í–‰í•©ë‹ˆë‹¤. ì´ ë°©ì‹ì€ **ìƒ˜í”Œë§ í¸í–¥**ì´ë‚˜ **ê³¼ì í•©** ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìœ¼ë©°, **ì†Œìˆ˜ í´ë˜ìŠ¤**ì˜ ì˜ˆì¸¡ ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ **ì ì ˆí•œ Kê°’**ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë©°, `n_neighbors`ë¥¼ ë„ˆë¬´ í¬ê²Œ ì„¤ì •í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

**í•´ê²°ì±…**:
- Kê°’ì„ **ì‘ê²Œ** ì„¤ì • (ì˜ˆ: 5)
- **êµì°¨ ê²€ì¦**ì„ í†µí•´ ìµœì ì˜ Kê°’ ì°¾ê¸°
- **ë°ì´í„° ê· í˜•** ë§ì¶”ê¸°


K-ìµœê·¼ì ‘ ì•Œê³ ë¦¬ì¦˜ì€ ì§€ë„í•™ìŠµ

- **í›ˆë ¨ ì„¸íŠ¸ (Training Set)**: ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë°ì´í„°.
- **í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ (Test Set)**: ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë°ì´í„°, í›ˆë ¨ì—ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ.

í›ˆë ¨ ì„¸íŠ¸ë¡œ ëª¨ë¸ì„ í•™ìŠµí•œ í›„, í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ëª¨ë¸ì„ í‰ê°€í•˜ì—¬ ì„±ëŠ¥ì„ í™•ì¸í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„°ì˜ 70~80%ëŠ” í›ˆë ¨ ì„¸íŠ¸, ë‚˜ë¨¸ì§€ 20~30%ëŠ” í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.

from imblearn.over_sampling import SMOTE
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# ë°ì´í„° ë¡œë“œ
wine = load_wine()
X = wine.data
y = wine.target

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SMOTE ì ìš©
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# ëª¨ë¸ í›ˆë ¨
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_res, y_train_res)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))









