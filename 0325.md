전통적인 프로그램
```
if fish_size >= 30:
  print('도미')
```

도미 vs 빙어
- 2개의 클래스(class)
- 분류(classification)
- 이진 분류(binary classification)

도미 데이터
bream_length = [25.4 ...
bream_weight = [242, ....
도미 무게와 길이 회귀 분석
도미의 무게(weight)와 길이(length) 관계를 분석하는 문제는 회귀(Regression) 문제임.


도미 vs 빙어 분류 (Binary Classification)
문제 정의:
입력(Input): 물고기의 길이, 무게 등의 특징(Feature)
출력(Output): 도미(1) 또는 빙어(0) (이진 분류)
목표: 주어진 물고기의 특징을 바탕으로 도미인지 빙어인지 분류

플랏(Plot)
📌 정의:
데이터를 시각화하는 그래프 전반을 의미
선 그래프(Line Plot), 바 그래프(Bar Plot), 산점도(Scatter Plot) 등 모든 그래프 포함
즉, 산점도도 플랏의 한 종류

산점도(Scatter Plot)
📌 정의:
두 개의 연속형 변수 간 관계를 점으로 나타내는 그래프
점들이 어떤 패턴을 형성하는지 확인하는 데 유용
상관 관계(Correlation) 분석에 많이 사용

도미와 빙어를 구분하려면?
도미와 빙어를 이진 분류(Binary Classification) 하려면 두 클래스(도미, 빙어)를 함께 학습해야 함.
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1. 도미 데이터 (길이, 무게)
domi_data = np.array([[25, 150], [30, 200], [35, 250]])

# 2. 빙어 데이터 (길이, 무게)
bingo_data = np.array([[8, 7], [9, 9], [10, 12]])

# 3. 클래스 레이블 (도미=1, 빙어=0)
domi_labels = np.ones(domi_data.shape[0])  # 도미는 1
bingo_labels = np.zeros(bingo_data.shape[0])  # 빙어는 0

# 4. 도미와 빙어 데이터 합치기
X = np.vstack((domi_data, bingo_data))  # 길이와 무게 데이터 합치기
y = np.concatenate((domi_labels, bingo_labels))  # 클래스 레이블 합치기

# 5. 훈련 데이터와 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 6. 데이터 표준화 (스케일링)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 7. 로지스틱 회귀 모델 학습
model = LogisticRegression()
model.fit(X_train, y_train)

# 8. 예측
y_pred = model.predict(X_test)

# 9. 정확도 평가
accuracy = accuracy_score(y_test, y_pred)
print("모델 정확도:", accuracy)

# 10. 데이터 시각화
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', label="Fish Data")
plt.xlabel("Length (cm)")
plt.ylabel("Weight (g)")
plt.title("도미 vs 빙어 산점도")
plt.legend()
plt.show()
```

`n_neighbors=len(X)`로 설정하면, **전체 데이터**를 이웃으로 간주하여 **다수결** 방식으로 예측을 진행합니다. 이 방식은 **샘플링 편향**이나 **과적합** 문제를 일으킬 수 있으며, **소수 클래스**의 예측 정확도가 떨어질 수 있습니다. 일반적으로 **적절한 K값**을 선택하는 것이 중요하며, `n_neighbors`를 너무 크게 설정하지 않는 것이 좋습니다.

**해결책**:
- K값을 **작게** 설정 (예: 5)
- **교차 검증**을 통해 최적의 K값 찾기
- **데이터 균형** 맞추기


K-최근접 알고리즘은 지도학습

- **훈련 세트 (Training Set)**: 모델을 학습시키는 데 사용되는 데이터.
- **테스트 세트 (Test Set)**: 모델의 성능을 평가하는 데 사용되는 데이터, 훈련에는 사용되지 않음.

훈련 세트로 모델을 학습한 후, 테스트 세트로 모델을 평가하여 성능을 확인합니다. 일반적으로 데이터의 70~80%는 훈련 세트, 나머지 20~30%는 테스트 세트로 나눕니다.

from imblearn.over_sampling import SMOTE
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# 데이터 로드
wine = load_wine()
X = wine.data
y = wine.target

# 훈련/테스트 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SMOTE 적용
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# 모델 훈련
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_res, y_train_res)

# 예측 및 평가
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

0~1로 정규화(Min-Max Scaling)해도 됨. 다만, **어떤 상황에서는 표준화(Z-score 변환)가 더 적절함**.  

---

## **📌 Min-Max Scaling (0~1 정규화) vs. Standard Scaling (표준화)**

| 방식 | 변환 방식 | 장점 | 단점 |
|---|---|---|---|
| **Min-Max Scaling (정규화)** | \(\frac{X - X_{\min}}{X_{\max} - X_{\min}}\) | - 값이 0~1 사이로 고정됨 <br> - 분포가 유지됨 | - 이상치(outlier)에 영향을 많이 받음 |
| **Standard Scaling (표준화)** | \(Z = \frac{X - \mu}{\sigma}\) | - 평균이 0, 표준 편차가 1이 됨 <br> - 이상치의 영향이 적음 | - 데이터 분포를 유지하지 않음 |

---

## **📌 언제 Min-Max Scaling을 쓰고, 언제 Standard Scaling을 써야 할까?**

### ✅ **Min-Max Scaling (0~1 정규화) 적합한 경우**
1. **데이터의 분포가 특정 범위(예: 0~100) 내에서 고르게 퍼져 있는 경우**  
2. **딥러닝 모델(CNN, RNN)** → 특히 **이미지 데이터**에서 많이 사용됨  
3. **특징 값의 범위가 균일할 때**  
   - 예: 센서 데이터(0~10V), RGB 값(0~255)

### ✅ **Standard Scaling (표준화) 적합한 경우**
1. **데이터가 정규분포를 따를 때**  
2. **이상치(Outlier)가 있는 경우** → 평균과 표준 편차로 정규화하면 극단적인 값의 영향을 덜 받음  
3. **거리 기반 알고리즘 (KNN, SVM, PCA 등)에서 많이 사용**  
   - 예: KNN, PCA, 선형 회귀, 로지스틱 회귀  

---

## **📌 결론**
- **"모든 데이터에서 0~1 정규화가 최선은 아님."**  
- 이상치 많거나 정규분포에 가까운 경우 → **표준화(평균 0, 표준편차 1) 더 적합**  
- 분포 유지가 중요한 경우 → **Min-Max Scaling(0~1 정규화) 사용 가능**  

어떤 방법을 선택할지는 **데이터의 특성과 사용하는 머신러닝 알고리즘에 따라 다름.**

머신러닝에서 **표준화(standardization)**와 **비표준화(non-standardization)**의 차이와 사용 시점을 정리해 보겠음.

---

## **📌 표준화 (Standardization)**

### **정의**:  
- **표준화**는 각 특성(feature)의 **평균을 0, 표준편차를 1로 변환**하는 과정이다.  
- 변환 공식:  
  \[
  Z = \frac{X - \mu}{\sigma}
  \]
  - \(X\): 원본 데이터 값  
  - \(\mu\): 평균  
  - \(\sigma\): 표준편차

### **언제 사용해야 할까?**
1. **데이터의 분포가 정규분포에 가까운 경우**  
   - 예: 키, 몸무게 등 대부분의 특성들이 평균을 중심으로 고르게 분포된 경우  
2. **이상치(Outlier)가 중요할 때**  
   - 표준화는 이상치에 덜 민감하여 영향을 받지 않음.  
3. **거리 기반 모델에서 사용**  
   - 예: **KNN, SVM, 선형 회귀**, **로지스틱 회귀** 등
   - 이 모델들은 **각 특성의 범위가 다르면** 학습에 불리할 수 있음.  
4. **PCA(주성분 분석)**  
   - 주성분 분석(PCA)과 같은 **차원 축소 기법**에서 필수적임.

### **장점**:
- **모든 특성이 같은 스케일로 변환**되기 때문에 **모델 성능 향상**.
- **이상치**에 대한 민감도가 적음.
  
### **단점**:
- **데이터가 정규분포를 따르지 않으면** 표준화가 덜 효과적일 수 있음.

---

## **📌 비표준화 (Non-standardization)**

### **정의**:  
- **비표준화**는 데이터를 변형하지 않고 원래의 형태를 그대로 사용하는 것이다.
- 이 경우 **특성들의 스케일이나 범위가 그대로 유지**된다.

### **언제 사용해야 할까?**
1. **트리 기반 모델에서 사용**  
   - 예: **랜덤 포레스트**, **결정 트리**, **XGBoost** 등  
   - 트리 기반 모델은 **특성 간의 관계나 범위**에 영향을 받지 않기 때문에 표준화가 필요 없다.  
2. **이미 특성들이 비슷한 범위에 있을 때**  
   - 예: 특정 범위 내에 이미 고르게 분포된 데이터.
3. **모델 해석이 중요할 때**  
   - 원본 데이터를 그대로 사용하면 각 특성의 해석이 더 직관적일 수 있다.

### **장점**:
- **간단하고 빠름**.
- **해석 용이**: 원본 데이터로 학습되므로 결과를 해석하기 쉽다.

### **단점**:
- **거리 기반 모델에서 학습이 불안정할 수 있음**.
- 특성들의 범위가 다르면, **모델의 가중치**가 특정 특성에 대해 왜곡될 수 있다.

---

## **📌 표준화 vs 비표준화 (비교)**

| 구분 | **표준화 (Standardization)** | **비표준화 (Non-standardization)** |
|---|---|---|
| **변환 방식** | 평균 0, 표준편차 1로 변환 | 원본 데이터 그대로 사용 |
| **적합한 알고리즘** | 거리 기반 모델(KNN, SVM, 로지스틱 회귀), PCA, 선형 회귀 | 트리 기반 모델(랜덤 포레스트, 결정 트리, XGBoost 등) |
| **이상치 처리** | 이상치에 덜 민감 | 이상치에 민감 |
| **장점** | 모델 성능 향상, 이상치의 영향 감소 | 빠르고 직관적 |
| **단점** | 정규분포가 아닌 경우 성능 저하 가능 | 거리 기반 모델에 불리 |

---

## **📌 결론**
- **표준화**는 **거리 기반 모델**이나 **정규분포를 가정하는 모델**에서 필요하고, 데이터의 **스케일 차이를 줄여주기 위해 사용**된다.  
- **비표준화**는 **트리 기반 모델**이나 **데이터 스케일이 고르게 분포되어 있는 경우** 사용한다.

- **과대적합 (Overfitting)**: 모델이 훈련 데이터에 지나치게 맞춰져서, 새로운 데이터에 대해 예측 성능이 떨어지는 현상.
- **과소적합 (Underfitting)**: 모델이 훈련 데이터의 패턴을 충분히 학습하지 못하고, 예측 성능이 낮은 현상.

**이웃 개수 줄이기 (KNN 모델에서 K값 조정)**:
- **K값을 줄이면**: 모델이 훈련 데이터에 민감해져서 **과대적합**이 발생할 수 있음. (훈련 데이터에 지나치게 맞춤)
- **K값을 늘리면**: 모델이 더 일반화되어 **과소적합**이 발생할 수 있음. (훈련 데이터와의 차이가 커짐)

적절한 K값을 찾는 것이 중요하며, **교차 검증**을 통해 최적의 K값을 선택해야 합니다.

**하이퍼파라미터 (Hyperparameter)**는 머신러닝 모델을 훈련할 때 **사용자가 설정하는 값**으로, 모델 학습 과정에서 **학습되지 않는 파라미터**입니다. 즉, 하이퍼파라미터는 모델을 훈련시키기 전에 미리 설정해야 하는 값들입니다.

### **하이퍼파라미터 예시**
1. **학습률 (Learning Rate)**: 모델이 업데이트되는 속도를 결정하는 값. 너무 크면 최적값을 지나칠 수 있고, 너무 작으면 학습이 느려지거나 지역 최적값에 빠질 수 있습니다.
   
2. **트리 깊이 (Tree Depth)**: 의사결정트리에서 트리의 최대 깊이를 설정하는 값. 너무 깊으면 **과대적합**, 너무 얕으면 **과소적합**을 초래할 수 있습니다.

3. **이웃 개수 (K in KNN)**: K-최근접 이웃 알고리즘에서 사용할 이웃의 개수 K를 설정하는 값. K가 너무 작으면 과대적합이 일어나고, 너무 크면 과소적합이 발생할 수 있습니다.

4. **배치 크기 (Batch Size)**: 딥러닝에서 한 번에 네트워크에 입력하는 데이터 샘플의 개수입니다. 큰 배치 크기는 더 빠르지만 메모리 소모가 크고, 작은 배치 크기는 더 느리지만 모델의 일반화 성능에 좋을 수 있습니다.

5. **정규화 파라미터 (Regularization Parameters)**: L1/L2 정규화에서 모델의 복잡도를 제어하는 값입니다. 정규화를 너무 많이 적용하면 과소적합이 발생하고, 너무 적게 적용하면 과대적합이 발생할 수 있습니다.

### **하이퍼파라미터 튜닝 방법**
1. **그리드 서치 (Grid Search)**: 여러 하이퍼파라미터 값들을 **전수 조사**하여 최적의 값을 찾는 방법.
2. **랜덤 서치 (Random Search)**: 그리드 서치보다 더 빠르게 하이퍼파라미터를 검색할 수 있도록, 하이퍼파라미터의 값들을 **무작위로 샘플링**하여 최적의 값을 찾는 방법.
3. **베이지안 최적화**: 이전의 실험 결과를 바탕으로 **확률적으로** 하이퍼파라미터를 탐색하는 방법.
4. **교차 검증 (Cross-Validation)**: 하이퍼파라미터를 선택할 때, **교차 검증**을 사용하여 모델 성능을 평가하고, 최적의 하이퍼파라미터를 찾습니다.

### **하이퍼파라미터와 모델 성능**
- 적절한 하이퍼파라미터 선택은 모델 성능에 매우 중요한 영향을 미칩니다.
- 하이퍼파라미터 튜닝을 통해 모델이 **과대적합** 또는 **과소적합** 되는 것을 방지하고, **최적의 성능**을 발휘할 수 있습니다.

**X가 하나일 때 정규화가 필요 없는 이유**는 **특성의 분포와 크기에 관계없이 데이터의 스케일을 변경할 필요가 없기 때문**입니다.

정규화 (또는 표준화)는 일반적으로 **여러 개의 특성(변수)**이 있을 때, 각 특성이 가진 **값의 범위가 다를 때** 필요합니다. 이때 특성들 간의 **단위 차이나 범위 차이**로 인해 머신러닝 알고리즘의 성능이 영향을 받을 수 있습니다. 예를 들어, 한 특성이 0~1 사이의 값으로 표현되고, 다른 특성이 1000~10000 사이의 값으로 표현되면, 후자의 특성이 모델 학습에서 더 큰 영향을 미칠 수 있습니다. 

### **X가 하나일 때 정규화가 필요 없는 이유**
- **하나의 특성**만 있다면, 특성 자체의 범위나 분포가 학습 과정에 영향을 미치지 않습니다.
- 예를 들어, **X가 하나**이고, 그 값이 0~10 사이에 있다면, 이 값이 다른 특성과 비교할 수 없기 때문에 정규화나 표준화의 효과가 없습니다.
- **단일 특성**이기 때문에 **거리 기반 알고리즘**(예: KNN, SVM 등)에서도 큰 영향을 미치지 않으며, 그냥 그대로 모델에 입력해도 문제가 없습니다.

### 예시
- **X가 하나일 때**: 
  ```python
  X = np.array([[1], [2], [3], [4], [5]])  # 특성이 하나일 때
  ```

- **정규화 필요 없는 경우**: 위와 같은 데이터에서 정규화를 적용해도 `1`과 `5` 사이의 차이만큼만 예측값에 영향을 미칠 것이기 때문에, 특별한 효과를 기대할 수 없습니다.

결론적으로, **하나의 특성만 있을 경우** 정규화나 표준화는 **학습에 큰 영향을 미치지 않기 때문에 필요하지 않습니다**. 여러 특성이 있을 때 이들이 서로 다른 범위를 가진다면 그때 정규화가 중요한 역할을 하게 됩니다.





