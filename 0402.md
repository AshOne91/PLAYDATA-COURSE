### **순차 데이터 (Sequential Data)**  
- 시간 또는 순서에 따라 변화하는 데이터를 의미함.  
- 예시: 주가 데이터, 자연어 문장, 음성 신호, 센서 데이터, 비디오 프레임 등.  

---

### **RNN (순환 신경망, Recurrent Neural Network)**  
- 순차 데이터를 처리하기 위한 신경망 구조.  
- 이전 단계의 정보를 기억하고, 이를 활용하여 다음 단계의 출력을 계산함.  
- **특징**: 은닉 상태(\( h_t \))를 사용하여 과거 정보를 저장하고, 이를 활용하여 다음 출력을 생성함.  

---

### **RNN의 입력**  
- RNN은 **순차 데이터의 각 시점별(feature별) 입력**을 받음.  
- 입력 형태:  
  \[
  X = [x_1, x_2, x_3, ..., x_T]
  \]
  - 여기서 \( x_t \) 는 각 시점 \( t \)에서의 입력 데이터  
  - \( T \)는 전체 시퀀스 길이  

- **입력 예시** (자연어 처리 - "bat" 예제):  
  - "bat"를 글자 단위 입력으로 나눠서 처리하면,  
    \[
    X = [\text{"b"}, \text{"a"}, \text{"t"}]
    \]
  - 각 글자는 임베딩 벡터로 변환됨.  
    \[
    X = [\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3]
    \]
  - \( \mathbf{x}_1 = \) "b"의 임베딩 벡터  
  - \( \mathbf{x}_2 = \) "a"의 임베딩 벡터  
  - \( \mathbf{x}_3 = \) "t"의 임베딩 벡터  

---

추가로 설명이 필요하면 요청 바람.

IMDB 데이터셋
케라스로 IMDB 데이터 불러오기
수치화된 문장
input은 단어
target은 긍정, 부정

훈련 세트 준비
히스토그램으로 그리기(한번 보고 적당히 해야지)

시퀀스 패딩(문장의 길이를 맞춰주는것 // 모양을 맞춰야징)
(앞에 채우냐 뒤에 채우냐 옵션이 있음)
짤라지던가 채우던가

순환 신경망 모델 만들기

원핫인코딩?
원핫으로 바뀌면 좀 길어짐

모델구조 확인해보고

모델 훈련 (adm)

단어 임베딩?

IMDB 데이터셋을 이용한 순환 신경망 모델을 구축하는 과정을 예제와 함께 상세히 설명하겠음.  

---

## **1. IMDB 데이터셋 불러오기**  
IMDB 데이터셋은 영화 리뷰에 대한 감성 분석 데이터셋으로, 리뷰 문장을 **수치화된 형태(정수 시퀀스)**로 제공함.  

### **IMDB 데이터셋 불러오기**  
케라스를 이용하여 IMDB 데이터셋을 불러오면, 리뷰 문장이 **단어의 정수 인덱스 시퀀스**로 변환된 형태로 제공됨.  

```python
from tensorflow.keras.datasets import imdb

# IMDB 데이터셋 로드 (상위 10,000개 단어만 사용)
num_words = 10000  # 가장 많이 등장한 10,000개 단어만 선택
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)

print(f"훈련 데이터 개수: {len(x_train)}, 테스트 데이터 개수: {len(x_test)}")
print("첫 번째 리뷰의 정수 인코딩:", x_train[0])  # 정수 시퀀스로 변환된 문장
print("첫 번째 리뷰의 라벨 (0: 부정, 1: 긍정):", y_train[0])
```
🔹 `num_words=10000`: 가장 많이 등장하는 단어 10,000개만 사용  
🔹 `x_train[i]`: i번째 리뷰의 **정수 인코딩된 문장**  
🔹 `y_train[i]`: 해당 리뷰가 **긍정(1)인지 부정(0)인지** 나타내는 정답 라벨  

---

## **2. 데이터 시각화 - 리뷰 길이 분포 확인**  
리뷰마다 길이가 다 다름. 이를 확인하기 위해 히스토그램을 그림.  

```python
import matplotlib.pyplot as plt
import numpy as np

review_lengths = [len(review) for review in x_train]  # 리뷰 길이 리스트

plt.hist(review_lengths, bins=50, color='blue', alpha=0.7)
plt.xlabel("리뷰 길이")
plt.ylabel("리뷰 개수")
plt.title("IMDB 리뷰 길이 분포")
plt.show()
```
🔹 `plt.hist(review_lengths, bins=50)`: 리뷰 길이를 50개의 구간으로 나누어 히스토그램을 그림.  
🔹 대부분의 리뷰 길이가 특정 범위(약 100~500 단어) 안에 있음.  

---

## **3. 시퀀스 패딩 (Sequence Padding)**
모델에 입력할 때 **모든 문장의 길이를 맞춰야 함**. 이를 위해 **짧은 문장은 0을 추가하고, 긴 문장은 자름**.  

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_len = 200  # 모든 문장의 길이를 200으로 통일
x_train_pad = pad_sequences(x_train, maxlen=max_len, padding='post', truncating='post')
x_test_pad = pad_sequences(x_test, maxlen=max_len, padding='post', truncating='post')

print("패딩된 첫 번째 리뷰:", x_train_pad[0])
print("패딩된 리뷰 길이:", len(x_train_pad[0]))  # 항상 max_len(200)이어야 함
```
🔹 `maxlen=200`: 모든 리뷰 길이를 **200개 단어로 맞춤**  
🔹 `padding='post'`: **뒤쪽에 0을 채움** (`pre`로 하면 앞쪽에 0을 채움)  
🔹 `truncating='post'`: **긴 문장은 뒤를 자름** (`pre`로 하면 앞쪽을 자름)  

---

## **4. 순환 신경망 (RNN) 모델 구축**
LSTM을 이용하여 순환 신경망 모델을 구축함.  

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# 모델 정의
model = Sequential([
    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),  # 단어 임베딩
    LSTM(64, return_sequences=False),  # LSTM 층 (64개의 LSTM 유닛)
    Dense(1, activation='sigmoid')  # 출력층 (이진 분류)
])

# 모델 컴파일
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 모델 구조 확인
model.summary()
```
🔹 `Embedding(input_dim=10000, output_dim=128)`: 단어 임베딩 벡터 크기를 128로 설정  
🔹 `LSTM(64)`: LSTM 층을 사용하여 64개의 유닛을 활용  
🔹 `Dense(1, activation='sigmoid')`: 감성 분류 (긍정/부정) → **이진 분류이므로 sigmoid 사용**  

---

## **5. 원-핫 인코딩 vs 단어 임베딩**  
**원-핫 인코딩(One-hot encoding)**은 단어를 벡터로 변환하는 방식 중 하나임.  
- 단점: 단어가 많아지면 **차원이 매우 커짐**.  
- 예시: 단어 사전 크기가 10,000이면, 각 단어는 **10,000차원 벡터**가 됨.  

```python
from tensorflow.keras.utils import to_categorical

# 원-핫 인코딩 예제
word_index = 3  # 예제 단어 인덱스
one_hot_vector = to_categorical(word_index, num_classes=10000)

print("단어의 원-핫 벡터:", one_hot_vector)
```
🔹 단어 인덱스 `word_index=3` → 원-핫 벡터는 `[0, 0, 0, 1, 0, 0, ..., 0]` 형태  

💡 **단어 임베딩(Embedding)**을 사용하면, 원-핫 인코딩보다 효율적이며 단어 간 의미를 학습할 수 있음.  

---

## **6. 모델 훈련**
```python
# 모델 훈련
history = model.fit(x_train_pad, y_train, epochs=5, batch_size=64, validation_data=(x_test_pad, y_test))
```
🔹 `epochs=5`: 5번 반복 학습  
🔹 `batch_size=64`: 한 번에 64개의 데이터를 학습  
🔹 `validation_data`: 테스트 데이터를 이용해 성능 확인  

---

## **7. 모델 평가 및 예측**
```python
# 모델 평가
loss, acc = model.evaluate(x_test_pad, y_test)
print(f"테스트 정확도: {acc:.4f}")

# 예측 예제
sample_review = x_test_pad[0].reshape(1, -1)  # 샘플 리뷰
prediction = model.predict(sample_review)

print(f"예측 확률: {prediction[0][0]:.4f}")  # 0에 가까우면 부정, 1에 가까우면 긍정
```
🔹 `model.evaluate()`: 테스트 데이터로 정확도를 계산  
🔹 `model.predict()`: 예측 수행 (0이면 부정, 1이면 긍정)  

---

### **✅ 정리**
1️⃣ **IMDB 데이터셋 불러오기** → 단어를 **정수 인덱스로 변환된 시퀀스** 형태  
2️⃣ **리뷰 길이 시각화** → 히스토그램으로 문장 길이 확인  
3️⃣ **시퀀스 패딩** → 모든 리뷰의 길이를 맞춤  
4️⃣ **순환 신경망(RNN) 모델 구축** → `Embedding + LSTM + Dense`  
5️⃣ **원-핫 인코딩 vs 단어 임베딩** → 임베딩이 더 효율적  
6️⃣ **모델 훈련 및 평가** → `adam` 옵티마이저 사용하여 학습  
7️⃣ **예측 테스트** → 감성 분석 결과 확인  

추가 질문 있으면 요청 바람.


단어 임베딩
 전통적으로 단어는 원핫표현
 단점 : 단어의 유사성을 파악할 수 없음 cat dog는 의미상 유사하지만 벡터간의 거리는 동일하거나 등..
 차원 문제 : 어휘가 크면 벡터차원이 증가 (희소행렬문제)
 해결 : 단어임베딩은 단어를 저차원(30~500차원) 실수 벡터로 표현해서 의미적으로 유사한 단어는 벡터상에서 가까운 위치에 배치
 ㅊ� `Embedding(input_dim=10000, output_dim=128)`: 단어 임베딩 벡터 크기를 128로 설정  
🔹 `LSTM(64)`: LSTM 층을 사용하여 64개의 유닛을 활용  
🔹 `Dense(1, activation='sigmoid')`: 감성 분류 (긍정/부정) → **이진 분류이므로 sigmoid 사용**  

---

## **5. 원-핫 인코딩 vs 단어 임베딩**  
**원-핫 인코딩(One-hot encoding)**은 단어를 벡터로 변환하는 방식 중 하나임.  
- 단점: 단어가 많아지면 **차원이 매우 커짐**.  
- 예시: 단어 사전 크기가 10,000이면, 각 단어는 **10,000차원 벡터**가 됨.  

```python
from tensorflow.keras.utils import to_categorical

# 원-핫 인코딩 예제
word_index = 3  # 예제 단어 인덱스
one_hot_vector = to_categorical(word_index, num_classes=10000)

print("단어의 원-핫 벡터:", one_hot_vector)
```
🔹 단어 인덱스 `word_index=3` → 원-핫 벡터는 `[0, 0, 0, 1, 0, 0, ..., 0]` 형태  

💡 **단어 임베딩(Embedding)**을 사용하면, 원-핫 인코딩보다 효율적이며 단어 간 의미를 학습할 수 있음.  

---

## **6. 모델 훈련**
```python
# 모델 훈련
history = model.fit(x_train_pad, y_train, epochs=5, batch_size=64, validation_data=(x_test_pad, y_test))
```
🔹 `epochs=5`: 5번 반복 학습  
🔹 `batch_size=64`: 한 번에 64개의 데이터를 학습  
🔹 `validation_data`: 테스트 데이터를 이용해 성능 확인  

---

## **7. 모델 평가 및 예측**
```python
# 모델 평가
loss, acc = model.evaluate(x_test_pad, y_test)
print(f"테스트 정확도: {acc:.4f}")

# 예측 예제
sample_review = x_test_pad[0].reshape(1, -1)  # 샘플 리뷰
prediction = model.predict(sample_review)

print(f"예측 확률: {prediction[0][0]:.4f}")  # 0에 가까우면 부정, 1에 가까우면 긍정
```
🔹 `model.evaluate()`: 테스트 데이터로 정확도를 계산  
🔹 `model.predict()`: 예측 수행 (0이면 부정, 1이면 긍정)  

---

### **✅ 정리**
1️⃣ **IMDB 데이터셋 불러오기** → 단어를 **정수 인덱스로 변환된 시퀀스** 형태  
2️⃣ **리뷰 길이 시각화** → 히스토그램으로 문장 길이 확인  
3️⃣ **시퀀스 패딩** → 모든 리뷰의 길이를 맞춤  
4️⃣ **순환 신경망(RNN) 모델 구축** → `Embedding + LSTM + Dense`  
5️⃣ **원-핫 인코딩 vs 단어 임베딩** → 임베딩이 더 효율적  
6️⃣ **모델 훈련 및 평가** → `adam` 옵티마이저 사용하여 학습  
7️⃣ **예측 테스트** → 감성 분석 결과 확인  

추가 질문 있으면 요청 바람.


단어 임베딩
 전통적으로 단어는 원핫표현
 단점 : 단어의 유사성을 파악할 수 없음 cat dog는 의미상 유사하지만 벡터간의 거리는 동일하거나 등..
 차원 문제 : 어휘가 크면 벡터차원이 증가 (희소행렬문제)
 해결 : 단어임베딩은 단어를 저차원(30~500차원) 실수 벡터로 표현해서 의미적으로 유사한 단어는 벡터상에서 가까운 위치에 배치
 의미적 유사성 : king, queen 이런 데이터는 벡터공간에서 가깝다
 king-man ~ queen-woman
 원핫보다는 훨씬 적은 저차원

Word2Vec
 Goole 만듦
 CBOW(Continuous Bag of Words) : 주변단어로 중심단어를 예측
 Skip-gram : 중심단어로 주변단어를 예측

손실함수

학습과정
  단어쌍 생성 : 문장에서 중심단어, 문맥단어쌍을 생성
  i like to eat apples ('like', 'i') ('like', 'to')
  모델 학습 skip-gram wt를 입력으로 받아서 wt+j를 예측하도록 학습rediction = model.predict(sample_review)

print(f"예측 확률: {prediction[0][0]:.4f}")  # 0에 가까우면 부정, 1에 가까우면 긍정
```
🔹 `model.evaluate()`: 테스트 데이터로 정확도를 계산  
🔹 `model.predict()`: 예측 수행 (0이면 부정, 1이면 긍정)  

```

### **✅ 정리**
1️⃣ **IMDB 데이터셋 불러오기** → 단어를 **정수 인덱스로 변환된 시퀀스** 형태  
2️⃣ **리뷰 길이 시각화** → 히스토그램으로 문장 길이 확인  
3️⃣ **시퀀스 패딩** → 모든 리뷰의 길이를 맞춤  
4️⃣ **순환 신경망(RNN) 모델 구축** → `Embedding + LSTM + Dense`  
5️⃣ **원-핫 인코딩 vs 단어 임베딩** → 임베딩이 더 효율적  
6️⃣ **모델 훈련 및 평가** → `adam` 옵티마이저 사용하여 학습  
7️⃣ **예측 테스트** → 감성 분석 결과 확인  

추가 질문 있으면 요청 바람.


단어 임베딩
 전통적으로 단어는 원핫표현
 단점 : 단어의 유사성을 파악할 수 없음 cat dog는 의미상 유사하지만 벡터간의 거리는 동일하거나 등..
 차원 문제 : 어휘가 크면 벡터차원이 증가 (희소행렬문제)
 해결 : 단어임베딩은 단어를 저차원(30~500차원) 실수 벡터로 표현해서 의미적으로 유사한 단어는 벡터상에서 가까운 위치에 배치
 의미적 유사성 : king, queen 이런 데이터는 벡터공간에서 가깝다
 king-man ~ queen-woman
 원핫보다는 훨씬 적은 저차원

Word2Vec
 Goole 만듦
 CBOW(Continuous Bag of Words) : 주변단어로 중심단어를 예측
 Skip-gram : 중심단어로 주변단어를 예측

손실함수

학습과정
  단어쌍 생성 : 문장에서 중심단어, 문맥단어쌍을 생성
  i like to eat apples ('like', 'i') ('like', 'to')
  모델 학습 skip-gram wt를 입력으로 받아서 wt+j를 예측하도록 학습

Word2Vec
 king queen은 문맥에서 자주 유사한 위치에 등장(love 주제) 학습후
  king = [0.8, 0.2, -0.1]
  queen = [0.7, 0.3, -0.2]
  loves = [0.1, 0.9, 0.5]

  ## **단어 임베딩 (Word Embedding)**  

### **1. 전통적인 단어 표현 방식 - 원핫 인코딩 (One-Hot Encoding)**
전통적으로 단어를 표현하는 방법은 **원핫 인코딩(One-Hot Encoding)**이었습니다.  
원핫 인코딩은 단어를 **고유한 정수 인덱스로 변환한 후, 해당 인덱스의 위치만 1이고 나머지는 0인 벡터**로 표현하는 방식입니다.

#### **예제 1: 원핫 인코딩**
어휘 집합 (Vocabulary) = { "cat", "dog", "apple", "banana" }  
각 단어에 대해 원핫 벡터를 만들면 다음과 같습니다.

| 단어   | 원핫 벡터 |
|--------|--------------------------------|
| cat    | [1, 0, 0, 0] |
| dog    | [0, 1, 0, 0] |
| apple  | [0, 0, 1, 0] |
| banana | [0, 0, 0, 1] |

#### **📌 원핫 인코딩의 단점**
1. **단어 간의 유사성을 반영하지 못함**  
   - "cat"과 "dog"는 의미적으로 유사하지만, 원핫 벡터에서는 완전히 다른 벡터로 표현됨.  
   - 두 벡터 간의 거리(코사인 유사도)를 계산해도 유사성을 알 수 없음.  
2. **고차원 문제 (희소 행렬, Sparse Matrix)**  
   - 단어의 개수가 수십만 개라면, 벡터 차원이 엄청나게 커짐.  
   - 많은 차원에서 대부분의 값이 0이기 때문에 비효율적임.  
3. **일반화 불가능**  
   - 모델이 새로운 단어를 만나면 처리할 방법이 없음.  

이러한 문제를 해결하기 위해 나온 것이 **단어 임베딩 (Word Embedding)**입니다.

---

### **2. 단어 임베딩 (Word Embedding)**
단어 임베딩은 **각 단어를 저차원 실수 벡터로 변환하는 방법**입니다.  
**유사한 의미를 가지는 단어는 벡터 공간에서 가까운 위치에 배치**됩니다.

#### **📌 단어 임베딩의 특징**
- **차원 축소**: 단어를 30~500 차원의 실수 벡터로 변환 → 원핫보다 훨씬 적은 차원 사용
- **의미적 유사성 반영**: king과 queen, cat과 dog 같은 의미적 유사성을 가진 단어는 유사한 벡터를 가짐.
- **벡터 연산 가능**: 단어의 관계를 벡터 연산으로 표현할 수 있음. (예: "king - man + woman ≈ queen")

#### **예제 2: 단어 임베딩의 의미적 유사성**
훈련을 마친 Word2Vec 모델이 다음과 같은 벡터를 학습했다고 가정함:

| 단어   | 벡터 표현 |
|--------|---------------------|
| king   | [0.8, 0.2, -0.1] |
| queen  | [0.7, 0.3, -0.2] |
| loves  | [0.1, 0.9, 0.5] |

- "king"과 "queen"은 문맥에서 자주 비슷한 위치에서 등장하므로, 벡터값이 유사함.
- 단어 간의 의미적 관계를 반영할 수 있음.

---

### **3. Word2Vec**
**Word2Vec**은 **구글(Google)**이 개발한 단어 임베딩 기법입니다.  
단어의 문맥을 학습하여 의미적으로 유사한 단어가 유사한 벡터를 가지도록 만듭니다.

Word2Vec에는 두 가지 학습 방법이 있음:
1. **CBOW (Continuous Bag of Words)**  
   - **주변 단어(문맥) → 중심 단어 예측**  
   - 예제: "I like to eat apples"  
     - ( "I", "to" ) → "like"  
     - ( "to", "eat" ) → "like"  
   - 전체 문장을 보면, "like"는 "I"와 "to" 사이에서 많이 등장하므로 의미를 학습할 수 있음.
  
2. **Skip-gram**  
   - **중심 단어 → 주변 단어 예측**  
   - 예제: "I like to eat apples"  
     - "like" → "I"  
     - "like" → "to"  
   - 중심 단어 하나를 가지고, 주변 단어들을 예측하도록 학습함.

---

### **4. Word2Vec 학습 과정**
1. **단어쌍 생성 (Context-Word Pairs 생성)**  
   - 주어진 문장에서 중심 단어와 문맥 단어를 쌍으로 만듦.
   - 예제 문장: `"I like to eat apples"`  
     - CBOW 방식:  
       - ('I', 'like')  
       - ('like', 'to')  
       - ('to', 'eat')  
       - ('eat', 'apples')  
     - Skip-gram 방식:  
       - ('like' → 'I')  
       - ('like' → 'to')  
       - ('to' → 'like')  
       - ('to' → 'eat')

2. **모델 학습 (Skip-gram 예시)**  
   - 입력 단어(중심 단어)를 받아서 **주변 단어를 예측**하도록 모델을 학습함.
   - 예제:  
     - 입력: `wt = "like"`  
     - 예측: `wt+j = ["I", "to"]`

3. **손실 함수 (Loss Function) 사용**  
   - 목표: 단어 벡터를 학습하여 **주어진 단어 쌍의 확률을 최대화**하도록 함.  
   - Word2Vec에서는 **Negative Sampling**을 사용하여 학습을 효율적으로 수행함.

---

### **5. Word2Vec 예제**
Word2Vec을 학습하면, 의미적으로 유사한 단어가 벡터 공간에서 가깝게 배치됨.  
또한, 벡터 연산이 가능하여 단어 간의 관계를 나타낼 수 있음.

#### **예제 3: 단어 벡터 연산**
훈련된 Word2Vec 모델에서:

```python
king - man + woman ≈ queen
```

- "king"에서 "man"을 빼고 "woman"을 더하면 **"queen"과 유사한 벡터 값이 나옴**.
- 즉, 단어 간의 관계를 벡터 연산으로 표현할 수 있음.

#### **예제 4: 가장 유사한 단어 찾기**
```python
model.most_similar("king")
```
출력:
```
[("queen", 0.92), ("prince", 0.85), ("monarch", 0.82)]
```
- "king"과 가장 유사한 단어가 "queen", "prince", "monarch" 등으로 나타남.

---

### **6. 결론**
- 원핫 인코딩의 문제점(고차원, 희소성, 의미 없음)을 해결하기 위해 **단어 임베딩** 사용.
- **Word2Vec**은 단어의 의미적 유사성을 반영하는 벡터를 학습함.
- **CBOW**: 주변 단어로 중심 단어를 예측  
- **Skip-gram**: 중심 단어로 주변 단어를 예측  
- Word2Vec 학습 후에는 **단어 간의 유사성, 관계**를 벡터 연산으로 표현할 수 있음.

이처럼 Word2Vec은 단순한 단어 표현이 아니라 **단어 간의 관계와 문맥을 반영하는 강력한 임베딩 기법**입니다. 🚀

RNN
자연어 NLP
1. 텍스트를 숫자로 변경(토큰화 임베딩)
토큰화 : 문장을 단어단위로 나눈다. (여기서는 임시로 공백을 기준으로 분리)
나는 밥을 먹는다 -> ['나는', '밥을', '먹는다']
단어에 숫자 부여
'나는' = 1
'밥을' = 2
'먹는다' = 3
임베딩(의미를 담는 벡터) 길이를 3
'나는' = [..]
'밥을' = [..]
'먹는다' = [..]
RNN으로 순서대로 처리
입력값 : 각 단어의 임베딩 벡터
hidden : 이전 단어의 정보를 담는 메모리
출력값 : 다음단어 예측
계산과정

### **RNN(Recurrent Neural Network) - 자연어 처리(NLP)에서의 활용**  

RNN은 시퀀스 데이터를 다룰 때 강력한 성능을 발휘하는 모델이며, 특히 **자연어 처리(NLP)**에서 많이 사용됨.  
- **텍스트 데이터를 숫자로 변환**하고,  
- **순차적으로 단어를 입력**받아  
- **이전 정보를 기억**하면서  
- **다음 단어를 예측**하는 방식으로 작동함.  

---

## **📌 1. 텍스트를 숫자로 변환 (토큰화 & 임베딩)**  
자연어 데이터를 RNN에 입력하려면 먼저 **텍스트를 숫자로 변환**해야 함.  
이 과정은 크게 **토큰화 → 정수 인코딩 → 임베딩**으로 나뉨.  

### **🔹 1-1. 토큰화 (Tokenization)**
문장을 단어 단위로 분리.  
예제 문장:  
> **"나는 밥을 먹는다"**  

공백을 기준으로 토큰화하면:  
```python
['나는', '밥을', '먹는다']
```

---

### **🔹 1-2. 정수 인코딩 (Integer Encoding)**
각 단어에 숫자를 부여.  
```python
'나는' = 1
'밥을' = 2
'먹는다' = 3
```
변환된 문장:  
```python
[1, 2, 3]
```

---

### **🔹 1-3. 임베딩 (Embedding)**
정수 인코딩된 단어를 저차원 벡터로 변환해 의미를 반영.  
(실제 임베딩 차원은 보통 50~300차원, 여기서는 3차원으로 설정)  

```python
'나는'  = [0.1, 0.5, 0.7]  
'밥을'  = [0.2, 0.4, 0.8]  
'먹는다' = [0.3, 0.6, 0.9]  
```
따라서 변환된 문장은  
```python
[[0.1, 0.5, 0.7],  
 [0.2, 0.4, 0.8],  
 [0.3, 0.6, 0.9]]
```

---

## **📌 2. RNN으로 순차적 처리**
RNN은 **단어를 순서대로 입력받고, 과거 정보를 기억하면서 학습**함.  

### **🔹 2-1. 입력값**  
각 단어의 임베딩 벡터를 순서대로 RNN에 입력.  
```python
X₁ = [0.1, 0.5, 0.7]  # '나는'  
X₂ = [0.2, 0.4, 0.8]  # '밥을'  
X₃ = [0.3, 0.6, 0.9]  # '먹는다'  
```

---

### **🔹 2-2. RNN 계산 과정**
각 입력값이 **순차적으로 들어오면서 Hidden State(이전 정보를 담는 메모리)가 업데이트**됨.  

1️⃣ 첫 번째 단어 ('나는') 입력  
\[
h_1 = tanh(W_h h_0 + W_x X_1 + b)
\]
(초기 상태인 \( h_0 \)는 0으로 가정)  

2️⃣ 두 번째 단어 ('밥을') 입력  
\[
h_2 = tanh(W_h h_1 + W_x X_2 + b)
\]

3️⃣ 세 번째 단어 ('먹는다') 입력  
\[
h_3 = tanh(W_h h_2 + W_x X_3 + b)
\]

- \( W_h \), \( W_x \), \( b \) 는 학습되는 가중치 행렬  
- **Hidden State (\( h_t \))** 는 단어가 입력될 때마다 업데이트됨  

---

### **🔹 2-3. 출력값 (다음 단어 예측)**
각 타임스텝마다 RNN은 **다음 단어를 예측**함.  

예를 들어, 입력이 **'나는 밥을'**일 때 **다음 단어('먹는다')를 예측**.  
\[
y_3 = softmax(W_y h_3)
\]

softmax를 적용하면 확률 값이 나옴.  
예:
```python
'먹는다' -> 80% 확률  
'마신다' -> 15% 확률  
'준비한다' -> 5% 확률  
```
이렇게 하면 RNN이 문장을 읽고 다음 단어를 예측할 수 있음.

---

## **📌 3. 예제 코드 (PyTorch 활용)**  
```python
import torch
import torch.nn as nn

# 데이터셋 (간단한 단어 임베딩)
word_dict = {'나는': 0, '밥을': 1, '먹는다': 2}
embedding_dim = 3
vocab_size = len(word_dict)

# 샘플 문장: "나는 밥을 먹는다"
sentence = torch.tensor([[0, 1, 2]], dtype=torch.long)  # 정수 인코딩된 문장

# 1️⃣ 임베딩 레이어
embedding = nn.Embedding(vocab_size, embedding_dim)
embedded_sentence = embedding(sentence)  # (1, 3, 3) 형태

# 2️⃣ RNN 모델
rnn = nn.RNN(input_size=embedding_dim, hidden_size=5, batch_first=True)

# 초기 Hidden State (0으로 초기화)
h_0 = torch.zeros(1, 1, 5)

# RNN 계산
output, h_n = rnn(embedded_sentence, h_0)

# 출력 확인
print(output)
```

---

## **📌 4. 정리**
✅ **텍스트 데이터 전처리 과정**  
① **토큰화** → ② **정수 인코딩** → ③ **임베딩 변환**  

✅ **RNN 동작 방식**  
1. **단어 임베딩을 입력으로 받음**  
2. **Hidden State를 업데이트하며 문맥 정보를 기억**  
3. **출력을 통해 다음 단어를 예측**  

✅ **예제 코드**
- PyTorch를 사용해 **간단한 임베딩 & RNN 연산 과정**을 구현함.  

이제 RNN을 활용해 **번역, 감성 분석, 문장 생성 등 다양한 NLP 작업**을 수행 가능! 🚀

데이터 증강
EDA(Easy Data Augumentation)
KoEDA
  동어의치환, 단어삭제, 단어순서변경,랜덤단어삽입
  형태소 Okt  

토근화
  BPE : 연속된 단어쌍을 병합해서 사전을 생성
  WordPiece : BERT라는 언어모델에서 사용, 확률 기반
  SentencePiece : 공백을 무시하고 학습

Word Embedding 기법
Word2Vec  : 단어의 분포를 고려한 벡터
FastText : 서브워드를 학습 희귀단어도 처리가능
BERT Embedding : 문맥을 고려한 동적 벡터
ELECTRA : GAN 방식으로 작은 모델에서도 성능 향상

전처리기법
이모티콘이나 특수문자제거
정규화 : 'ㅋㅋㅋ' 재미있다 'ㅠ.ㅠ' 지루하다 재미없다
불용어제거 : 의미없는 단어

최종정리
RNN
 데이터 증강
 최신 토크나이져 적용
 벡터방식
 정제(불용어, 특수문자, 이모티콘등등)
 정규화 : 필요시.. 줄임말을 표준어 표현

 ### **📌 자연어처리(NLP)에서 데이터 전처리 및 데이터 증강 개념 정리**  

---

## **1️⃣ 데이터 증강(Data Augmentation)**
- **자연어 데이터가 부족하거나 모델의 일반화 성능을 향상시키기 위해 사용**  
- **EDA(Easy Data Augmentation)**  
  - 기존 문장에서 데이터를 변형하여 새로운 학습 데이터를 생성  
  - **KoEDA**: 한국어 데이터 증강 기법으로 **Okt 형태소 분석기 활용**  
  - 주요 기법  
    - **동의어 치환(Synonym Replacement)**: 유사한 의미의 단어로 변경  
      - 예시: "나는 밥을 먹었다" → "나는 식사를 했다"  
    - **단어 삭제(Random Deletion)**: 문장에서 불필요한 단어 삭제  
      - 예시: "오늘 날씨가 매우 맑다" → "오늘 날씨 맑다"  
    - **단어 순서 변경(Random Swap)**: 문장 내 단어들의 위치 변경  
      - 예시: "나는 학교에 간다" → "학교에 나는 간다"  
    - **랜덤 단어 삽입(Random Insertion)**: 문장 내 의미 있는 단어 추가  
      - 예시: "오늘 기분이 좋다" → "오늘 정말 기분이 좋다"  

---

## **2️⃣ 토큰화(Tokenization)**
- **텍스트 데이터를 숫자로 변환하는 과정**  
- 주요 기법  
  - **BPE(Byte Pair Encoding)**  
    - 연속된 문자 쌍을 병합하여 단어 사전 구축  
    - 희귀 단어 처리에 강점  
  - **WordPiece**  
    - BERT 모델에서 사용  
    - 확률 기반으로 단어를 분할하여 의미 유지  
  - **SentencePiece**  
    - 공백을 포함하지 않고 단어를 분할  
    - 문장 전체를 단위로 학습하여 처리  

---

## **3️⃣ 단어 임베딩(Word Embedding) 기법**
- **단어를 벡터(숫자로 표현된 다차원 공간)로 변환하는 방법**  

| 기법 | 특징 | 예제 |
|------|------|------|
| **Word2Vec** | 단어의 분포를 고려하여 벡터 표현 | king - man + woman ≈ queen |
| **FastText** | 서브워드를 학습하여 희귀 단어도 처리 가능 | '인공지능' → ['인공', '공지', '지능'] |
| **BERT Embedding** | 문맥을 고려한 동적 벡터 | 'Apple' (과일/회사) 의미 구별 |
| **ELECTRA** | GAN 방식으로 작은 모델에서도 성능 향상 | 적은 데이터로도 성능 개선 가능 |

---

## **4️⃣ 전처리 기법(Preprocessing Techniques)**
- **텍스트 데이터 정제 및 가공 과정**  
- 주요 기법  
  - **이모티콘 및 특수문자 제거**  
    - "오늘 기분이 최고👍" → "오늘 기분이 최고"  
  - **정규화(Normalization)**  
    - "ㅋㅋㅋ" → "재미있다", "ㅠㅠ" → "슬프다"  
  - **불용어 제거(Stopword Removal)**  
    - 의미 없는 단어(조사, 접속사) 제거  
    - 예시: "나는 학교에 간다" → "학교 간다"  

---

## **5️⃣ 최종 요약 및 학습 흐름**
1️⃣ **데이터 증강(EDA, KoEDA 적용)**  
2️⃣ **최신 토큰화 적용(BPE, WordPiece, SentencePiece 등)**  
3️⃣ **벡터 변환(Word2Vec, FastText, BERT 등)**  
4️⃣ **데이터 정제(이모티콘, 특수문자, 불용어 제거 등)**  
5️⃣ **정규화(필요 시 줄임말 변환, 감성 표현 보정 등)**  

✅ **예제 코드 (KoEDA 적용 예시)**  

```python
from koeda import AEDA
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
aeda = AEDA(tokenizer, punc_ratio=0.3)

text = "오늘 날씨가 정말 좋아요!"
augmented_text = aeda(text)

print(augmented_text)
```

✅ **예제 코드 (Word2Vec 학습 예시)**  

```python
from gensim.models import Word2Vec

sentences = [["나는", "밥을", "먹는다"], ["그녀는", "학교에", "간다"]]
model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, workers=4)

print(model.wv["밥을"])  # "밥을" 단어의 벡터 값 출력
```

📌 **이러한 과정을 통해 NLP 모델을 학습하고, 데이터 품질을 높일 수 있음!**




