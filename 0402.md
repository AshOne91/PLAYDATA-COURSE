### **순차 데이터 (Sequential Data)**  
- 시간 또는 순서에 따라 변화하는 데이터를 의미함.  
- 예시: 주가 데이터, 자연어 문장, 음성 신호, 센서 데이터, 비디오 프레임 등.  

---

### **RNN (순환 신경망, Recurrent Neural Network)**  
- 순차 데이터를 처리하기 위한 신경망 구조.  
- 이전 단계의 정보를 기억하고, 이를 활용하여 다음 단계의 출력을 계산함.  
- **특징**: 은닉 상태(\( h_t \))를 사용하여 과거 정보를 저장하고, 이를 활용하여 다음 출력을 생성함.  

---

### **RNN의 입력**  
- RNN은 **순차 데이터의 각 시점별(feature별) 입력**을 받음.  
- 입력 형태:  
  \[
  X = [x_1, x_2, x_3, ..., x_T]
  \]
  - 여기서 \( x_t \) 는 각 시점 \( t \)에서의 입력 데이터  
  - \( T \)는 전체 시퀀스 길이  

- **입력 예시** (자연어 처리 - "bat" 예제):  
  - "bat"를 글자 단위 입력으로 나눠서 처리하면,  
    \[
    X = [\text{"b"}, \text{"a"}, \text{"t"}]
    \]
  - 각 글자는 임베딩 벡터로 변환됨.  
    \[
    X = [\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3]
    \]
  - \( \mathbf{x}_1 = \) "b"의 임베딩 벡터  
  - \( \mathbf{x}_2 = \) "a"의 임베딩 벡터  
  - \( \mathbf{x}_3 = \) "t"의 임베딩 벡터  

---

추가로 설명이 필요하면 요청 바람.

IMDB 데이터셋
케라스로 IMDB 데이터 불러오기
수치화된 문장
input은 단어
target은 긍정, 부정

훈련 세트 준비
히스토그램으로 그리기(한번 보고 적당히 해야지)

시퀀스 패딩(문장의 길이를 맞춰주는것 // 모양을 맞춰야징)
(앞에 채우냐 뒤에 채우냐 옵션이 있음)
짤라지던가 채우던가

순환 신경망 모델 만들기

원핫인코딩?
원핫으로 바뀌면 좀 길어짐

모델구조 확인해보고

모델 훈련 (adm)

단어 임베딩?

IMDB 데이터셋을 이용한 순환 신경망 모델을 구축하는 과정을 예제와 함께 상세히 설명하겠음.  

---

## **1. IMDB 데이터셋 불러오기**  
IMDB 데이터셋은 영화 리뷰에 대한 감성 분석 데이터셋으로, 리뷰 문장을 **수치화된 형태(정수 시퀀스)**로 제공함.  

### **IMDB 데이터셋 불러오기**  
케라스를 이용하여 IMDB 데이터셋을 불러오면, 리뷰 문장이 **단어의 정수 인덱스 시퀀스**로 변환된 형태로 제공됨.  

```python
from tensorflow.keras.datasets import imdb

# IMDB 데이터셋 로드 (상위 10,000개 단어만 사용)
num_words = 10000  # 가장 많이 등장한 10,000개 단어만 선택
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)

print(f"훈련 데이터 개수: {len(x_train)}, 테스트 데이터 개수: {len(x_test)}")
print("첫 번째 리뷰의 정수 인코딩:", x_train[0])  # 정수 시퀀스로 변환된 문장
print("첫 번째 리뷰의 라벨 (0: 부정, 1: 긍정):", y_train[0])
```
🔹 `num_words=10000`: 가장 많이 등장하는 단어 10,000개만 사용  
🔹 `x_train[i]`: i번째 리뷰의 **정수 인코딩된 문장**  
🔹 `y_train[i]`: 해당 리뷰가 **긍정(1)인지 부정(0)인지** 나타내는 정답 라벨  

---

## **2. 데이터 시각화 - 리뷰 길이 분포 확인**  
리뷰마다 길이가 다 다름. 이를 확인하기 위해 히스토그램을 그림.  

```python
import matplotlib.pyplot as plt
import numpy as np

review_lengths = [len(review) for review in x_train]  # 리뷰 길이 리스트

plt.hist(review_lengths, bins=50, color='blue', alpha=0.7)
plt.xlabel("리뷰 길이")
plt.ylabel("리뷰 개수")
plt.title("IMDB 리뷰 길이 분포")
plt.show()
```
🔹 `plt.hist(review_lengths, bins=50)`: 리뷰 길이를 50개의 구간으로 나누어 히스토그램을 그림.  
🔹 대부분의 리뷰 길이가 특정 범위(약 100~500 단어) 안에 있음.  

---

## **3. 시퀀스 패딩 (Sequence Padding)**
모델에 입력할 때 **모든 문장의 길이를 맞춰야 함**. 이를 위해 **짧은 문장은 0을 추가하고, 긴 문장은 자름**.  

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_len = 200  # 모든 문장의 길이를 200으로 통일
x_train_pad = pad_sequences(x_train, maxlen=max_len, padding='post', truncating='post')
x_test_pad = pad_sequences(x_test, maxlen=max_len, padding='post', truncating='post')

print("패딩된 첫 번째 리뷰:", x_train_pad[0])
print("패딩된 리뷰 길이:", len(x_train_pad[0]))  # 항상 max_len(200)이어야 함
```
🔹 `maxlen=200`: 모든 리뷰 길이를 **200개 단어로 맞춤**  
🔹 `padding='post'`: **뒤쪽에 0을 채움** (`pre`로 하면 앞쪽에 0을 채움)  
🔹 `truncating='post'`: **긴 문장은 뒤를 자름** (`pre`로 하면 앞쪽을 자름)  

---

## **4. 순환 신경망 (RNN) 모델 구축**
LSTM을 이용하여 순환 신경망 모델을 구축함.  

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# 모델 정의
model = Sequential([
    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),  # 단어 임베딩
    LSTM(64, return_sequences=False),  # LSTM 층 (64개의 LSTM 유닛)
    Dense(1, activation='sigmoid')  # 출력층 (이진 분류)
])

# 모델 컴파일
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 모델 구조 확인
model.summary()
```
🔹 `Embedding(input_dim=10000, output_dim=128)`: 단어 임베딩 벡터 크기를 128로 설정  
🔹 `LSTM(64)`: LSTM 층을 사용하여 64개의 유닛을 활용  
🔹 `Dense(1, activation='sigmoid')`: 감성 분류 (긍정/부정) → **이진 분류이므로 sigmoid 사용**  

---

## **5. 원-핫 인코딩 vs 단어 임베딩**  
**원-핫 인코딩(One-hot encoding)**은 단어를 벡터로 변환하는 방식 중 하나임.  
- 단점: 단어가 많아지면 **차원이 매우 커짐**.  
- 예시: 단어 사전 크기가 10,000이면, 각 단어는 **10,000차원 벡터**가 됨.  

```python
from tensorflow.keras.utils import to_categorical

# 원-핫 인코딩 예제
word_index = 3  # 예제 단어 인덱스
one_hot_vector = to_categorical(word_index, num_classes=10000)

print("단어의 원-핫 벡터:", one_hot_vector)
```
🔹 단어 인덱스 `word_index=3` → 원-핫 벡터는 `[0, 0, 0, 1, 0, 0, ..., 0]` 형태  

💡 **단어 임베딩(Embedding)**을 사용하면, 원-핫 인코딩보다 효율적이며 단어 간 의미를 학습할 수 있음.  

---

## **6. 모델 훈련**
```python
# 모델 훈련
history = model.fit(x_train_pad, y_train, epochs=5, batch_size=64, validation_data=(x_test_pad, y_test))
```
🔹 `epochs=5`: 5번 반복 학습  
🔹 `batch_size=64`: 한 번에 64개의 데이터를 학습  
🔹 `validation_data`: 테스트 데이터를 이용해 성능 확인  

---

## **7. 모델 평가 및 예측**
```python
# 모델 평가
loss, acc = model.evaluate(x_test_pad, y_test)
print(f"테스트 정확도: {acc:.4f}")

# 예측 예제
sample_review = x_test_pad[0].reshape(1, -1)  # 샘플 리뷰
prediction = model.predict(sample_review)

print(f"예측 확률: {prediction[0][0]:.4f}")  # 0에 가까우면 부정, 1에 가까우면 긍정
```
🔹 `model.evaluate()`: 테스트 데이터로 정확도를 계산  
🔹 `model.predict()`: 예측 수행 (0이면 부정, 1이면 긍정)  

---

### **✅ 정리**
1️⃣ **IMDB 데이터셋 불러오기** → 단어를 **정수 인덱스로 변환된 시퀀스** 형태  
2️⃣ **리뷰 길이 시각화** → 히스토그램으로 문장 길이 확인  
3️⃣ **시퀀스 패딩** → 모든 리뷰의 길이를 맞춤  
4️⃣ **순환 신경망(RNN) 모델 구축** → `Embedding + LSTM + Dense`  
5️⃣ **원-핫 인코딩 vs 단어 임베딩** → 임베딩이 더 효율적  
6️⃣ **모델 훈련 및 평가** → `adam` 옵티마이저 사용하여 학습  
7️⃣ **예측 테스트** → 감성 분석 결과 확인  

추가 질문 있으면 요청 바람.




