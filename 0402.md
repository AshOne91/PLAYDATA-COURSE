### **순차 데이터 (Sequential Data)**  
- 시간 또는 순서에 따라 변화하는 데이터를 의미함.  
- 예시: 주가 데이터, 자연어 문장, 음성 신호, 센서 데이터, 비디오 프레임 등.  

---

### **RNN (순환 신경망, Recurrent Neural Network)**  
- 순차 데이터를 처리하기 위한 신경망 구조.  
- 이전 단계의 정보를 기억하고, 이를 활용하여 다음 단계의 출력을 계산함.  
- **특징**: 은닉 상태(\( h_t \))를 사용하여 과거 정보를 저장하고, 이를 활용하여 다음 출력을 생성함.  

---

### **RNN의 입력**  
- RNN은 **순차 데이터의 각 시점별(feature별) 입력**을 받음.  
- 입력 형태:  
  \[
  X = [x_1, x_2, x_3, ..., x_T]
  \]
  - 여기서 \( x_t \) 는 각 시점 \( t \)에서의 입력 데이터  
  - \( T \)는 전체 시퀀스 길이  

- **입력 예시** (자연어 처리 - "bat" 예제):  
  - "bat"를 글자 단위 입력으로 나눠서 처리하면,  
    \[
    X = [\text{"b"}, \text{"a"}, \text{"t"}]
    \]
  - 각 글자는 임베딩 벡터로 변환됨.  
    \[
    X = [\mathbf{x}_1, \mathbf{x}_2, \mathbf{x}_3]
    \]
  - \( \mathbf{x}_1 = \) "b"의 임베딩 벡터  
  - \( \mathbf{x}_2 = \) "a"의 임베딩 벡터  
  - \( \mathbf{x}_3 = \) "t"의 임베딩 벡터  

---

추가로 설명이 필요하면 요청 바람.

IMDB 데이터셋
케라스로 IMDB 데이터 불러오기
수치화된 문장
input은 단어
target은 긍정, 부정

훈련 세트 준비
히스토그램으로 그리기(한번 보고 적당히 해야지)

시퀀스 패딩(문장의 길이를 맞춰주는것 // 모양을 맞춰야징)
(앞에 채우냐 뒤에 채우냐 옵션이 있음)
짤라지던가 채우던가

순환 신경망 모델 만들기

원핫인코딩?
원핫으로 바뀌면 좀 길어짐

모델구조 확인해보고

모델 훈련 (adm)

단어 임베딩?

IMDB 데이터셋을 이용한 순환 신경망 모델을 구축하는 과정을 예제와 함께 상세히 설명하겠음.  

---

## **1. IMDB 데이터셋 불러오기**  
IMDB 데이터셋은 영화 리뷰에 대한 감성 분석 데이터셋으로, 리뷰 문장을 **수치화된 형태(정수 시퀀스)**로 제공함.  

### **IMDB 데이터셋 불러오기**  
케라스를 이용하여 IMDB 데이터셋을 불러오면, 리뷰 문장이 **단어의 정수 인덱스 시퀀스**로 변환된 형태로 제공됨.  

```python
from tensorflow.keras.datasets import imdb

# IMDB 데이터셋 로드 (상위 10,000개 단어만 사용)
num_words = 10000  # 가장 많이 등장한 10,000개 단어만 선택
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)

print(f"훈련 데이터 개수: {len(x_train)}, 테스트 데이터 개수: {len(x_test)}")
print("첫 번째 리뷰의 정수 인코딩:", x_train[0])  # 정수 시퀀스로 변환된 문장
print("첫 번째 리뷰의 라벨 (0: 부정, 1: 긍정):", y_train[0])
```
🔹 `num_words=10000`: 가장 많이 등장하는 단어 10,000개만 사용  
🔹 `x_train[i]`: i번째 리뷰의 **정수 인코딩된 문장**  
🔹 `y_train[i]`: 해당 리뷰가 **긍정(1)인지 부정(0)인지** 나타내는 정답 라벨  

---

## **2. 데이터 시각화 - 리뷰 길이 분포 확인**  
리뷰마다 길이가 다 다름. 이를 확인하기 위해 히스토그램을 그림.  

```python
import matplotlib.pyplot as plt
import numpy as np

review_lengths = [len(review) for review in x_train]  # 리뷰 길이 리스트

plt.hist(review_lengths, bins=50, color='blue', alpha=0.7)
plt.xlabel("리뷰 길이")
plt.ylabel("리뷰 개수")
plt.title("IMDB 리뷰 길이 분포")
plt.show()
```
🔹 `plt.hist(review_lengths, bins=50)`: 리뷰 길이를 50개의 구간으로 나누어 히스토그램을 그림.  
🔹 대부분의 리뷰 길이가 특정 범위(약 100~500 단어) 안에 있음.  

---

## **3. 시퀀스 패딩 (Sequence Padding)**
모델에 입력할 때 **모든 문장의 길이를 맞춰야 함**. 이를 위해 **짧은 문장은 0을 추가하고, 긴 문장은 자름**.  

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_len = 200  # 모든 문장의 길이를 200으로 통일
x_train_pad = pad_sequences(x_train, maxlen=max_len, padding='post', truncating='post')
x_test_pad = pad_sequences(x_test, maxlen=max_len, padding='post', truncating='post')

print("패딩된 첫 번째 리뷰:", x_train_pad[0])
print("패딩된 리뷰 길이:", len(x_train_pad[0]))  # 항상 max_len(200)이어야 함
```
🔹 `maxlen=200`: 모든 리뷰 길이를 **200개 단어로 맞춤**  
🔹 `padding='post'`: **뒤쪽에 0을 채움** (`pre`로 하면 앞쪽에 0을 채움)  
🔹 `truncating='post'`: **긴 문장은 뒤를 자름** (`pre`로 하면 앞쪽을 자름)  

---

## **4. 순환 신경망 (RNN) 모델 구축**
LSTM을 이용하여 순환 신경망 모델을 구축함.  

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# 모델 정의
model = Sequential([
    Embedding(input_dim=num_words, output_dim=128, input_length=max_len),  # 단어 임베딩
    LSTM(64, return_sequences=False),  # LSTM 층 (64개의 LSTM 유닛)
    Dense(1, activation='sigmoid')  # 출력층 (이진 분류)
])

# 모델 컴파일
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 모델 구조 확인
model.summary()
```
🔹 `Embedding(input_dim=10000, output_dim=128)`: 단어 임베딩 벡터 크기를 128로 설정  
🔹 `LSTM(64)`: LSTM 층을 사용하여 64개의 유닛을 활용  
🔹 `Dense(1, activation='sigmoid')`: 감성 분류 (긍정/부정) → **이진 분류이므로 sigmoid 사용**  

---

## **5. 원-핫 인코딩 vs 단어 임베딩**  
**원-핫 인코딩(One-hot encoding)**은 단어를 벡터로 변환하는 방식 중 하나임.  
- 단점: 단어가 많아지면 **차원이 매우 커짐**.  
- 예시: 단어 사전 크기가 10,000이면, 각 단어는 **10,000차원 벡터**가 됨.  

```python
from tensorflow.keras.utils import to_categorical

# 원-핫 인코딩 예제
word_index = 3  # 예제 단어 인덱스
one_hot_vector = to_categorical(word_index, num_classes=10000)

print("단어의 원-핫 벡터:", one_hot_vector)
```
🔹 단어 인덱스 `word_index=3` → 원-핫 벡터는 `[0, 0, 0, 1, 0, 0, ..., 0]` 형태  

💡 **단어 임베딩(Embedding)**을 사용하면, 원-핫 인코딩보다 효율적이며 단어 간 의미를 학습할 수 있음.  

---

## **6. 모델 훈련**
```python
# 모델 훈련
history = model.fit(x_train_pad, y_train, epochs=5, batch_size=64, validation_data=(x_test_pad, y_test))
```
🔹 `epochs=5`: 5번 반복 학습  
🔹 `batch_size=64`: 한 번에 64개의 데이터를 학습  
🔹 `validation_data`: 테스트 데이터를 이용해 성능 확인  

---

## **7. 모델 평가 및 예측**
```python
# 모델 평가
loss, acc = model.evaluate(x_test_pad, y_test)
print(f"테스트 정확도: {acc:.4f}")

# 예측 예제
sample_review = x_test_pad[0].reshape(1, -1)  # 샘플 리뷰
prediction = model.predict(sample_review)

print(f"예측 확률: {prediction[0][0]:.4f}")  # 0에 가까우면 부정, 1에 가까우면 긍정
```
🔹 `model.evaluate()`: 테스트 데이터로 정확도를 계산  
🔹 `model.predict()`: 예측 수행 (0이면 부정, 1이면 긍정)  

---

### **✅ 정리**
1️⃣ **IMDB 데이터셋 불러오기** → 단어를 **정수 인덱스로 변환된 시퀀스** 형태  
2️⃣ **리뷰 길이 시각화** → 히스토그램으로 문장 길이 확인  
3️⃣ **시퀀스 패딩** → 모든 리뷰의 길이를 맞춤  
4️⃣ **순환 신경망(RNN) 모델 구축** → `Embedding + LSTM + Dense`  
5️⃣ **원-핫 인코딩 vs 단어 임베딩** → 임베딩이 더 효율적  
6️⃣ **모델 훈련 및 평가** → `adam` 옵티마이저 사용하여 학습  
7️⃣ **예측 테스트** → 감성 분석 결과 확인  

추가 질문 있으면 요청 바람.


단어 임베딩
 전통적으로 단어는 원핫표현
 단점 : 단어의 유사성을 파악할 수 없음 cat dog는 의미상 유사하지만 벡터간의 거리는 동일하거나 등..
 차원 문제 : 어휘가 크면 벡터차원이 증가 (희소행렬문제)
 해결 : 단어임베딩은 단어를 저차원(30~500차원) 실수 벡터로 표현해서 의미적으로 유사한 단어는 벡터상에서 가까운 위치에 배치
 ㅊ� `Embedding(input_dim=10000, output_dim=128)`: 단어 임베딩 벡터 크기를 128로 설정  
🔹 `LSTM(64)`: LSTM 층을 사용하여 64개의 유닛을 활용  
🔹 `Dense(1, activation='sigmoid')`: 감성 분류 (긍정/부정) → **이진 분류이므로 sigmoid 사용**  

---

## **5. 원-핫 인코딩 vs 단어 임베딩**  
**원-핫 인코딩(One-hot encoding)**은 단어를 벡터로 변환하는 방식 중 하나임.  
- 단점: 단어가 많아지면 **차원이 매우 커짐**.  
- 예시: 단어 사전 크기가 10,000이면, 각 단어는 **10,000차원 벡터**가 됨.  

```python
from tensorflow.keras.utils import to_categorical

# 원-핫 인코딩 예제
word_index = 3  # 예제 단어 인덱스
one_hot_vector = to_categorical(word_index, num_classes=10000)

print("단어의 원-핫 벡터:", one_hot_vector)
```
🔹 단어 인덱스 `word_index=3` → 원-핫 벡터는 `[0, 0, 0, 1, 0, 0, ..., 0]` 형태  

💡 **단어 임베딩(Embedding)**을 사용하면, 원-핫 인코딩보다 효율적이며 단어 간 의미를 학습할 수 있음.  

---

## **6. 모델 훈련**
```python
# 모델 훈련
history = model.fit(x_train_pad, y_train, epochs=5, batch_size=64, validation_data=(x_test_pad, y_test))
```
🔹 `epochs=5`: 5번 반복 학습  
🔹 `batch_size=64`: 한 번에 64개의 데이터를 학습  
🔹 `validation_data`: 테스트 데이터를 이용해 성능 확인  

---

## **7. 모델 평가 및 예측**
```python
# 모델 평가
loss, acc = model.evaluate(x_test_pad, y_test)
print(f"테스트 정확도: {acc:.4f}")

# 예측 예제
sample_review = x_test_pad[0].reshape(1, -1)  # 샘플 리뷰
prediction = model.predict(sample_review)

print(f"예측 확률: {prediction[0][0]:.4f}")  # 0에 가까우면 부정, 1에 가까우면 긍정
```
🔹 `model.evaluate()`: 테스트 데이터로 정확도를 계산  
🔹 `model.predict()`: 예측 수행 (0이면 부정, 1이면 긍정)  

---

### **✅ 정리**
1️⃣ **IMDB 데이터셋 불러오기** → 단어를 **정수 인덱스로 변환된 시퀀스** 형태  
2️⃣ **리뷰 길이 시각화** → 히스토그램으로 문장 길이 확인  
3️⃣ **시퀀스 패딩** → 모든 리뷰의 길이를 맞춤  
4️⃣ **순환 신경망(RNN) 모델 구축** → `Embedding + LSTM + Dense`  
5️⃣ **원-핫 인코딩 vs 단어 임베딩** → 임베딩이 더 효율적  
6️⃣ **모델 훈련 및 평가** → `adam` 옵티마이저 사용하여 학습  
7️⃣ **예측 테스트** → 감성 분석 결과 확인  

추가 질문 있으면 요청 바람.


단어 임베딩
 전통적으로 단어는 원핫표현
 단점 : 단어의 유사성을 파악할 수 없음 cat dog는 의미상 유사하지만 벡터간의 거리는 동일하거나 등..
 차원 문제 : 어휘가 크면 벡터차원이 증가 (희소행렬문제)
 해결 : 단어임베딩은 단어를 저차원(30~500차원) 실수 벡터로 표현해서 의미적으로 유사한 단어는 벡터상에서 가까운 위치에 배치
 의미적 유사성 : king, queen 이런 데이터는 벡터공간에서 가깝다
 king-man ~ queen-woman
 원핫보다는 훨씬 적은 저차원

Word2Vec
 Goole 만듦
 CBOW(Continuous Bag of Words) : 주변단어로 중심단어를 예측
 Skip-gram : 중심단어로 주변단어를 예측

손실함수

학습과정
  단어쌍 생성 : 문장에서 중심단어, 문맥단어쌍을 생성
  i like to eat apples ('like', 'i') ('like', 'to')
  모델 학습 skip-gram wt를 입력으로 받아서 wt+j를 예측하도록 학습rediction = model.predict(sample_review)

print(f"예측 확률: {prediction[0][0]:.4f}")  # 0에 가까우면 부정, 1에 가까우면 긍정
```
🔹 `model.evaluate()`: 테스트 데이터로 정확도를 계산  
🔹 `model.predict()`: 예측 수행 (0이면 부정, 1이면 긍정)  

```

### **✅ 정리**
1️⃣ **IMDB 데이터셋 불러오기** → 단어를 **정수 인덱스로 변환된 시퀀스** 형태  
2️⃣ **리뷰 길이 시각화** → 히스토그램으로 문장 길이 확인  
3️⃣ **시퀀스 패딩** → 모든 리뷰의 길이를 맞춤  
4️⃣ **순환 신경망(RNN) 모델 구축** → `Embedding + LSTM + Dense`  
5️⃣ **원-핫 인코딩 vs 단어 임베딩** → 임베딩이 더 효율적  
6️⃣ **모델 훈련 및 평가** → `adam` 옵티마이저 사용하여 학습  
7️⃣ **예측 테스트** → 감성 분석 결과 확인  

추가 질문 있으면 요청 바람.


단어 임베딩
 전통적으로 단어는 원핫표현
 단점 : 단어의 유사성을 파악할 수 없음 cat dog는 의미상 유사하지만 벡터간의 거리는 동일하거나 등..
 차원 문제 : 어휘가 크면 벡터차원이 증가 (희소행렬문제)
 해결 : 단어임베딩은 단어를 저차원(30~500차원) 실수 벡터로 표현해서 의미적으로 유사한 단어는 벡터상에서 가까운 위치에 배치
 의미적 유사성 : king, queen 이런 데이터는 벡터공간에서 가깝다
 king-man ~ queen-woman
 원핫보다는 훨씬 적은 저차원

Word2Vec
 Goole 만듦
 CBOW(Continuous Bag of Words) : 주변단어로 중심단어를 예측
 Skip-gram : 중심단어로 주변단어를 예측

손실함수

학습과정
  단어쌍 생성 : 문장에서 중심단어, 문맥단어쌍을 생성
  i like to eat apples ('like', 'i') ('like', 'to')
  모델 학습 skip-gram wt를 입력으로 받아서 wt+j를 예측하도록 학습

Word2Vec
 king queen은 문맥에서 자주 유사한 위치에 등장(love 주제) 학습후
  king = [0.8, 0.2, -0.1]
  queen = [0.7, 0.3, -0.2]
  loves = [0.1, 0.9, 0.5]

  ## **단어 임베딩 (Word Embedding)**  

### **1. 전통적인 단어 표현 방식 - 원핫 인코딩 (One-Hot Encoding)**
전통적으로 단어를 표현하는 방법은 **원핫 인코딩(One-Hot Encoding)**이었습니다.  
원핫 인코딩은 단어를 **고유한 정수 인덱스로 변환한 후, 해당 인덱스의 위치만 1이고 나머지는 0인 벡터**로 표현하는 방식입니다.

#### **예제 1: 원핫 인코딩**
어휘 집합 (Vocabulary) = { "cat", "dog", "apple", "banana" }  
각 단어에 대해 원핫 벡터를 만들면 다음과 같습니다.

| 단어   | 원핫 벡터 |
|--------|--------------------------------|
| cat    | [1, 0, 0, 0] |
| dog    | [0, 1, 0, 0] |
| apple  | [0, 0, 1, 0] |
| banana | [0, 0, 0, 1] |

#### **📌 원핫 인코딩의 단점**
1. **단어 간의 유사성을 반영하지 못함**  
   - "cat"과 "dog"는 의미적으로 유사하지만, 원핫 벡터에서는 완전히 다른 벡터로 표현됨.  
   - 두 벡터 간의 거리(코사인 유사도)를 계산해도 유사성을 알 수 없음.  
2. **고차원 문제 (희소 행렬, Sparse Matrix)**  
   - 단어의 개수가 수십만 개라면, 벡터 차원이 엄청나게 커짐.  
   - 많은 차원에서 대부분의 값이 0이기 때문에 비효율적임.  
3. **일반화 불가능**  
   - 모델이 새로운 단어를 만나면 처리할 방법이 없음.  

이러한 문제를 해결하기 위해 나온 것이 **단어 임베딩 (Word Embedding)**입니다.

---

### **2. 단어 임베딩 (Word Embedding)**
단어 임베딩은 **각 단어를 저차원 실수 벡터로 변환하는 방법**입니다.  
**유사한 의미를 가지는 단어는 벡터 공간에서 가까운 위치에 배치**됩니다.

#### **📌 단어 임베딩의 특징**
- **차원 축소**: 단어를 30~500 차원의 실수 벡터로 변환 → 원핫보다 훨씬 적은 차원 사용
- **의미적 유사성 반영**: king과 queen, cat과 dog 같은 의미적 유사성을 가진 단어는 유사한 벡터를 가짐.
- **벡터 연산 가능**: 단어의 관계를 벡터 연산으로 표현할 수 있음. (예: "king - man + woman ≈ queen")

#### **예제 2: 단어 임베딩의 의미적 유사성**
훈련을 마친 Word2Vec 모델이 다음과 같은 벡터를 학습했다고 가정함:

| 단어   | 벡터 표현 |
|--------|---------------------|
| king   | [0.8, 0.2, -0.1] |
| queen  | [0.7, 0.3, -0.2] |
| loves  | [0.1, 0.9, 0.5] |

- "king"과 "queen"은 문맥에서 자주 비슷한 위치에서 등장하므로, 벡터값이 유사함.
- 단어 간의 의미적 관계를 반영할 수 있음.

---

### **3. Word2Vec**
**Word2Vec**은 **구글(Google)**이 개발한 단어 임베딩 기법입니다.  
단어의 문맥을 학습하여 의미적으로 유사한 단어가 유사한 벡터를 가지도록 만듭니다.

Word2Vec에는 두 가지 학습 방법이 있음:
1. **CBOW (Continuous Bag of Words)**  
   - **주변 단어(문맥) → 중심 단어 예측**  
   - 예제: "I like to eat apples"  
     - ( "I", "to" ) → "like"  
     - ( "to", "eat" ) → "like"  
   - 전체 문장을 보면, "like"는 "I"와 "to" 사이에서 많이 등장하므로 의미를 학습할 수 있음.
  
2. **Skip-gram**  
   - **중심 단어 → 주변 단어 예측**  
   - 예제: "I like to eat apples"  
     - "like" → "I"  
     - "like" → "to"  
   - 중심 단어 하나를 가지고, 주변 단어들을 예측하도록 학습함.

---

### **4. Word2Vec 학습 과정**
1. **단어쌍 생성 (Context-Word Pairs 생성)**  
   - 주어진 문장에서 중심 단어와 문맥 단어를 쌍으로 만듦.
   - 예제 문장: `"I like to eat apples"`  
     - CBOW 방식:  
       - ('I', 'like')  
       - ('like', 'to')  
       - ('to', 'eat')  
       - ('eat', 'apples')  
     - Skip-gram 방식:  
       - ('like' → 'I')  
       - ('like' → 'to')  
       - ('to' → 'like')  
       - ('to' → 'eat')

2. **모델 학습 (Skip-gram 예시)**  
   - 입력 단어(중심 단어)를 받아서 **주변 단어를 예측**하도록 모델을 학습함.
   - 예제:  
     - 입력: `wt = "like"`  
     - 예측: `wt+j = ["I", "to"]`

3. **손실 함수 (Loss Function) 사용**  
   - 목표: 단어 벡터를 학습하여 **주어진 단어 쌍의 확률을 최대화**하도록 함.  
   - Word2Vec에서는 **Negative Sampling**을 사용하여 학습을 효율적으로 수행함.

---

### **5. Word2Vec 예제**
Word2Vec을 학습하면, 의미적으로 유사한 단어가 벡터 공간에서 가깝게 배치됨.  
또한, 벡터 연산이 가능하여 단어 간의 관계를 나타낼 수 있음.

#### **예제 3: 단어 벡터 연산**
훈련된 Word2Vec 모델에서:

```python
king - man + woman ≈ queen
```

- "king"에서 "man"을 빼고 "woman"을 더하면 **"queen"과 유사한 벡터 값이 나옴**.
- 즉, 단어 간의 관계를 벡터 연산으로 표현할 수 있음.

#### **예제 4: 가장 유사한 단어 찾기**
```python
model.most_similar("king")
```
출력:
```
[("queen", 0.92), ("prince", 0.85), ("monarch", 0.82)]
```
- "king"과 가장 유사한 단어가 "queen", "prince", "monarch" 등으로 나타남.

---

### **6. 결론**
- 원핫 인코딩의 문제점(고차원, 희소성, 의미 없음)을 해결하기 위해 **단어 임베딩** 사용.
- **Word2Vec**은 단어의 의미적 유사성을 반영하는 벡터를 학습함.
- **CBOW**: 주변 단어로 중심 단어를 예측  
- **Skip-gram**: 중심 단어로 주변 단어를 예측  
- Word2Vec 학습 후에는 **단어 간의 유사성, 관계**를 벡터 연산으로 표현할 수 있음.

이처럼 Word2Vec은 단순한 단어 표현이 아니라 **단어 간의 관계와 문맥을 반영하는 강력한 임베딩 기법**입니다. 🚀

  


