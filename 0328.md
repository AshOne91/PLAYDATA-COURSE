### **배깅(Bagging, Bootstrap Aggregating)**  
- 데이터의 일부를 **부트스트래핑(bootstrap)** 기법으로 샘플링하여 여러 개의 모델을 독립적으로 학습시키고, 최종적으로 예측 결과를 **평균(회귀)** 또는 **다수결(분류)** 방식으로 결정하는 방법.  
- 대표 모델: **랜덤 포레스트(Random Forest)**  
- 수식 표현:  
  \[
  f(x) = \frac{1}{T} \sum_{t=1}^{T} h_t(x)
  \]  
  여기서 \( T \)는 모델 개수, \( h_t(x) \)는 개별 모델의 예측 결과.  

---

### **부스팅(Boosting)**  
- 여러 개의 모델을 순차적으로 학습하며, **이전 모델이 만든 오차를 줄이는 방향으로 학습을 진행**하는 방식.  
- 강력한 모델을 만들기 위해 약한 학습기를 여러 개 조합하여 최적의 결과를 도출함.  
- 대표 알고리즘:
  - **AdaBoost**: 틀린 샘플에 가중치를 더 줘서 다음 모델이 더 집중하게 만듦.
  - **Gradient Boosting**: 오차(잔차)를 줄이도록 새로운 모델을 훈련.
  - **XGBoost**: Gradient Boosting을 최적화한 강력한 모델.  

---

### **스태킹(Stacking)**  
- 여러 개의 서로 다른 모델을 사용하여 예측한 후, 그 예측 결과를 **메타 모델(meta model)**이 학습하여 최종 결정을 내리는 방식.  
- 예: 개별 모델이 각각 예측한 결과를 입력 데이터로 삼아 **로지스틱 회귀** 같은 최종 모델이 예측을 수행.  

---

### **투표(Voting)**  
- 여러 개의 개별 모델을 조합하여 최종 예측을 결정하는 방식.  
- 방법:
  - **하드 보팅(Hard Voting)**: 각 모델이 예측한 결과 중 **다수결**로 최종 결정.
  - **소프트 보팅(Soft Voting)**: 각 모델이 예측한 **확률값의 평균**을 사용하여 결정.  

각 방식은 개별 모델의 성능을 더 높이고, 과적합을 방지하는 데 사용됨.

### **서포트 벡터 머신(SVM, Support Vector Machine)**  
- 지도학습(Supervised Learning) 기반 **분류(Classification)와 회귀(Regression) 모델**  
- **결정 경계(Decision Boundary)를 최대한 넓게 만드는 초평면(Hyperplane)을 찾아 분류**  
- 주로 **이진 분류(Binary Classification)** 문제에서 사용됨  

---

## **1️⃣ SVM 핵심 개념**  
### 📌 **1. 초평면(Hyperplane)**
- 데이터 포인트를 나누는 **결정 경계**  
- **n차원 공간에서 (n-1)차원의 평면**  
  - 2D → 1D 선  
  - 3D → 2D 평면  
- SVM은 이 초평면을 가장 **최적의 위치**에 놓는 것이 핵심  

### 📌 **2. 서포트 벡터(Support Vector)**
- **초평면과 가장 가까운 데이터 포인트들**  
- 이 점들이 결정 경계를 형성하는 데 중요한 역할을 함  

### 📌 **3. 마진(Margin)**
- **초평면과 서포트 벡터 사이의 거리**  
- SVM은 이 마진을 **최대한 넓히는 방향**으로 학습됨  
- **마진이 클수록 일반화 성능이 좋아짐** (과적합 방지)  

✅ **최적의 초평면 찾기**  
\[
\max \frac{2}{\|\mathbf{w}\|}
\]
\(\mathbf{w}\) = 초평면의 기울기 (가중치 벡터)  

---

## **2️⃣ 하드 마진 vs 소프트 마진**  
### **✔️ 하드 마진(Hard Margin)**
- **완벽한 선형 분리**가 가능한 경우  
- **데이터가 100% 분류 가능해야 적용 가능**  
- 현실에서는 노이즈나 이상치(outlier) 때문에 거의 불가능  

### **✔️ 소프트 마진(Soft Margin)**
- **오차를 일부 허용하면서 초평면을 찾는 방법**  
- 일부 데이터가 결정 경계를 넘어가더라도 전체적으로 최적의 마진을 찾음  
- 현실에서 더 많이 사용됨  

---

## **3️⃣ 커널 트릭(Kernel Trick)**
- 선형 SVM은 직선(평면)으로 분리할 수 있는 데이터만 분류 가능  
- **비선형 데이터는 커널 트릭을 사용하여 고차원 공간으로 변환**해 분류  
- 데이터를 더 높은 차원으로 맵핑하여 선형 분리가 가능하도록 변형  

### 📌 **대표적인 커널 종류**
1. **선형 커널 (Linear Kernel)**  
   - 단순한 선형 분류에 사용됨  
   - 공식:  
     \[
     K(x, y) = x \cdot y
     \]  
2. **다항식 커널 (Polynomial Kernel)**  
   - 다항식 변환을 통해 고차원 특성을 고려  
   - 공식:  
     \[
     K(x, y) = (x \cdot y + c)^d
     \]  
3. **RBF (Radial Basis Function, 가우시안 커널)**
   - 데이터의 곡선 분포를 잘 학습  
   - 공식:  
     \[
     K(x, y) = \exp\left(-\frac{\|x - y\|^2}{2\sigma^2}\right)
     \]  
   - **가장 많이 사용되는 커널**  
4. **시그모이드 커널 (Sigmoid Kernel)**
   - 신경망(NN)과 유사한 구조  
   - 공식:  
     \[
     K(x, y) = \tanh(\alpha x \cdot y + c)
     \]  

---

## **4️⃣ SVM 최적화 하이퍼파라미터**
- **C (Regularization Parameter)**  
  - 오차 허용 정도 조절  
  - **작을수록 마진이 커지고 일반화 성능이 높아짐 (과적합 방지)**  
  - **클수록 마진이 작아지고 오차를 최소화하려 함 (과적합 가능성 증가)**  

- **감마 (Gamma, RBF 커널용)**
  - **작을수록 넓은 영역을 고려** (일반화 성능 증가)  
  - **클수록 좁은 영역을 고려** (복잡한 결정 경계 형성)  

---

## **5️⃣ SVM 장점과 단점**
### ✔️ **장점**
✅ 고차원 데이터에서도 성능이 좋음  
✅ 과적합 방지가 뛰어남 (마진 최대화)  
✅ 커널 트릭을 사용하면 비선형 데이터도 잘 처리 가능  

### ❌ **단점**
🚫 데이터 양이 많으면 학습 속도가 느림  
🚫 하이퍼파라미터 (C, Gamma) 튜닝이 필요  
🚫 해석력이 낮음 (결과를 직관적으로 설명하기 어려움)  

---

## **6️⃣ SVM 활용 분야**
✅ 얼굴 인식 (Face Recognition)  
✅ 텍스트 분류 (Spam Filtering)  
✅ 이미지 분류 (Handwritten Digit Recognition)  
✅ 유전자 데이터 분석 (Gene Classification)  

---

**📌 결론**  
SVM은 **마진을 최대화하는 초평면을 찾는 분류 알고리즘**으로,  
특히 **고차원 데이터나 작은 데이터셋에서 강력한 성능**을 보임.  
비선형 데이터는 **커널 트릭**을 활용해 변환 가능.  
하지만 **대량의 데이터 처리 시 속도가 느리고 튜닝이 필요**함.
