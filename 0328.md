### **배깅(Bagging, Bootstrap Aggregating)**  
- 데이터의 일부를 **부트스트래핑(bootstrap)** 기법으로 샘플링하여 여러 개의 모델을 독립적으로 학습시키고, 최종적으로 예측 결과를 **평균(회귀)** 또는 **다수결(분류)** 방식으로 결정하는 방법.  
- 대표 모델: **랜덤 포레스트(Random Forest)**  
- 수식 표현:  
  \[
  f(x) = \frac{1}{T} \sum_{t=1}^{T} h_t(x)
  \]  
  여기서 \( T \)는 모델 개수, \( h_t(x) \)는 개별 모델의 예측 결과.  

---

### **부스팅(Boosting)**  
- 여러 개의 모델을 순차적으로 학습하며, **이전 모델이 만든 오차를 줄이는 방향으로 학습을 진행**하는 방식.  
- 강력한 모델을 만들기 위해 약한 학습기를 여러 개 조합하여 최적의 결과를 도출함.  
- 대표 알고리즘:
  - **AdaBoost**: 틀린 샘플에 가중치를 더 줘서 다음 모델이 더 집중하게 만듦.
  - **Gradient Boosting**: 오차(잔차)를 줄이도록 새로운 모델을 훈련.
  - **XGBoost**: Gradient Boosting을 최적화한 강력한 모델.  

---

### **스태킹(Stacking)**  
- 여러 개의 서로 다른 모델을 사용하여 예측한 후, 그 예측 결과를 **메타 모델(meta model)**이 학습하여 최종 결정을 내리는 방식.  
- 예: 개별 모델이 각각 예측한 결과를 입력 데이터로 삼아 **로지스틱 회귀** 같은 최종 모델이 예측을 수행.  

---

### **투표(Voting)**  
- 여러 개의 개별 모델을 조합하여 최종 예측을 결정하는 방식.  
- 방법:
  - **하드 보팅(Hard Voting)**: 각 모델이 예측한 결과 중 **다수결**로 최종 결정.
  - **소프트 보팅(Soft Voting)**: 각 모델이 예측한 **확률값의 평균**을 사용하여 결정.  

각 방식은 개별 모델의 성능을 더 높이고, 과적합을 방지하는 데 사용됨.
