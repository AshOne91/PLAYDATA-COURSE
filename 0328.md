### **배깅(Bagging, Bootstrap Aggregating)**  
- 데이터의 일부를 **부트스트래핑(bootstrap)** 기법으로 샘플링하여 여러 개의 모델을 독립적으로 학습시키고, 최종적으로 예측 결과를 **평균(회귀)** 또는 **다수결(분류)** 방식으로 결정하는 방법.  
- 대표 모델: **랜덤 포레스트(Random Forest)**  
- 수식 표현:  
  \[
  f(x) = \frac{1}{T} \sum_{t=1}^{T} h_t(x)
  \]  
  여기서 \( T \)는 모델 개수, \( h_t(x) \)는 개별 모델의 예측 결과.  

---

### **부스팅(Boosting)**  
- 여러 개의 모델을 순차적으로 학습하며, **이전 모델이 만든 오차를 줄이는 방향으로 학습을 진행**하는 방식.  
- 강력한 모델을 만들기 위해 약한 학습기를 여러 개 조합하여 최적의 결과를 도출함.  
- 대표 알고리즘:
  - **AdaBoost**: 틀린 샘플에 가중치를 더 줘서 다음 모델이 더 집중하게 만듦.
  - **Gradient Boosting**: 오차(잔차)를 줄이도록 새로운 모델을 훈련.
  - **XGBoost**: Gradient Boosting을 최적화한 강력한 모델.  

---

### **스태킹(Stacking)**  
- 여러 개의 서로 다른 모델을 사용하여 예측한 후, 그 예측 결과를 **메타 모델(meta model)**이 학습하여 최종 결정을 내리는 방식.  
- 예: 개별 모델이 각각 예측한 결과를 입력 데이터로 삼아 **로지스틱 회귀** 같은 최종 모델이 예측을 수행.  

---

### **투표(Voting)**  
- 여러 개의 개별 모델을 조합하여 최종 예측을 결정하는 방식.  
- 방법:
  - **하드 보팅(Hard Voting)**: 각 모델이 예측한 결과 중 **다수결**로 최종 결정.
  - **소프트 보팅(Soft Voting)**: 각 모델이 예측한 **확률값의 평균**을 사용하여 결정.  

각 방식은 개별 모델의 성능을 더 높이고, 과적합을 방지하는 데 사용됨.

### **서포트 벡터 머신(SVM, Support Vector Machine)**  
- 지도학습(Supervised Learning) 기반 **분류(Classification)와 회귀(Regression) 모델**  
- **결정 경계(Decision Boundary)를 최대한 넓게 만드는 초평면(Hyperplane)을 찾아 분류**  
- 주로 **이진 분류(Binary Classification)** 문제에서 사용됨  

---

## **1️⃣ SVM 핵심 개념**  
### 📌 **1. 초평면(Hyperplane)**
- 데이터 포인트를 나누는 **결정 경계**  
- **n차원 공간에서 (n-1)차원의 평면**  
  - 2D → 1D 선  
  - 3D → 2D 평면  
- SVM은 이 초평면을 가장 **최적의 위치**에 놓는 것이 핵심  

### 📌 **2. 서포트 벡터(Support Vector)**
- **초평면과 가장 가까운 데이터 포인트들**  
- 이 점들이 결정 경계를 형성하는 데 중요한 역할을 함  

### 📌 **3. 마진(Margin)**
- **초평면과 서포트 벡터 사이의 거리**  
- SVM은 이 마진을 **최대한 넓히는 방향**으로 학습됨  
- **마진이 클수록 일반화 성능이 좋아짐** (과적합 방지)  

✅ **최적의 초평면 찾기**  
\[
\max \frac{2}{\|\mathbf{w}\|}
\]
\(\mathbf{w}\) = 초평면의 기울기 (가중치 벡터)  

---

## **2️⃣ 하드 마진 vs 소프트 마진**  
### **✔️ 하드 마진(Hard Margin)**
- **완벽한 선형 분리**가 가능한 경우  
- **데이터가 100% 분류 가능해야 적용 가능**  
- 현실에서는 노이즈나 이상치(outlier) 때문에 거의 불가능  

### **✔️ 소프트 마진(Soft Margin)**
- **오차를 일부 허용하면서 초평면을 찾는 방법**  
- 일부 데이터가 결정 경계를 넘어가더라도 전체적으로 최적의 마진을 찾음  
- 현실에서 더 많이 사용됨  

---

## **3️⃣ 커널 트릭(Kernel Trick)**
- 선형 SVM은 직선(평면)으로 분리할 수 있는 데이터만 분류 가능  
- **비선형 데이터는 커널 트릭을 사용하여 고차원 공간으로 변환**해 분류  
- 데이터를 더 높은 차원으로 맵핑하여 선형 분리가 가능하도록 변형  

### 📌 **대표적인 커널 종류**
1. **선형 커널 (Linear Kernel)**  
   - 단순한 선형 분류에 사용됨  
   - 공식:  
     \[
     K(x, y) = x \cdot y
     \]  
2. **다항식 커널 (Polynomial Kernel)**  
   - 다항식 변환을 통해 고차원 특성을 고려  
   - 공식:  
     \[
     K(x, y) = (x \cdot y + c)^d
     \]  
3. **RBF (Radial Basis Function, 가우시안 커널)**
   - 데이터의 곡선 분포를 잘 학습  
   - 공식:  
     \[
     K(x, y) = \exp\left(-\frac{\|x - y\|^2}{2\sigma^2}\right)
     \]  
   - **가장 많이 사용되는 커널**  
4. **시그모이드 커널 (Sigmoid Kernel)**
   - 신경망(NN)과 유사한 구조  
   - 공식:  
     \[
     K(x, y) = \tanh(\alpha x \cdot y + c)
     \]  

---

## **4️⃣ SVM 최적화 하이퍼파라미터**
- **C (Regularization Parameter)**  
  - 오차 허용 정도 조절  
  - **작을수록 마진이 커지고 일반화 성능이 높아짐 (과적합 방지)**  
  - **클수록 마진이 작아지고 오차를 최소화하려 함 (과적합 가능성 증가)**  

- **감마 (Gamma, RBF 커널용)**
  - **작을수록 넓은 영역을 고려** (일반화 성능 증가)  
  - **클수록 좁은 영역을 고려** (복잡한 결정 경계 형성)  

---

## **5️⃣ SVM 장점과 단점**
### ✔️ **장점**
✅ 고차원 데이터에서도 성능이 좋음  
✅ 과적합 방지가 뛰어남 (마진 최대화)  
✅ 커널 트릭을 사용하면 비선형 데이터도 잘 처리 가능  

### ❌ **단점**
🚫 데이터 양이 많으면 학습 속도가 느림  
🚫 하이퍼파라미터 (C, Gamma) 튜닝이 필요  
🚫 해석력이 낮음 (결과를 직관적으로 설명하기 어려움)  

---

## **6️⃣ SVM 활용 분야**
✅ 얼굴 인식 (Face Recognition)  
✅ 텍스트 분류 (Spam Filtering)  
✅ 이미지 분류 (Handwritten Digit Recognition)  
✅ 유전자 데이터 분석 (Gene Classification)  

---

**📌 결론**  
SVM은 **마진을 최대화하는 초평면을 찾는 분류 알고리즘**으로,  
특히 **고차원 데이터나 작은 데이터셋에서 강력한 성능**을 보임.  
비선형 데이터는 **커널 트릭**을 활용해 변환 가능.  
하지만 **대량의 데이터 처리 시 속도가 느리고 튜닝이 필요**함.

### **📌 군집화(Clustering) 모델 훈련 과정**  

군집화는 비지도학습(Unsupervised Learning) 기법 중 하나로, 데이터를 유사한 그룹으로 나누는 방식임.  
군집화 모델을 훈련하는 과정은 **1) 데이터 준비 → 2) 모델 선택 → 3) 하이퍼파라미터 설정 → 4) 모델 훈련 → 5) 성능 평가** 순서로 진행됨.  

---

## **1️⃣ 데이터 준비**  
- **데이터 로드 및 전처리**  
  - 이상치(Outlier) 제거  
  - 결측치 처리 (삭제 or 평균 대체)  
  - 정규화(Normalization) 또는 표준화(Standardization)  

- **차원 축소(Dimensionality Reduction) 활용 가능**  
  - PCA (주성분 분석)  
  - t-SNE, UMAP  

---

## **2️⃣ 군집화 알고리즘 선택**  
군집화 모델 선택은 **데이터 특성에 따라 달라짐.**  

### 📌 **군집화 알고리즘 종류**  
| 알고리즘 | 특징 | 데이터 유형 |
|---------|------|-----------|
| **K-Means** | K개의 중심점(Centroid) 기준, 거리 기반 | 밀집된 군집, 원형 구조 |
| **Hierarchical Clustering** | 계층적 방식(병합/분할) | 계층적 관계가 있는 데이터 |
| **DBSCAN** | 밀도 기반 군집화, 이상치 제거 가능 | 비선형 구조, 이상치 포함 데이터 |
| **GMM (Gaussian Mixture Model)** | 확률 기반 군집화 | 데이터가 여러 가우시안 분포를 따를 때 |
| **Mean-Shift** | 데이터의 밀도 중심으로 이동 | 데이터 분포가 명확할 때 |

---

## **3️⃣ 하이퍼파라미터 설정**  
### **K-Means의 주요 하이퍼파라미터**
- **K 값 (클러스터 개수)** → 엘보우 방법(Elbow Method) 또는 실루엣 분석(Silhouette Score)로 최적값 찾음  
- **초기 중심 선택 방식** → K-Means++, 랜덤  
- **거리 측정 방식** → 유클리드 거리(Euclidean Distance), 맨해튼 거리(Manhattan Distance)  

### **DBSCAN의 주요 하이퍼파라미터**
- **eps (반경 내 거리)** → 데이터 밀도에 따라 조정  
- **min_samples (최소 포인트 수)** → 클러스터로 인식할 최소 개수  

### **GMM의 주요 하이퍼파라미터**
- **클러스터 개수(K) 설정**  
- **공분산 행렬 유형 (diagonal, full 등)**  

---

## **4️⃣ 모델 훈련(Fitting Model)**  
모델을 실제 데이터에 적용하여 훈련 진행  

### ✅ **K-Means 예제 (Python)**
```python
from sklearn.cluster import KMeans

# K-Means 모델 훈련
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)
kmeans.fit(X)

# 클러스터 할당 결과
labels = kmeans.labels_
centroids = kmeans.cluster_centers_
```

### ✅ **DBSCAN 예제**
```python
from sklearn.cluster import DBSCAN

# DBSCAN 모델 훈련
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan.fit(X)

# 클러스터 할당 결과
labels = dbscan.labels_
```

---

## **5️⃣ 성능 평가(Evaluation)**  
비지도학습이므로 **정확도(Accuracy) 개념 없음.** 대신 아래 지표 활용  

### 📌 **군집 평가 지표**  
- **실루엣 점수(Silhouette Score)** → 클러스터 내부 응집력 & 클러스터 간 분리 정도  
- **엘보우 방법(Elbow Method)** → K-Means 최적 K 찾기  
- **DBI (Davies-Bouldin Index)** → 클러스터 간 분리도 평가  
- **CH 지수 (Calinski-Harabasz Index)** → 군집 응집력 평가  

### ✅ **K-Means 실루엣 점수 예제**
```python
from sklearn.metrics import silhouette_score

score = silhouette_score(X, kmeans.labels_)
print(f"Silhouette Score: {score}")
```

---

## **📌 결론**  
1️⃣ **데이터를 전처리하고** 차원 축소 가능  
2️⃣ **알맞은 군집화 알고리즘 선택** (K-Means, DBSCAN 등)  
3️⃣ **하이퍼파라미터 튜닝 (K 값, eps 등 조정)**  
4️⃣ **모델을 훈련하고 클러스터 결과 분석**  
5️⃣ **실루엣 점수 등으로 성능 평가**  

실제 프로젝트에서는 **군집화 후 분석(시각화, 프로파일링)이 핵심**임.
